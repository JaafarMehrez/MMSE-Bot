Question: Did the 2019 discovery of O(N log(N)) multiplication have a practical outcome?
Body: <p>Some time ago I've read this news article, <a href="https://www.quantamagazine.org/mathematicians-discover-the-perfect-way-to-multiply-20190411/" rel="noreferrer">Mathematicians Discover the Perfect Way to Multiply</a>, reporting a discovery published in 2019, where  Harvey and Hoeven[1] found a algorithm able to execute multiplication in <span class="math-container">$N \log N$</span> steps. Compare with the <span class="math-container">$N^2$</span> we are used to while doing multiplication by hand.</p>
<p>That amused me, because I had no idea in Mathematics there was still open problems in basic arithmetic, something I took for granted, long ago settled knowledge, since childhood.</p>
<p>Now I wonder, did this discovery help, or could help, materials modeling? Did a code developed somewhere for this purpose made use of it. A downside of the new algorithm is a set up phase where you need to put the numbers in a suitable form, so this initial effort is paid only for large numbers. My impression is that in matter modeling our algorithms are more about multiplying lots of small numbers fast, instead of some big numbers, so I guess the answer is probably no. But I'm not sure.</p>
<p>If not, can someone explain in detail the impact of <em>any</em> of the multiplication algorithms scaling better than <span class="math-container">$N^2$</span>, for some practical application?</p>
<p>[1] David Harvey, Joris van der Hoeven. Integer multiplication in time O(n log n). 2019. <a href="https://hal.archives-ouvertes.fr/hal-02070778" rel="noreferrer">⟨hal-02070778⟩</a></p>

Best Answer: <h1>What are the state-of-the-art algorithms for long-integer multiplication?</h1>
<p>First let me address the point you raised about the schoolbook algorithm having <span class="math-container">$\mathcal{O}(n^2)$</span> scaling, by saying that this was not the state-of-the-art algorithm used in most matter modeling software. Below I give a brief overview:</p>
<p><strong>(1960) Karatsuba multiplication. <span class="math-container">$\mathcal{O}(n^{1.58})$</span>:</strong> Faster than naive multiplication after <span class="math-container">$n$</span> gets ~<span class="math-container">$10^{96}$</span>.<br>
<strong>(1963-2005) Toom-Cook-Knuth. <span class="math-container">$\mathcal{O}(n\cdot 2^{\sqrt{2\log n}}\log n)$</span>:</strong> Generalization of Karatsuba.<br>
<strong>(1971) Schönhage-Strassen. <span class="math-container">$\mathcal{O}(n\log n\log\log n)$</span>:</strong> Outperforms TCK after ~<span class="math-container">$10^{10000}$</span>.<br>
<strong>(2007) <strong>Fürer.</strong>  <span class="math-container">$\mathcal{O}(n\log n\cdot 2^{\mathcal{O}(\log^*n)})$</span>:</strong> Outperforms SS after ~<span class="math-container">$10^{10^{18}}$</span>.<br>
<strong>(2015) <strong>Harvey <em>et al</em>.</strong>  <span class="math-container">$\mathcal{O}(n\log n\cdot 2^{3\log^*n})$</span>:</strong> Similar to Fürer's algorithm.<br>
<strong>(2015) <strong>Harvey <em>et al</em>.</strong>  <span class="math-container">$\mathcal{O}(n\log n\cdot 2^{2\log^*n})$</span>:</strong> Relies on conjectures not yet proven.<br>
<strong>(2016) Covanov-Thomé.  <span class="math-container">$\mathcal{O}(n\log n\cdot 2^{2\log^*n})$</span>:</strong> Relies on (different) conjectures not yet proven.<br>
<strong>(2018) Harvey &amp; van der Hoeven.  <span class="math-container">$\mathcal{O}(n\log n\cdot 2^{2\log^*n})$</span>:</strong> Finally proven without conjectures.<br>
<strong>(2019) Harvey &amp; van der Hoeven.  <span class="math-container">$\mathcal{O}(n\log n)$</span>:</strong> The algorithm mentioned in the paper you cited.</p>
<hr><br>
<h1>Which of these algorithms have practical consequences?</h1>
<p><strong>Schönhage-Strassen</strong>: <a href="https://en.wikipedia.org/wiki/GNU_Multiple_Precision_Arithmetic_Library" rel="noreferrer">GNU multi-precision library</a> uses it for #s with 33,000 to 150,000 digits.<br>
<strong>Toom-Cook</strong>: is used for intermediate-sized numbers, basically until Schönhage-Strassen is used.<br>
<strong>Karatsuba</strong>: is a specific case of Toom-Cook: not likely used for numbers smaller than <span class="math-container">$10^{96}$</span>.</p>
<hr><br>
<h1>So what are the consequences of the 2019 algorithm?</h1>
<p>Likely nothing for the calculations we typically do. Schönhage and Strassen predicted very long ago that <span class="math-container">$\mathcal{O}(n\log n)$</span> would be the most efficient possible algorithm from a computational complexity point of view, and in 2019 the algorithm that achieves this predicted &quot;lower bound&quot; was found by Harvey and van der Hoeven. It is probably not implemented in any library, just as the 2018, 2016, 2015, and 2007 algorithms are also not implemented anywhere as far as I know. They are all beautiful mathematics papers which give theoretical scalings, but likely have no practical consequences.</p>
<p>Do you ever multiply together integers with 96 digits? Typically in double-precision floating point arithmetic we multiply numbers with no more than 18 digits, and in quadruple-precision arithmetic (which is indeed used in matter modeling for things like numerical derivatives in variational energy calculations, but quite rarely) numbers have up to 36 digits, but it is unlikely that anyone in matter modeling is frequently multiplying numbers with 96 digits, so even the Karatsuba algorithm is in practice worse than the schoolbook <span class="math-container">$n^2$</span> algorithm, due to Karatsuba involving extra shifts and additions as overhead. The Toom-Cook algorithms (such as Karatsuba) are useful in number theory, and in fact <em>we use them every day when we do e-banking or when we use GitHub involving RSA keys</em>, since RSA numbers are hundreds or thousands of digits long. The Schönhage-Strassen is used mainly in number theory for things like calculating record numbers of digits in <span class="math-container">$\pi$</span>, and for practical applications such as multiplying polynomials with huge coefficients.</p>
<p><strong>Conclusion:</strong> The 2019 algorithm for integer multiplication does not affect real-world applications.</p>



================================================================================

Question: Is there a database where we can find previously determined geometries of materials?
Body: <p>For small molecules, NIST has a database of experimentally determined geometries <a href="https://cccbdb.nist.gov/expgeom1x.asp" rel="noreferrer">here</a>, and <em>ab initio</em> geometries <a href="https://cccbdb.nist.gov/geom1x.asp" rel="noreferrer">here</a>. I wonder if there is something similar for materials?</p>

Best Answer: <p><a href="http://www.2dmatpedia.org/" rel="noreferrer"><strong>2DMatPedia</strong></a> </p>

<p>An open computational database of two-dimensional materials. A large dataset of 2D materials, with more than 6,000 monolayer structures, obtained from both top-down and bottom-up discovery procedures</p>

<p><a href="https://www.materialscloud.org/discover/2dstructures/dashboard/ptable" rel="noreferrer"><strong>2D structures and layered materials</strong></a></p>

<p>Results from screening all known 3D crystal structures finding those that can be computationally exfoliated producing 2D materials candidates. </p>

<p><a href="http://www.2dmatpedia.org/" rel="noreferrer"><strong>TopoMat database</strong></a> </p>

<p>The <a href="https://www.materialscloud.org/discover/2dstructures/dashboard/ptable" rel="noreferrer">database</a> contains the results of high-throughput first-principles screening of known crystal structures for topological materials (topological insulators, Dirac and Weyl semimetals, etc.).</p>

<p><strong>Update</strong></p>

<p>I recently came across the publication <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/advs.201900808" rel="noreferrer">Data‐Driven Materials Science: Status, Challenges and Perspectives</a> by Himanen et. al featuring this list of current major materials data infrastructures.</p>

<p><a href="https://i.sstatic.net/oebL5.png" rel="noreferrer"><img src="https://i.sstatic.net/oebL5.png" alt="enter image description here"></a>
<a href="https://i.sstatic.net/5f9o3.png" rel="noreferrer"><img src="https://i.sstatic.net/5f9o3.png" alt="enter image description here"></a>
<a href="https://i.sstatic.net/NeN1I.png" rel="noreferrer"><img src="https://i.sstatic.net/NeN1I.png" alt="enter image description here"></a></p>

<p>References:</p>

<ol>
<li>Himanen, Lauri, et al. "Data‐Driven Materials Science: Status, Challenges, and Perspectives." Advanced Science 6.21 (2019): 1900808.</li>
</ol>


================================================================================

Question: What is the closest thing we have to &quot;the&quot; universal density functional?
Body: <p>It is understood that finding "the" universal functional is an NP-complete problem. However, progressively better functionals have been constructed decade after decade and fitted to more and more comprehensive datasets. </p>

<p>What is the closest humans have achieved, to a universal functional?</p>

Best Answer: <p>I think this question somewhat comes down to what "camp" of DFT progression you subscribe to. I should specify upfront that this summary is mainly centered around molecular systems, so some of the recommendations likely vary for materials where the computational workload can often be much greater.</p>

<p>One side really emphasizes accuracy with respect to experiment and is somewhat less concerned with the physical interpretation of the functional form. These groups make efforts to directly improve the accuracy of functionals with respect to experiment by extensive fitting and parameterization. Some functionals that fit into this category would be the <a href="https://en.wikipedia.org/wiki/Minnesota_functionals" rel="noreferrer">Minnesota Functionals</a> from the Truhlar group, as well the ωB97X and ωB97M functionals from the Head-Gordon group. Based on fairly extensive benchmarking (see this <a href="https://www.tandfonline.com/doi/full/10.1080/00268976.2017.1333644" rel="noreferrer">excellent paper</a>), these functionals are tough to beat for a wide variety of energetic metrics and types of molecules.</p>

<p>On the other side, the form of functionals is more physically motivated. This is done by ensuring the functional satisfies certain exact constraints of the "universal functional". A prominent example of this type is the <a href="https://arxiv.org/ftp/arxiv/papers/1511/1511.01089.pdf" rel="noreferrer">SCAN functional</a> from Perdew et al. While these types of functionals have not been able to achieve the same experimental accuracy as more heavily parameterized functionals, there is a chance that they are more robust and amenable to improvement, as they exactly match known properties of the "universal functional".</p>

<p>So it depends in what you are interested. If you want the closest functional form to the "universal functional", you would likely want something from the second camp. However, if your interest is in what will give you the best results for a wide range of complexes/materials right now, you will likely want to go with a functional that has been extensively parameterized on a large training set. </p>


================================================================================

Question: DeepMind just announced a breakthrough in protein folding, what are the consequences?
Body: <p>There was some recent media reporting about a purported Google breakthrough on applying <a href="https://en.wikipedia.org/wiki/Machine_learning" rel="noreferrer">machine learning</a> techniques to tackle the <a href="https://en.wikipedia.org/wiki/Protein_folding" rel="noreferrer">protein folding</a> problem, as told for example in this news article, <a href="https://arstechnica.com/science/2020/11/deepmind-ai-handles-protein-folding-which-humbled-previous-software/" rel="noreferrer">DeepMind AI handles protein folding, which humbled previous software</a>.</p>
<p>Unfortunately there is not much details, as no peer-reviewed paper was published yet. But supposing the paper is eventually published, and the claims confirmed as legitimate and not just hot air, what are the implications? I understand that a reliable way to predict the folding patterns of proteins <em>in silico</em> could be a huge step over the experimental means like <a href="https://en.wikipedia.org/wiki/X-ray_crystallography" rel="noreferrer">x-ray crystallography</a>, that sometimes requires even crystals <a href="https://www.nature.com/articles/npjmgrav201510" rel="noreferrer">grown in space</a> at great cost. Besides the large quantitative but incremental cost and time savings, are there any non-obvious qualitative differences in the kind of research enabled if such a breaktrough is confirmed?</p>

Best Answer: <p>Imagine if given an amino acid sequence, you could quickly calculate what the shape of the corresponding protein would be.</p>
<ul>
<li><strong>You would be able to predict what effect a <em>mutation</em> would have on the shape of the protein.</strong> Switching just one glutamic acid with valine completely changes the shape of hemoglobin to the extent that people with this mutation are said to have a disease called &quot;sickle cell anemia&quot;. Likewise, <em><strong>there might be a substitution that we haven't even discovered yet</strong></em>  that would <em>increase</em> our capacity to inhale oxygen, leading to super-human strength just as the glutamic acid to valine substitution causes can have severe implications (sick cell anemia).</li>
<li><strong>You could do inverse design.</strong> For example if you want a protein that looks and can behave like a stapler, you could search for the amino acid sequence that gives you the protein shaped the way you want, a bit like DNA origami but <em>without the need for staples:</em></li>
</ul>
<p><a href="https://i.sstatic.net/KgoPS.png" rel="noreferrer"><img src="https://i.sstatic.net/KgoPS.png" alt="enter image description here" /></a></p>
<h3>So what did DeepMind announce 2 days ago?</h3>
<p>First of all, the link you provided is to an article <em>about</em> what they did. <a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology" rel="noreferrer">This is the original blog post that DeepMind published 2 days ago</a>.</p>
<p>What they announced was that they did very well in the CASP 14 competition. This is a competition for protein folding predictions that has been going on every two years since 1993 and this is the 14th competition that has taken place. In this competition, researchers try to predict the structure of various proteins, and the predictions are evaluated based on the <a href="https://en.wikipedia.org/wiki/Global_distance_test" rel="noreferrer">Global Distance Test</a>. In 2018 DeepMind won the CASP 13 competition with a median score of almost 60 GDT, whereas the winning teams in CASP 7 (2006) to CASP 12 (2016) never got far above 40 GDT.</p>
<p>In the 2020 competition, DeepMind shattered this record again, by getting a median score of 92.4 GDT, which means that atom positions were predicted correctly within 1.6 Angstroms.</p>
<h3>Was it peer reviewed?</h3>
<p>This was a competition, for which impartial judges evaluate contestants <em>quantitatively</em> based on a single number, the GDP. This is even better than peer review. Peer review for journals, does not involve the referees actually testing the authors' software to see if it reproduces some standard benchmark: it is just a process where scientists guess whether or not they believe that the authors' calculation really did do what they said they did. This competition was a bit like a race, where Usain Bolt was determined by Olympic judges to break the record for the 100m sprint, and non-judges can also believe it because they for the most part saw it happen on TV, just like you can check the results of the 2020 protein folding competition <a href="https://predictioncenter.org/decoysets2019/results.cgi" rel="noreferrer">here</a>.</p>
<h3>Then why are people saying that they haven't submitted the paper yet?</h3>
<p>They won a major competition, so now the authors can get credit for it in the way that is most relevant for their careers as researchers: by publishing in Nature or Science and getting citations on Google Scholar for it. They will likely just report the results of the competition, which we already know. They might reveal some details about any algorithmic developments they made between 2018 and 2020, but they are also a private company and might not reveal everything the way a research institute not owned by a for-profit company sometimes would by institutional policy be required.</p>
<h3>So was a &quot;50-year-old problem solved&quot;, as they claim?</h3>
<p>Not really. The particular benchmark set for CASP 14 was solved to an accuracy that had never been achieved by any research group for any previous CASP benchmark set. <strong>However, next time the CASP benchmark set can just be made more difficult and they will be back to not being able to predict the protein structures.</strong></p>
<p>It is however a significant achievement that the protein structures in this benchmark set (which was supposed to be hard by 2020 standards) were reproduced with atom positions correct to about 1.6 Angstroms. It means that scientists can now be <em>that</em> sure about their protein folding predictions for proteins of similar complexity to the ones in the CASP 14 competition set.</p>
<p><strong>Update!</strong> After this question, someone else asked a question which pointed out that AlphaFold might struggle for some important types of proteins such hemoglobin, here: <a href="https://mattermodeling.stackexchange.com/q/3887/5">Does DeepMind&#39;s new protein folding software (AlphaFold) also work well for metalloproteins (proteins with metal cofactors)?</a></p>


================================================================================

Question: What are the types of charge analysis?
Body: <p>I am evaluating the atomic charge of a system using inter-atomic potentials and comparing with using DFT. I know about the following types of partial charge: Mulliken, Bader, Qeq. I wonder what are the differences between the methods, such as pros and cons.</p>
<p>Perhaps we can get an answer explaining each of the following types of <a href="https://en.wikipedia.org/wiki/Partial_charge" rel="noreferrer">partial charge</a>:</p>
<ul>
<li><p>B̶a̶d̶e̶r̶ <a href="https://mattermodeling.stackexchange.com/a/1444/5">[link to answer]</a></p>
</li>
<li><p>M̶u̶l̶l̶i̶k̶e̶n̶, <a href="https://mattermodeling.stackexchange.com/a/1447/5">[link to answer]</a>, <a href="https://mattermodeling.stackexchange.com/a/1465/5">[link to answer]</a></p>
</li>
<li><p>L̶ö̶w̶d̶i̶n̶, <a href="https://mattermodeling.stackexchange.com/a/1465/5">[link to answer]</a></p>
</li>
<li><p>E̶S̶P̶-̶d̶e̶r̶i̶v̶e̶d̶ <a href="https://mattermodeling.stackexchange.com/a/1450/5">[link to answer]</a></p>
</li>
<li><p>Coulson,</p>
</li>
<li><p>natural charges,</p>
</li>
<li><p>C̶M̶5̶ <a href="https://mattermodeling.stackexchange.com/a/1446/5">[link to answer]</a>,</p>
</li>
<li><p>density fitted,</p>
</li>
<li><p>H̶i̶r̶s̶h̶f̶e̶l̶d̶ <a href="https://mattermodeling.stackexchange.com/a/1446/5">[link to answer]</a>,</p>
</li>
<li><p>Maslen,</p>
</li>
<li><p>Politzer,</p>
</li>
<li><p>V̶o̶r̶o̶n̶o̶i̶, <a href="https://mattermodeling.stackexchange.com/a/1505/5">[link to answer]</a></p>
</li>
<li><p>D̶D̶E̶C̶, [<a href="https://mattermodeling.stackexchange.com/a/2294/5">link to answer</a>]</p>
</li>
<li><p>NBO-based,</p>
</li>
<li><p>dipole-based,</p>
</li>
<li><p>ATP/Born/Callen/Szigeti,</p>
</li>
<li><p>Chelp,</p>
</li>
<li><p>C̶h̶e̶l̶p̶G̶ ̶(̶B̶r̶e̶n̶e̶m̶a̶n̶)̶ <a href="https://mattermodeling.stackexchange.com/a/1450/5">[link to answer]</a>,</p>
</li>
<li><p>Merz-Singh-Kollman,</p>
</li>
<li><p>formal charges;</p>
</li>
<li><p>any QM or theoretically derived charge model.</p>
</li>
</ul>
<p>Here is a good <a href="http://bigdft.org/images/5/54/2011-10_AS_Charges.pdf" rel="noreferrer">starting point</a> for anyone that wants to explain one of the above types of partial charge in the format of my (Nike Dattani's) answer.</p>

Best Answer: <h2>Bader charge analysis</h2>
<p>In Bader's theory of <em>Atoms in Molecules</em>, we partition a molecule into &quot;atoms&quot; which are separated from each other by surfaces of minimum charge density:</p>
<p>                                          <img src="https://i.sstatic.net/mjQzH.png" width="300"></p>
<p>You can then calculate the partial charges of the &quot;atoms&quot; in the molecule, e.g. H<span class="math-container">$_2$</span>O might yield:</p>
<p><span class="math-container">\begin{array}{cc}
\rm{Atom} &amp; \rm{Charge}\\
\hline
\rm{O} &amp; -1.150\\
\rm{H} &amp; +0.425\\
\rm{H} &amp; +0.425
\end{array}</span></p>
<p>Meaning that each hydrogen has &quot;given away&quot; 0.575 of an electron.</p>
<p><strong>Pros:</strong></p>
<ul>
<li>There is an <a href="http://theory.cm.utexas.edu/henkelman/code/bader/" rel="nofollow noreferrer">up-to-date code</a> last updated in 2020, and available in <a href="https://wiki.fysik.dtu.dk/gpaw/tutorials/bader/bader.html" rel="nofollow noreferrer">GPAW</a>, <a href="http://bigdft.org/Wiki/index.php?title=Bader_charge_analysis" rel="nofollow noreferrer">BigDFT</a>, VASP and CASTEP. It is also implemented for VASP in <a href="https://github.com/jkitchin/vasp/blob/master/vasp/bader.py" rel="nofollow noreferrer">this python script</a> and also <a href="http://www.demon-software.com/public_html/support/htmlug/ug-node105.html" rel="nofollow noreferrer">available in deMon2k</a>. Martin's comment says it's also available in Multiwfn. A 3rd option is the <a href="https://github.com/materialsproject/pymatgen/blob/7c70c1ad5a1c489bfb5e4fbd53804630ac48d248/pymatgen/command_line/bader_caller.py" rel="nofollow noreferrer"><code>BaderAnalysis</code> class</a> in <code>pymatgen</code> for use with VASP and Cube.</li>
<li>It is based on the density rather than the wavefunction. <a href="https://mattermodeling.stackexchange.com/q/1394/5">Walter Kohn also believes the wavefunction is inappropriate for large molecules</a> but has no problem with the density.</li>
<li>Susi's comment says that Bader's method has a basis set limit; and Susi's and ksousa's answers explain that the Mulliken method doesn't have a well-defined basis set limit, so it won't converge to anything when you approach towards a complete basis set.</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>I will need help here. I only found out what &quot;charge analysis&quot; is after doing research to try to answer this question. I've never used (or learned) Mulliken, Bader, or any other type of charge analysis before.</li>
<li>Susi mentioned that Bader charges can be counterintuitive, which based on my hour of research on the topic today, seems to be a fundament of Bader's AIM concept for interpreting chemical bonds in general.</li>
<li><a href="https://mattermodeling.stackexchange.com/users/206/andrew-rosen">Andrew Rosen</a> pointed out that <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.10351" rel="nofollow noreferrer">this paper</a> describes another con.  Indeed, my reading of the article's abstract found a claim that Bader charge analysis can suggest ionic character even for covalent bonds.</li>
</ul>


================================================================================

Question: What are some recent developments in density functional theory?
Body: <p>In the book <a href="https://books.google.co.in/books?id=FzOTAwAAQBAJ&amp;printsec=frontcover&amp;dq=Materials%20Modelling%20using%20Density%20Functional%20Theory&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiHyL7svZvpAhXBF3IKHRdDDlMQ6AEIJzAA#v=onepage&amp;q&amp;f=false" rel="noreferrer">Materials Modelling Using Density Functional Theory: Properties and Predictions</a> by Feliciano Giustino, a timeline of milestones in DFT was given for achievements between 1964 and 1996:</p>
<p><span class="math-container">$$\small\begin{array}{|c|c|c|} \hline
\textbf{Year} &amp; \textbf{Milestone} &amp; \textbf{Researchers} \\ \hline
1964, 1965 &amp; \text{HK Theorem/KS Formulation} &amp; \text{Kohn, Hohenberg, Sham}  \\  
1972, 1973 &amp; \text{Relativistic DFT} &amp; \text{von Barth/Hedin, Rajapol/Callway} \\  
1980, 1981 &amp; \text{Local Density Approximiation(LDA)} &amp;\text{Ceperley/Alder, Perdew/Zunger} \\
1984 &amp; \text{TDDFT} &amp; \text{Runge, Gross}  \\
1985 &amp; \text{First Principles MD} &amp; \text{Carr, Parrinello}   \\  
1986 &amp; \text{Quasiparticle Corrections} &amp; \text{Hybertsen, Louie} \\  
1987 &amp; \text{Density Functional Perturbation Theory} &amp; \text{Baroni, Giannozzi, Testa}  \\  
1988, 1993 &amp; \text{Toward Chemical Accuracy} &amp; \text{Lee/Yang/Parr (1988), Becke (1993)} \\
1991 &amp; \text{Hubbard Correction} &amp; \text{Anisimov, Zaanen, Andersen} \\
1992, 1996 &amp; \text{Generalized Gradient Approximation} &amp; \text{Perdew/Burke/Ernzerhof} \\
\hline
\end{array}$$</span></p>
<p><strong>Has there been any milestone after 1996, or not included in the above list?</strong> <br></p>
<p>Please limit each answer to one milestone!</p>
<hr>
<p>Since there's now so many answers that it takes a very long time to scroll down to a specific one, I Have created links for the milestones that have already been explained in an answer:</p>
<ul>
<li><a href="https://mattermodeling.stackexchange.com/a/1173/5">1993 (Becke) Hybrid Functionals</a></li>
<li><a href="https://mattermodeling.stackexchange.com/a/1284/5">1995 (Casida) TD-DFRT</a></li>
<li><a href="https://mattermodeling.stackexchange.com/a/1188/5">2004 (Yanai <em>et al.</em>) Range separation</a></li>
<li><a href="https://mattermodeling.stackexchange.com/a/1212/5">2005 (Bartlett) <em>ab initio</em> DFT</a></li>
<li><a href="https://mattermodeling.stackexchange.com/a/1132/5">2006 (Grimme) Double Hybrids</a></li>
<li><a href="https://mattermodeling.stackexchange.com/a/1227/5">2007 (Becke-Johnson) Dispersion correction</a></li>
<li><a href="https://mattermodeling.stackexchange.com/a/1260/5">2013 (Kim <em>et al.</em>) Density-corrected DFT</a></li>
<li><a href="https://mattermodeling.stackexchange.com/a/1192/5">2015 (Sun <em>et al.</em>) SCAN functional</a></li>
</ul>

Best Answer: <h1>2006 (Grimme): Double hybrid functionals</h1>

<p>The timeline of milestones you have given, includes a hybrid functional called B3LYP, which mixes a Hartree-Fock exchange functional with a GGA exchange-functional. <a href="https://aip.scitation.org/doi/abs/10.1063/1.2148954?journalCode=jcp" rel="noreferrer">In 2006</a>, Stefan Grimme introduced what later became known as "double hybrid functionals", which not only mix the Hartree-Fock exchange functional with a GGA exchange-correlation functional, but also a many-body perturbation theory correction:</p>

<p><span class="math-container">\begin{equation}
E_{\textrm{xc}}=\left(1-a_\textrm{x}\right)E_\textrm{x}^{\textrm{GGA}} + a_\textrm{x}E_\textrm{x}^{\textrm{HF}} + bE_\textrm{c}^{\textrm{GGA}} + cE_\textrm{c}^{\textrm{MBPT}}.
\tag{1}
\end{equation}</span></p>

<p>For the MBPT correction, Grimme used MP2 with the Kohn-Sham orbitals and single-excitations neglected. He tried various double hybrids, and the one that he finally recommended was obtained after setting b=1-c, and then using the Becke-88 functional for <span class="math-container">$E_\textrm{x}^{\textrm{HF}}$</span> and the LYP functional for <span class="math-container">$E_\textrm{c}^{\textrm{GGA}}$</span>. He called this new functional <strong>B2PLYP</strong>.</p>

<p>By 2011 there existed several different double hybrid functionals made by various other groups, and <a href="https://pubs.rsc.org/en/content/articlelanding/2011/cp/c0cp02984j/unauth#!divAbstract" rel="noreferrer">Goerigk and Grimme created</a> an enormous dataset by combining 30 test sets, containing a grand total of 841 relative energies involving 1218 total energies, and compared the performance of 47 functionals (2 LDA, 14 GGA, 3 meta-GGA, 23 Hybrid, 5 Double Hybrid) on this test suite. <strong>Double hybrids were by far the most accurate family of functionals, with an estimated average error of only 1.8 kcal/mol:</strong></p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="https://i.sstatic.net/mpIgV.png" width="400"></p>


================================================================================

Question: How should one choose the time step in a molecular dynamics integration?
Body: <p>Choosing too small of a timestep leads to an unrealistic simulation time, whereas too big of a timestep leads to the system not being represented correctly (or, in the case of an algorithm like <a href="https://en.wikipedia.org/wiki/Constraint_(computational_chemistry)#The_SHAKE_algorithm" rel="noreferrer">SHAKE</a>, a SHAKE failure). Given a molecular system to be integrated in time, what are bases on which to decide what value of the time step is ideal? What can be used to assert that the choice is the correct one?</p>

<p>For example for hyperbolic partial differential equations, there is the <a href="https://en.wikipedia.org/wiki/Courant%E2%80%93Friedrichs%E2%80%93Lewy_condition" rel="noreferrer">Courant-Friedrichts-Lewy</a> condition which aids in deciding the necessary timestep size in order to get convergence. Is there an equivalent for the equations of motion in molecular dynamics?</p>

Best Answer: <h2>The Rule</h2>
<p>The timestep should be less than the period of the fastest vibration by at least 2. In signal processing this is known as <a href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem" rel="noreferrer">Nyquist's theorem.</a></p>
<blockquote>
<p>If a function
<span class="math-container">${\displaystyle x(t)}$</span>
contains no frequencies higher than B hertz, it is completely determined by giving its ordinates at a series of points spaced <span class="math-container">${\displaystyle 1/(2B)}$</span>  seconds apart.</p>
</blockquote>
<p>The frequency of a C-H bond is around 3000 <span class="math-container">$cm^{-1}$</span>. Converting to Hertz this is about 8.99e+13 <span class="math-container">$Hz$</span> or a period of 11 femtoseconds.</p>
<p>Therefore, we need a timestep of at least 5 fs but the integrator also introduces some error.</p>
<p>However, even when doing SHAKE (which removes most of the high-frequency vibrations) <strong>most MD stick with a 2 fs timestep</strong>. For example, see this <a href="https://www.charmm.org/ubbthreads/ubbthreads.php?ubb=showflat&amp;Number=10000" rel="noreferrer">CHARMM post</a>.</p>
<hr />
<h2>So how do we check?</h2>
<p>One way you can check if the timestep is okay is to check if there is any drift in a constant energy simulation (NVE). If there is that can mean that the integrator is not behaving time-reversibly. I ran the following with a time-step of 3 fs and no shake and the energy looks constant</p>
<p><a href="https://i.sstatic.net/0kGke.png" rel="noreferrer"><img src="https://i.sstatic.net/0kGke.png" alt="enter image description here" /></a></p>
<p>I tried to sequentially increase the time-step to demonstrate the drift. However, the energy apparently deviated so fast from constant energy that the energy blew up and OpenMM complained ( this happened at a timestep of 4 fs)</p>
<p>Lastly, I wanted to update this post with this excellent <a href="https://doi.org/10.1016/j.phpro.2014.06.027" rel="noreferrer">open-access document:</a></p>
<p>In that document they give excellent advice on the choice of the time step:</p>
<ul>
<li>fluctuations of about 1 part in 5000 of the total system energy per
twenty time steps are acceptable</li>
<li>time step size is about 0.0333
to 0.01 of the smallest vibrational period in the simulation</li>
</ul>


================================================================================

Question: Supercomputers around the world
Body: <p>For a matter modelling person, the most valuable resource is computing power. For many of us, computing power at hand limits the scale of problems we can solve. There are many national supercomputing facilities for academics. What are the resources available in each country?</p>

Best Answer: <h1>Canada: Compute Canada</h1>
<hr>
<p><strong>Before Compute Canada (Antiquity)</strong></p>
<p>Supercomputing in Canada began with several disparate  groups:</p>
<ul>
<li><strong>1997</strong>: MACI (Multimedia Advanced Comp. Infrastructure) is formed from  Alberta universities.
<ul>
<li><strong>2001</strong>: <a href="https://www.westgrid.ca/about_westgrid/history" rel="nofollow noreferrer">WestGrid</a> is formed by 7 Alberta and BC institutions (facilities launched in 2003).</li>
</ul>
</li>
<li><strong>1998</strong>: <a href="https://en.wikipedia.org/wiki/High_Performance_Computing_Virtual_Laboratory" rel="nofollow noreferrer">HPCVL</a> is formed by institutions in Toronto and eastern Ontario.</li>
<li><strong>2000</strong>: <a href="https://web.archive.org/web/20001204153800/http://www.rqchp.qc.ca:80/" rel="nofollow noreferrer">RQCHP</a> is formed in Quebec. Little information is available about it (see the URL link).</li>
<li><strong>2001</strong>: <a href="https://www.sharcnet.ca/my/about/history" rel="nofollow noreferrer">SHARCNET</a> is formed by 7 Ontario universities, by 2019 it has 19 institutions.</li>
<li><strong>2001</strong>: <a href="http://www.clumeq.ca/" rel="nofollow noreferrer">CLUMEQ</a> is formed with some institutions in Quebec (facilities launched in 2002).</li>
<li><strong>2003</strong>: <a href="https://www.ace-net.ca/who-we-are/acenet/" rel="nofollow noreferrer">ACENET</a> is formed with institutions in Atlantic (Far-Eastern) Canada.</li>
<li><strong>2004</strong>: <a href="https://www.scinethpc.ca/" rel="nofollow noreferrer">SciNet</a> is formed with University of Toronto &amp; some Ontario hospitals (facilities in 2009).</li>
</ul>
<p>Also in <strong>2003</strong>: high-speed optical link was made between WestGrid &amp; SHARCNET (West and East).</p>
<p><strong>Amalgamation into Compute Canada (CC)</strong></p>
<ul>
<li><strong>2006</strong>: WestGrid, HPCVL, SciNet, SHARCNET, CLUMEQ, RQCHP &amp; ACEnet formed CC.
<ul>
<li>\$60 million from CFI's National Platforms Fund and \$2 million/year from NSERC.</li>
</ul>
</li>
<li><strong>2011:</strong> Everything is split into just 4 regions: West, Ontario, Quebec, East:
<ul>
<li>HPCVL, SciNet &amp; SHARCNET form Compute Ontario (just a name, they're autonomous).</li>
<li>CLUMEQ &amp; RQCHP form Calcul Quebec.</li>
</ul>
</li>
</ul>

<ul>
<li><strong>2012</strong>:  CC is incorporated as a non-profit, and received $30 million more from CFI.</li>
<li><strong>2016</strong>: CC starts deploying supercomputers (HPC existed before but through the sub-groups).</li>
</ul>
<p>Throughout this time SHARCNET, and the others continue expanding to include more universities, colleges, and research institutions. ComputeOntario adds HPC4Health. Sub-groups of CC grow.</p>
<hr>
<p><strong>HPC facilities offered</strong></p>
<p>You would have to ask a separate question to go into detail about all the systems that are offered by virtue of the fact that CC is an amalgamation of several pre-existing consortia. The following is what was made available after the formation of CC:</p>
<ul>
<li><strong><a href="https://docs.computecanada.ca/wiki/Cedar" rel="nofollow noreferrer">2017: Cedar</a> (Vancouver, BC)</strong>
<ul>
<li>Enetered Top500 <a href="https://www.top500.org/system/179113/" rel="nofollow noreferrer">ranked #87</a>.</li>
<li>94,528 CPU cores, 2502 nodes.</li>
<li>1532 GPUs.</li>
<li>504T of RAM (for CPUs alone).</li>
<li>2.2P of node-local SSD space.</li>
<li><code>/home</code> 250TB total.</li>
<li><code>/scratch</code> 3.7PB total (LUSTRE).</li>
<li><code>/project</code> 10PB total.</li>
<li>Double-precision: 936 TFLOPS for CPUs, 2.744 PFLOPS for GPUs, 3.6 PFLOPS total.</li>
<li>CPU: Broadwell E5-2683/E7-4809/E5-2650, some SkyLake &amp; CascadeLake added later.</li>
<li>GPU: Originally Pascal P100 w/ 12G or 16G HBM2 RAM but 32G Volta V100 added later.</li>
<li>Interconnect: OmniPath (v1) at 100Gbit/s bandwidth.</li>
</ul>
</li>
</ul>


<ul>
<li><strong><a href="https://docs.computecanada.ca/wiki/Graham" rel="nofollow noreferrer">2017: Graham</a> (Waterloo, Ontario)</strong>
<ul>
<li>Enetered Top500  <a href="https://www.top500.org/system/179047/" rel="nofollow noreferrer">ranked #96</a>.</li>
<li>41,548 CPU cores, 1185 nodes.</li>
<li>520 GPUs</li>
<li>128T of RAM (for CPUs alone).</li>
<li>1.5P of node-local SSD space.</li>
<li><code>/home</code> 64 TB total.</li>
<li><code>/scratch</code> 3.6PB total (LUSTRE).</li>
<li><code>/project</code> 16 PB total.</li>
<li>Double-precision: Unknown.</li>
<li>CPU: Broadwell {E5-2683, E7-4850} v4, some SkyLake &amp; CascadeLake added later.</li>
<li>GPU: Pascal P100 w/ 12G HBM2 RAM but 16G Volta V100 &amp; Turing T4 added later.</li>
<li>Interconnect: Mellanox FDR 56Gb/s (GPU &amp; cloud), EDR 100Gb/s InfiniBand (rest).</li>
</ul>
</li>
</ul>




================================================================================

Question: Why do we have so many DFT codes (softwares)? Are they redundant?
Body: <p><a href="https://science.sciencemag.org/content/351/6280/aad3000" rel="noreferrer">Lejaeghere et. al</a>  studied reproducibility of DFT codes (softwares) by comparing 15 different codes employing 40 different potentials. The study concluded that most codes agree very well, with pairwise differences that are comparable to those between different high-precision experiments.</p>

<p>Why do we have so many different codes? Are they redundant?</p>

Best Answer: <p>There are many reasons why so many different scientific packages have been developed</p>

<ol>
<li><p>These packages were developed by individual researchers who were in competition with each other and also work mostly independently. It was natural that different packages sprung out of different regions (e.g. NWChem stands for NorthWest Chem, because it’s based at Pacific Northwest National Lab (PNNL)) and for different purposes or specialties (e.g. Molpro for excited-states). Also remember that this was before version control or the idea of open-access was a thing, so sharing code was not so easy. </p></li>
<li><p>The opportunity to make money with the software was also a huge draw and further enticed people to start there own company, or in contrast distance themselves from the company because of their own personal beliefs. For example, the company Gaussian has a well known history of <a href="https://en.wikipedia.org/wiki/Gaussian_(software)" rel="noreferrer">controversy</a> with some scientists over how the company was ran (perhaps even with the founder of Gaussian itself, John Pople). </p></li>
<li><p>Despite the code similarity most scientific software is very poorly managed because the developers work independently and are not computer programmers. So in many cases, people wanted to start something fresh in an attempt to make it better, or easier for them to write their own custom code. Again, open-access and object oriented programming is fairly recent. A great example of a software package that was developed to make reading and improving the code is <a href="http://openmm.org/" rel="noreferrer">OpenMM</a>. And because of this, OpenMM now has superior GPU kernels and object oriented design making it one of the best and fastest codes for molecular dynamics today.</p></li>
<li><p>There is also a difference in preferred languages. Historically scientific software was written in Fortran. Then people started using C/C++. Nowadays people are even using python!</p></li>
</ol>

<p>It is my opinion that all types of codes should be continued to be developed independently but hopefully they follow good coding practices like object-oriented programming. If that is the case then it's easy to mix and match code from different developers. </p>


================================================================================

Question: How to write my own density functional theory (DFT) code in Python?
Body: <p>I am a DFT user and at some point in the future, I would like to write my own DFT code in Python to help gain a deeper understanding of DFT. As mentioned in a <a href="https://materials.stackexchange.com/a/578/88">previous answer</a> people have written their own DFT codes to understand more deeply how the theory and algorithms work. </p>

<p>Where do I start? What are the important aspects I should consider?</p>

Best Answer: <p>Understanding, deriving, writing, testing and debugging an ab initio code can be a lenghty and tedious task. I'd like to provide a starting point for you here. If you just to it for pedagocial reasons, it might be advisable to start with the atomic problem and try to solve it with DFT. The effort for that is not too big, but it covers nearly all the nesseary physical principles to understand how one arrives at total energies and electron densities. With such a program, you can predict for example, energy levels of a certain element. Also, If you want to extend your program to treat crystalline material i. e., the written routines for the atomic problem will be of fundamental use. Core states are essential calculated like this.</p>

<p>The following websites provide some beginner-friendly introduction to the topic of simple DFT calculations of atoms.</p>

<ul>
<li>[1]: <a href="https://github.com/certik/dftatom" rel="noreferrer">https://github.com/certik/dftatom</a></li>
<li>[2]: <a href="https://github.com/theochem/tinydft" rel="noreferrer">https://github.com/theochem/tinydft</a></li>
<li>[3]: <a href="https://compphys.go.ro/dft-for-an-atom/" rel="noreferrer">https://compphys.go.ro/dft-for-an-atom/</a></li>
<li>[4]: <a href="https://www.theoretical-physics.com/" rel="noreferrer">https://www.theoretical-physics.com/</a></li>
</ul>

<p>I want to list down some the steps that are nessary for a simple solver written in python.</p>

<p><strong>Setting up the equations</strong></p>

<p>The starting point is the Kohn–Sham equation, which is the one-electron Schrödinger-like equation of a fictitious system of non-interacting particles. If you take the Born-Oppenheimer approximation into account and assume that the Kohn-Sham potential is radial symmetric, the task reduces to solve the radial part of the Kohn-Sham equation under some restrictions. Basically, you need to construct a solver that calculates the radial wave function and the eigenenergy for a given Kohn-Sham potential. For this you will need a shooting method similar to [1]. Basically, one solves the an inital value problem starting from <span class="math-container">$r=0$</span> and <span class="math-container">$r=\infty$</span> and change the energy till the solution matches at some point.</p>

<p><strong>Calculating the Kohn-Sham potential</strong></p>

<p>The effective Kohn-Sham potential is the sum of the core contributions (<span class="math-container">$1/r$</span> coloumb potential), the exchange-correlation (input here is the electron density) and the Hartree potential. In order to get the Hartree potential, you will need to solve the radial Poisson equation. </p>

<p>In order to set up a self-consistent cycle, you will need to calculate the poisson equation, which can be treaten as an inital value problem or also as an boundary value problem.</p>

<p><strong>Remark to computational efficency in Python</strong></p>

<p>The computational effort is considerable for bigger atoms. You might want to take a look some acceleration methods for Python like Numba.</p>

<p><a href="http://numba.pydata.org/" rel="noreferrer">http://numba.pydata.org/</a></p>


================================================================================

Question: Was Walter Kohn wrong about this?
Body: <p>In Kohn's <a href="https://www.nobelprize.org/uploads/2018/06/kohn-lecture.pdf" rel="noreferrer">Nobel Lecture</a>, he claimed that:</p>
<p><em>&quot;In general the many-electron wave function <span class="math-container">$\Psi(\mathbf{r}_1,\ldots,\mathbf{r}_N)$</span> for a system of <span class="math-container">$N$</span> electrons is not a legitimate scientific concept, when <span class="math-container">$N\geq N_0$</span> where <span class="math-container">$N_0\simeq10^3$</span>.&quot;</em></p>
<p>He explains this in two ways: the first is that <span class="math-container">$\Psi$</span> cannot be calculated with sufficient accuracy, and the second that is cannot be stored with sufficient accuracy. As far as I understand, what he had in mind here was comparing traditional wave function methods in quantum chemistry (e.g. configuration interaction) with density functional theory, which is based on the density rather than the wave function.</p>
<p>My question concerns Monte Carlo techniques. <a href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.73.33" rel="noreferrer">Quantum Monte Carlo</a> and stochastic extensions to traditional quantum chemistry techniques, for example <a href="https://www.nature.com/articles/nature11770?draft=collection&amp;proof=true" rel="noreferrer">Full Configuration Interaction Quantum Monte Carlo</a>, could be called wave function methods in that the central quantity is the wave function, and are being almost routinely used for very high accuracy solid state calculations with large <span class="math-container">$N$</span>. Additionally, their nice scaling properties suggest that they will be able to exploit future parallel computer resources effectively.</p>
<p>So the question is: how should we view quantum Monte Carlo techniques in view of Kohn's statement? Could we say that these techniques allow us to bypass the issue Kohn identified with wave functions by only <em>sampling</em> the wave function rather than calculating/storing it?</p>

Best Answer: <p>Kohn is easily one of my favorite humans of all time, and he was a role model to whom I looked up in great admiration for most of my academic life; in fact before this site was created, I <a href="https://area51.meta.stackexchange.com/a/31086/190792">proposed</a> that we name it after him.</p>
<p>However I completely disagree with the sentence that you have quoted. Keep in mind that even though the Nobel Lecture was in 1999, Kohn was born in 1923, so I was not alive for much of his life, and I don't know what possible connotations might have surrounded the word &quot;legitimate&quot; back in those days; but certainly the way we use the word &quot;legitimate&quot; nowadays, and every dictionary definition of legitimate I've seen, would indicate that he might have been speaking in hyperbole.</p>
<p>Let me address now, some of the specific matters in your question:</p>
<h2>We have accurate wavefunctions for systems with far more than <span class="math-container">${\small N=10^3}$</span></h2>
<p>In <a href="https://mattermodeling.stackexchange.com/a/1352/5">this answer</a> I recently mentioned that CCSD(T) with local orbital methods <a href="https://pubs.acs.org/doi/10.1021/acs.jctc.9b00511" rel="noreferrer">have calculated</a> wavefunctions for systems with up to 1023 atoms; in this case it was a lipid transfer protein (PDB: 1N89) for which I'd estimate the number of electrons is about 10,000. Kohn may have written <span class="math-container">$N\simeq 10^3$</span> instead of <span class="math-container">$N = 10^3$</span>, but the <em>order of magnitude</em> turned out to be wrong 20 years after that quote. Surely the order of magnitude will also increase again.</p>
<h2>Accuracy</h2>
<p>Energy differences with CCSD(T) or even LNO-CCSD(T) in the 4-zeta basis set that they used in the above example, are likely to be accurate to within 1.5 kcal/mol for a molecule like this, whereas <a href="https://mattermodeling.stackexchange.com/a/1132/5">DFT is unlikely to give you anything with an error less than 4 kcal/mol</a> unless you use hybrids (which by definition use wavefunctions). A good energy difference doesn't necessarily mean an accurate wavefunction, but
coupled cluster wavefunctions are not bad at all (otherwise you wouldn't be able to calculate accurate properties like polarizabilities).</p>
<h2>Storage</h2>
<p>The wavefunction in the above case is stored via cluster amplitudes, for which we can store billions (in fact trillions in the largest cases)
of them. Since cluster amplitudes appear in the argument of an exponential, we actually get CI coefficients for 100% of the non-zero determinants. So storage of a big wavefunction is no problem <em><strong>when you use a compact representation</strong></em>. Sure there will be a point at which classical computers can no longer accurately store quantum wavefunctions, but there will also be a point at which electron densities can't be stored either, so in that sense why not call the density &quot;illegitimate&quot; too? Furthermore, not being able to &quot;store&quot; the wavefunction is only a problem <strong>if using a classical storage device,</strong> whereas if you use qubits instead of bits the statement no longer has legs.</p>
<h2>Quantum Monte Carlo (QMC)</h2>
<p>FCIQMC, VMC, DMC, AFQMC, and similar methods, are all wavefunction methods. They <em><strong>are</strong></em> wavefunctions methods, whereas you wrote that they &quot;could be called wave function methods, in that...&quot;. They do in fact manage to represent wavefunctions in huge Hilbert spaces, often by taking advantage of sparsity of the wavefunction, but using a <em><strong>compact representation</strong></em> as in the case of coupled cluster, means that you don't even need a &quot;sparse&quot; representation or a &quot;stochastic sampling&quot;, you can represent the whole wavefunction by storing only the argument of the exponential. I am not trying to take shine away from QMC; I say all this as a contributor to a major FCIQMC code, and having used FCIQMC in papers <a href="https://ui.adsabs.harvard.edu/abs/2016isms.confETK07D/abstract" rel="noreferrer">with the inventor of FCIQMC</a> and also <a href="https://aip.scitation.org/doi/10.1063/1.4990673" rel="noreferrer">separately on my own</a> and even on <a href="https://arxiv.org/abs/2006.13453" rel="noreferrer">this paper</a> I put on arXiv only two days ago. FCIQMC has its place as one of the best methods for ultra-high accuracy in large multi-reference systems, but it is not needed in the proof that Kohn's statement is wrong: coupled cluster can be very accurate on even bigger systems if they don't have too much multi-reference character.</p>
<h2>Number of electrons is actually a <em>red herring</em></h2>
<p>The problem with wavefunctions when using <em>classical computers</em> has less to do with the number of electrons and more to do with its structure:</p>
<ul>
<li><strong>Bosonic wavefunctions</strong> don't suffer from the fermionic sign-problem, so you can represent them with a Hartree product, and might be able to store a larger wavefunction than you can store a density! Kohn's statement is about electronic wavefunctions, but this bullet serves as a reminder that Kohn's argument is not so much about fundamental physics and ontology, than it is about &quot;computability&quot; (you probably already agreed with this, so this bullet is more for other people).</li>
<li><strong>Fermionic wavefunctions involving only one determinant</strong> (which can still be accurate for a very single-reference system) are very simple: instead of the <span class="math-container">$\binom{M}{N}$</span> type scaling for the number of determinants in a full CI expansion, <em>you only have one term</em>.</li>
<li><strong>Fermionic wavefunctions involving many determinants but only static correlation</strong>, can be represented by <em>matrix product states</em> and calculated using the polynomially scaling DMRG.</li>
<li><strong>Fermionic wavefunctions involving many determinants but only dynamic correlation</strong>, can be represented by <em>coupled cluster ansatze</em> which are also polynomially scaling.</li>
<li><strong>Fermionic wavefunctions involving infinitely many determinants</strong> can also be represented by a <em>compact representation</em>.</li>
</ul>
<p>The problem is more about how many digits you need for each of the CI coefficients. Then you quickly see that the computational complexity of <span class="math-container">$2^N$</span> vs <span class="math-container">$N^3$</span> is irrelevant and what really matters is something more subtle, which is: how complicated is the wavefunction, not how many electrons are there.</p>
<p><em><strong>50 electrons in a CAS(50,50) is currently an absolutely <a href="https://aip.scitation.org/doi/10.1063/1.5063376" rel="noreferrer">brutal calculation</a> but 10,000 electrons in a CCSD(10000,44000) was done easily in the paper listed above. So there are cases where <span class="math-container">$N=50$</span> electrons is harder than <span class="math-container">$N=10^4$</span>, and in those highly multi-reference cases, good luck getting an accurate energy with a single-reference method like DFT!</strong></em></p>


================================================================================

Question: What is a good programming language for matter modeling?
Body: <p>What is a good programming language for matter (e.g. molecular or materials) modeling?</p>
<p>Since this is a broad field, I don't expect there to be only one answer.</p>

Best Answer: <h1>Julia</h1>
<p>The answers above allude to what some call the &quot;two-language problem&quot;. In materials science it takes the form of writing your code in Fortran for speed, and writing an interface to it in Python for sanity and interactivity. Fortran will not go away any time soon due to the massive amount of legacy code available. For new codes, there is a new option: <strong>Julia</strong>.</p>
<p>With a little bit of care (follow a few simple rules given in the &quot;performance tips&quot; section of the manual), one can easily mix Python-style high-level code and Fortran-style tight inner loops. Julia is easily interoperable with other languages, and reuse existing libraries (the Python interface, in particular, being particularly seamless). Coupled with a very good native ecosystem for numerical computing (unlike Python which is forced to hack together a core language not designed for numerics and NumPy), this makes it a particularly appealing language to use.</p>
<p>At least that has been our experience developing DFTK (<a href="https://github.com//JuliaMolSim/DFTK.jl/" rel="noreferrer">https://github.com//JuliaMolSim/DFTK.jl/</a>), a plane-wave DFT code built from scratch. The code is about one year old, ~4k LOC, and covers the basics of such codes. Had we chosen Fortran for this task, we'd still be writing the input file parser and makefile (I'm only partly joking).</p>


================================================================================

Question: Machine learning interatomic potentials for molecular dynamic simulations: are they good?
Body: <p>I know the general question of machine learning in computational chemistry has been already raised here: <a href="https://materials.stackexchange.com/q/18">What is the current status of machine learning applied to materials or molecular systems?</a></p>

<p>However, still, I'm curious, what are the pros and cons of ML force fields for MD simulations.   </p>

<p>Classical empirical potential models are fast but they are incorrect or cannot predict specific chemistry and bond formation/dissociation. Reactive potentials have some accuracy (depending on parameters) but they are slow. </p>

<p>Then... where is the position of ML potentials? Are they accurate? Or fast? Or both? I read some papers by Podryabinkin <em>et. al.</em><sup>[1]</sup> and by Deringer <em>et al.</em>.<sup>[2]</sup></p>

<p>However, as a person who never tried ML potentials before, it is really hard to judge or feel the state of the ML force field. </p>

<p>So, if anyone tried various interatomic potentials including ML force fields (in Gromacs or Lammps or any platforms) may I ask how much accurate/fast are they, and what is the advantage/disadvantage of ML force field? Is this easy/hard to learn, or easy/hard to get "good parameter", etc.. </p>

<h3>References:</h3>

<ol>
<li>Podryabinkin, E. V.; Tikhonov, E. V.; Shapeev, A. V.; Oganov, A. R. Accelerating crystal structure prediction by machine-learning interatomic potentials with active learning. <em>Phys. Rev. B</em> <strong>2019,</strong> <em>99</em> (6), No. 064114. <a href="https://doi.org/10.1103/PhysRevB.99.064114" rel="noreferrer">DOI: 10.1103/PhysRevB.99.064114</a>.</li>
<li>Deringer, V. L.; Caro, M. A.; Csányi, G. Machine Learning Interatomic Potentials as Emerging Tools for Materials Science. <em>Adv. Mater.</em> <strong>2019,</strong> <em>31</em> (46), 1902765. <a href="https://doi.org/10.1002/adma.201902765" rel="noreferrer">DOI: 10.1002/adma.201902765</a>.</li>
</ol>

Best Answer: <p>(Expanding my comment into an answer)
When ML-based forcefields are compared to classical forcefield directly, I think we miss the most important points. ML-based models have several advantages:</p>

<ul>
<li>They do not need an <em>a priori</em> "correct" description of the system, nor are they limited by the applicability of specific theories to your system. Classical force fields are very sophisticated for problems like biological molecules or water simulation, but once you start simulating transition metal compounds or their oxides, suddenly you have to invent new types of forcefields. ML-based models are flexible; you do not have to invent it every single time. Also, they are flexible in the sense that their accuracy is not limited by the model you choose: it is possible to improve their accuracy by training them on more data.</li>
</ul>

<p>Therefore, theoretically, one can reach CCSD(T) or whatever level of accuracy if enough data is previously collected <a href="https://doi.org/10.1038/s41467-019-10827-4" rel="noreferrer">[1]</a>. </p>

<ul>
<li>We generally know (and measure) the errors. It is not really a theoretical limitation, but as far as I know, MM software like Gromacs or NAMD is not telling you when your system is far from the training set. </li>
</ul>

<p>Why is it important?
We can do things like new forcefields on the fly: one can start an MD simulation with DFT, and parallel train a forcefield. Continue the simulation with ML-forcefield and calculate DFT only when something new is happening (a structure that is far from the training set)<a href="https://www.vasp.at/wiki/index.php/Machine_learning_force_field:_Theory" rel="noreferrer">[2]</a>. </p>

<p>Are they slow compared to a GROMACS force field? Yes.
But it is mostly irrelevant in cases where there is no classic force field at all or where more accuracy is needed or when you speed up a QM-MD simulation x100...0(?) times.</p>

<ol>
<li>Smith, J.S., Nebgen, B.T., Zubatyuk, R. et al. "Approaching coupled cluster accuracy with a general-purpose neural network potential through transfer learning." Nat Commun 10, 2903 (2019). DOI: 10.1038/s41467-019-10827-4</li>
<li>Vasp.at. 2020. Machine Learning Force Field: Theory - Vaspwiki. [online] Available at: <a href="https://www.vasp.at/wiki/index.php/Machine_learning_force_field:_Theory" rel="noreferrer">https://www.vasp.at/wiki/index.php/Machine_learning_force_field:_Theory</a> [Accessed 18 May 2020]</li>
</ol>


================================================================================

Question: In molecular mechanics, how are van der Waals forces modelled?
Body: <p>In terms of energy, how are van der Waals forces modelled (are there formulas/laws that govern these)?</p>

Best Answer: <p>First I will try to directly answer this question:</p>

<blockquote>
  <p>In terms of energy, how are van der Waals forces modelled (are there
  formulas that govern these)?</p>
</blockquote>

<p>The most common way to model the potential energy between two ground state (S-state) atoms that are far apart, is by the London dispersion formula:</p>

<p><span class="math-container">$$V(r) = -\frac{C_6}{r^6}$$</span></p>

<p>where <span class="math-container">$C_6$</span> depends on the dipole polarizabilities (<span class="math-container">$\alpha_1$</span> and <span class="math-container">$\alpha_2$</span>) of the two atoms. One decent approximation is the Slater-Kirkwood formula:</p>

<p><span class="math-container">$$C_6 \approx \frac{3}{2}\frac{I_1I_2}{I_1+I_2}\alpha_1\alpha_2$$</span>
where <span class="math-container">$I_1$</span> and <span class="math-container">$I_2$</span> are the first ionization potentials of the atoms.</p>

<p>However the London dispersion potential is not the only one:</p>

<ul>
<li>The Casimir-Polder potential is used in the relativistic regime, and it's often closer to <span class="math-container">$C_7/r^7$</span></li>
<li>The resonance dipole-dipole potential: <span class="math-container">$C_3/r^3$</span> used between S-state and P-state atoms</li>
<li>If one particle is charged you can get:  <span class="math-container">$C_4/r^4$</span> as in <a href="https://arxiv.org/pdf/1410.4895.pdf" rel="noreferrer">Eq. 2 of this paper of mine</a>.</li>
</ul>

<blockquote>
  <p>In molecular mechanics, how are van der Waals forces modelled?</p>
</blockquote>

<p>Most often the <span class="math-container">$C_6/r^6$</span> formula is used, which is reasonable unless dealing with ions, or excited states, or extremely long-range (relativistic) situations. However this formula is for two particles that are very far apart, and we need a force to go the other way when particles are too close to each other, and that is where the 1924 Lennard-Jones potential enters (it has already been written by AloneProgrammer, but in a different way):</p>

<p><span class="math-container">$$V(r) = \frac{C_{12}}{r^{12}}-\frac{C_6}{r^6}$$</span></p>

<p><strong>While the <span class="math-container">$r^6$</span> has rigorous theoretical foundations, the <span class="math-container">$r^{12}$</span> does not</strong>, but in molecular mechanics calculations, this function might need to be evaluated billions of times, so it is convenient that once you've calculated <code>temp=r^6</code> in your computer program, you can just do <code>temp2=temp*temp</code> to get <span class="math-container">$r^{12}$</span>. This might sound crazy now, but the earliest computers were so slow that being able to reuse the calculation <span class="math-container">$r^6$</span> in order to take a short cut to calculate <span class="math-container">$r^{12}$</span>, actually made a big difference, and the most high-performance codes, even today, still use this short-cut.</p>

<hr>

<p>However, now we have to address the <a href="https://materials.stackexchange.com/questions/9/in-molecular-mechanics-how-are-van-der-waals-forces-modelled#comment409_9">comment of Charlie Crown</a>:</p>

<blockquote>
  <p>LJ and EXP6 are merely two of the most cliche ways. They are both very
  simple and easy to use, but neither is all that great. LJ is too steep
  at close distances and EXP6 is unphysical at small distances</p>
</blockquote>

<p>This is exactly what I told you: <span class="math-container">$C_6/r^6$</span> is only valid when the atoms are very far apart, and <span class="math-container">$C_{12}/r^{12}$</span> has no physical foundation at all (it is simply convenient since <span class="math-container">$(r^6)^2=r^{12}$</span>.</p>

<p>AloneProgrammer gave the Morse potential (from 1929) which is actually really good when the atoms are closer together:</p>

<p><span class="math-container">$$V(r) = D_e\left(1 - e^{\beta(r-r_e)}\right)^2$$</span></p>

<p>where <span class="math-container">$r_e$</span> is the inter-atomic distance at equilibrium, <span class="math-container">$D_e$</span> is the "Depth at equilibrium" and <span class="math-container">$\beta$</span> controls the shape. While this is good at short-range, it is bad at long-range, because if you work out the asymptotic behaviour as <span class="math-container">$r\rightarrow \infty$</span> you will see that it decays exponentially, when in fact we know it should decay with an inverse-power (proportional to <span class="math-container">$1/r^6$</span>), and exponentials behave very differently from inverse-power functions.</p>

<p><strong>The solution is the <a href="https://en.wikipedia.org/wiki/Morse/Long-range_potential" rel="noreferrer">Morse/long-range function</a> or MLR, which was introduced by <a href="https://aip.scitation.org/doi/10.1063/1.3264688" rel="noreferrer">Bob LeRoy and myself in 2009</a></strong> </p>

<p>It looks exactly like the Morse potential when <span class="math-container">$r$</span> is close to <span class="math-container">$r_e$</span> (when the system is close to equilibrium). But if you calculate the form of the function as <span class="math-container">$\lim\limits_{r\rightarrow \infty}$</span>, you literally get <span class="math-container">$V(r) \rightarrow -u(r)$</span> where <span class="math-container">$u(r)$</span> can be anything you want: <span class="math-container">$C_6/r^6$</span>, <span class="math-container">$C_3/r^3$</span>, <span class="math-container">$C_4/r^4$</span>, etc.</p>

<p><strong>Therefore the MLR potential is Morse-like near equilibrium, and LJ-like when far from equilibrium, which is exactly what Charlie Crown said was problematic if you use pure Morse or pure Lennard-Jones.</strong></p>

<p>The MLR potential isn't used in mainstream molecular mechanics calculations, because evaluating the function would be slower than simply using the <span class="math-container">$(r^6)^2=r^{12}$</span> trick (which makes calculations very fast when using the LJ potential). The MLR potential is more accurate though, and solves the problem of the LJ being wrong at equilibrium and the Morse being wrong when far from equilibrium, so it can give more accurate results. Often there's so many approximations going on in molecular mechanics that it doesn't hurt to use the LJ potential which both other answers mentioned already. The MLR tends to be used for high-precision spectroscopy more than for molecular mechanics, but it's an option if one wanted more accurate results.</p>


================================================================================

Question: What are the positives and negatives of periodic DFT codes that don&#39;t use plane-wave basis sets?
Body: <p>Most periodic density functional theory (DFT) codes use plane-wave basis sets in conjunction with three-dimensional periodic boundary conditions. In contrast, for molecular systems of finite size, Gaussian basis sets are often used. The former are quite efficient for periodic systems, but more expensive methods such as hybrid functionals become intractable for large system sizes. The latter is well-suited for finite clusters but rarely is appropriate for periodic systems.</p>

<p><a href="https://www.cp2k.org/" rel="noreferrer">CP2K</a> uses a mixed Gaussian and plane-wave approach (GAPW) for periodic systems. <a href="https://www.crystal.unito.it/index.php" rel="noreferrer">Crystal</a> models periodic systems using atom-centered Gaussian functions. <a href="https://parsec.oden.utexas.edu/" rel="noreferrer">PARSEC</a> expresses wavefunctions in real space, avoiding explicit basis sets.</p>

<p>What are some positives and negatives of these alternate approaches, when compared to more conventional periodic DFT packages? </p>

Best Answer: <p>Pure plane-wave basis sets have the following advantages when used in periodic DFT (or HF) simulations:</p>
<ul>
<li>Orthogonal</li>
<li>Computationally simple (operators with derivatives are particularly straightforward)</li>
<li>Low-scaling methods allow easy transformations between real- and reciprocal-space</li>
<li>Basis set size does not scale with electron count</li>
<li>Independent of atomic positions</li>
<li>Their accuracy is controlled with a single parameter, and it is systematically improvable</li>
<li>Model all space with equal accuracy</li>
</ul>
<p>However there are some disadvantages:</p>
<ul>
<li>Basis set size scales with simulation volume - vacuum is not &quot;free&quot;</li>
<li>Basis sets are usually large &quot;per atom&quot; - it is not usually practical to construct the full Hamiltonian explicitly (or any other operator) and you must solve the eigenequations iteratively</li>
<li>Model all space with equal accuracy - no scope for focusing effort on &quot;interesting&quot; regions</li>
<li>Extend throughout space (no simple real-space truncation possible in integrals - e.g. the Fock operator is computationally expensive)</li>
</ul>
<p>In contrast, (periodic) local basis sets generally have the following advantages:</p>
<ul>
<li>Basis set size does not scale with simulation volume</li>
<li>Basis set is typically <em>compact</em>, with few basis states &quot;per atom&quot;</li>
<li>Model space with variable accuracy - basis can be tuned to improve representation in regions of interest, and reduce accuracy in uninteresting regions</li>
<li>Basis functions are local, and real-space truncations are straightforward in multi-basis set integrals</li>
<li>Some basis choices (e.g. Gaussians) allow analytic integration of some energy terms</li>
</ul>
<p>and the following disadvantages:</p>
<ul>
<li>Non-orthogonal</li>
<li>Computationally complicated (often)</li>
<li>They depend on the atomic positions (leading to Pulay forces)</li>
<li>Basis set size scales with electron count</li>
<li>Model space with variable accuracy - need to decide a priori where to spend the computational effort, i.e. which regions are &quot;interesting&quot;</li>
<li>No single parameter to control their accuracy; not always systematically improvable</li>
<li>Some basis set choices are not easy to transform between real- and reciprocal-space</li>
</ul>
<p>Roughly speaking, plane-wave methods are efficient when computing and applying the terms of the Hamiltonian, but lead to a much larger dimensionality in the eigenvalue problem and must compute a subset of states; local basis sets often take more time constructing the eigenvalue problem, but it is quite compact and can be solved directly (e.g. with LAPACK) to generate the full eigenspectrum.</p>
<p>There is no reason in principle why you cannot use a hybrid approach (e.g. like CP2K) whereby you transform to a different basis set to perform certain parts of the calculation. You can gain some of the advantages of both, but unfortunately you may suffer from some of the disadvantages of both as well -- for example, when switching from plane-waves to Gaussians the Fock operator becomes much more compact and computationally tractable, but you need to ensure that there are Gaussians in all the &quot;interesting&quot; regions of space. The computational cost of the transformation can also be problematic.</p>
<p>Two final comments:</p>
<ul>
<li><p>&quot;Muffin tin&quot; programs use mixed basis sets, using localised basis functions to represent the regions of space near nuclei, and plane-waves in the interstitial regions. This is efficient in both regions, but matching the descriptions at the boundaries can be tricky</p>
</li>
<li><p>Wannier transformations allow a &quot;lossless&quot; transformation of the occupied Kohn-Sham states from a plane-wave representation to a local representation. However, the transformation scales cubically and is not well-conditioned, usually relying on a &quot;guess&quot; transformation which would be generated from a local basis set (typically LCAO)</p>
</li>
</ul>


================================================================================

Question: What are the freely available crystal-structure visualization softwares?
Body: <p>I use <a href="https://jp-minerals.org/vesta/en/download.html" rel="noreferrer">VESTA</a> mostly for crystal structure visualizations.</p>
<p>What other options are available?</p>

Best Answer: <h2>ASE</h2>
<p>The Atomic Simulation Environment (ASE) has visualization capabilities. It is a python environment that allows direct interaction with a lot of atomistic simulation tools.</p>
<p><a href="https://wiki.fysik.dtu.dk/ase/" rel="nofollow noreferrer">https://wiki.fysik.dtu.dk/ase/</a></p>
<p>Visual Molecular Dynamics (VMD) and XCrySDen are the two tools I personally work with the most time. They produce nice graphics and can visualize densities etc. VMD also has rendering capabilites by invoking e.g. PovRay.</p>
<p><a href="https://www.ks.uiuc.edu/Research/vmd/" rel="nofollow noreferrer">https://www.ks.uiuc.edu/Research/vmd/</a></p>
<p><a href="http://www.xcrysden.org/" rel="nofollow noreferrer">http://www.xcrysden.org/</a></p>
<p>The Visualization Toolkit (VTK) and its graphical interface ParaView are capable of rendering truly stunning images and animations but have a rather steep learning curve.</p>
<p><a href="https://vtk.org/" rel="nofollow noreferrer">https://vtk.org/</a></p>
<p><a href="https://www.paraview.org/" rel="nofollow noreferrer">https://www.paraview.org/</a></p>
<p>I am sure there are a lot more options than this though.</p>


================================================================================

Question: What is a good replacement for Gaussian?
Body: <p>When I started studying computational chemistry (<em>circa</em> 2007), my supervisor used to tell me about the controversy surrounding Gaussian, Inc. regarding the banning of researchers involved in the development of competing software (there is a very famous <a href="https://www.nature.com/articles/429231a" rel="noreferrer">paper</a> in Nature about that). He didn't care much about it, said it was possibly a hoax and openly defied Gaussian's licensing terms because he thought he would not be punished and that the scientists who created the anonymous website <em>bannedbygaussian.org</em> were just disseminators of fake news.</p>
<p>Eventually came the day  when my advisor published an article in which he compared the computational efficiency of Spartan with that of Gaussian in simulating a PAH he was studying. A few months later, he was surprised to receive a notification (when it was time to renew the license, if I remember correctly) that both he and his coworkers were no longer allowed to use Gaussian. At the time he was already retiring and was not too worried (he died in 2018). However, he deeply regretted doubting the anonymous community of scientists who created <em>bannedbygaussian.org</em>. By that time, I had just left the academic world, but in any case I promised myself that I would not use Gaussian software anymore.</p>
<p>Long story short, recently I am considering returning to computational chemistry studies and I am not aware of software that can replace Gaussian. As I have not been in contact with matter modeling for many years, I would like a <strong>suggestion</strong> from the community so as not to have to break my promise, honoring my late advisor.</p>


Best Answer: <h1>ORCA</h1>
<ul>
<li><p>Official website: <a href="https://orcaforum.kofo.mpg.de/app.php/portal" rel="noreferrer">https://orcaforum.kofo.mpg.de/app.php/portal</a></p>
</li>
<li><p>What is ORCA?</p>
</li>
</ul>
<blockquote>
<p>ORCA is a general-purpose quantum chemistry program package that features virtually all modern electronic structure methods (density functional theory, many-body perturbation, and coupled-cluster theories, and multireference and semiempirical methods). It is a flexible, efficient, and easy-to-use general-purpose tool for quantum chemistry with specific emphasis on spectroscopic properties of open-shell molecules. It features a wide variety of standard quantum chemical methods ranging from semiempirical methods to DFT to single- and multireference correlated ab initio methods. It can also treat environmental and relativistic effects.</p>
</blockquote>
<blockquote>
<p>ORCA uses standard Gaussian basis functions and is fully parallelized. Due to the user-friendly style, ORCA is considered to be a helpful tool not only for computational chemists but also for chemists, physicists, and biologists that are interested in developing the full information content of their experimental data with help of calculations.</p>
</blockquote>
<ul>
<li><p>Which can ORCA do?</p>
<ul>
<li>geometry optimizations;</li>
<li>spectroscopic parameters;</li>
<li>Hartree Fock, DFT...</li>
</ul>
</li>
</ul>
<p>PS: You can find more choices from <a href="https://en.wikipedia.org/wiki/List_of_quantum_chemistry_and_solid-state_physics_software" rel="noreferrer">this Wikipedia link</a>.</p>
<p>Hope it helps.</p>


================================================================================

Question: Is there a free package with robust CASSCF functionality?
Body: <p>Among free software packages, which one has robust CASSCF functionality? The FCI solver can be implemented in either way, e.g., direct solver, DMRG, etc.</p>

Best Answer: <h2>OpenMOLCAS</h2>
<p>I'm surprised that the other two answers were given without yet any mention of OpenMOLCAS yet! <br>
There is a recent paper<sup>[1]</sup> about it for which I happen to be one of the many co-authors.</p>
<p>MOLCAS has the quality of a commercial software like MOLPRO, since it was a commercial software that people paid big money for since the 1980s, but recently an open source branch called OpenMOLCAS was made freely available.</p>
<p>Furthermore, (Open)MOLCAS is the only program with CAS in its name, suggesting that it caters well to CASSCF calculations. Indeed, it originated in the 1980s in the group of Björn Roos, and Roos in fact <a href="http://www.molcas.org/roos.html" rel="noreferrer">described the CASSCF method in the year 1980</a>  and later in the same decade developed the CASPT2 method.</p>
<p>CASSCF, RASSCF, and GASSCF are all implemented well in OpenMOLCAS, as it is used mainly by people who specialize in active-space calculations.</p>
<p><strong>References</strong></p>
<ol>
<li><a href="https://doi.org/10.1021/acs.jctc.9b00532" rel="noreferrer"><em>J. Chem. Theory Comput.</em> <strong>2019,</strong> <em>15</em> (11), 5925–5964</a></li>
</ol>


================================================================================

Question: What correlation effects are included in DFT?
Body: <p>In wavefunction methods the accuracy of the description of a system of electrons can be improved systematically starting from a reference, usually a Hartree-Fock wavefunction. This difference between the HF energy and the true non-relativistic energy is called (Coulomb) correlation energy and, as far as I know, it can be divided in, at least, two types: static correlation and dynamic correlation. </p>

<p>Dynamic correlation can be described by perturbative methods or Coupled Cluster theory, while static correlation needs multi-reference descriptions. Although the definition of dynamic and static correlation can be ambiguous, in some cases the effects of static correlation can be "separated" from dynamic effects and it becomes important to know what correlated method is needed.</p>

<p>In DFT, however, it seems that the amount of dynamic correlation, introduced by the exchange-correlation (XC) potential, is unspecified. Moreover, Kohn-Sham orbitals are constructed such that they reproduce the real electron density, which means that KS orbitals account some correlation effects. Also, the KS exchange energy is based only on a <em>single</em> determinant, thus, one can think that static effects are neglected. However, I'm not really sure about that sentence.</p>

<p>So, the question is, <strong>what correlation effects are included in DFT?</strong> </p>

Best Answer: <p>By definition all of them, because Density Functional Theory is in principle exact. </p>

<p>Becke states:<sup>[1]</sup></p>

<blockquote>
  <p>Density-functional theory (DFT) is a subtle, seductive, provocative business. Its basic premise, that all the intricate motions and pair correlations in a many-electron system are somehow contained in the total electron density alone, is so compelling it can drive one mad.<br>
  [...]<br>
  Let us introduce the acronym DFA at this point for “density-functional approximation.” If you attend DFT meetings, you will know that Mel Levy often needs to remind us that DFT is exact. The failures we report at meetings and in papers are not failures of DFT, but failures of DFAs.</p>
</blockquote>

<p>DFT does model the exact electron density, hence all electron correlation. The problem becomes which of part of correlation is treated by the particular DFA, and how. Given the large number of approaches and parameterisations, this is probably too much to handle on this platform.<sup>[2]</sup></p>

<p>As you also state that the definition of dynamic and static correlation can be ambiguous, it is at first only really meaningful for wave-function based approaches. As you separate the electron density differently in DFA, you have even more ambiguity.</p>

<p>You can see that in real life, when you calculate 'strongly-correlated' systems with DFT, and surprisingly they manage quite well, where wave-function based methods completely fail.<br>
Another point in favour of this is the broken symmetry approach often taken, see for example in the popular Orca input library: <a href="https://sites.google.com/site/orcainputlibrary/dft/broken-symmetry-dft" rel="noreferrer">broken symmetry DFT</a>. </p>

<h3>References:</h3>

<ol>
<li><p>Becke, A. D. Perspective: Fifty years of density-functional theory in chemical physics. <em>J. Chem. Phys.</em> <strong>2014,</strong> <em>140</em> (18), 18A301. <a href="https://doi.org/10.1063/1.4869598" rel="noreferrer">DOI: 10.1063/1.4869598</a>.</p></li>
<li><p>I have written a bit more about functionals, their shortcomings and advantages, etc. at Chemistry.se: <a href="https://chemistry.stackexchange.com/q/27302/4945">DFT Functional Selection Criteria</a>.</p></li>
</ol>


================================================================================

Question: Where/when did the fields of Operations Research and Spin Physics or Molecular Dynamics begin to cross-pollinate?
Body: <p><a href="https://or.stackexchange.com/">Operations Research</a> is a field of mathematics in which <strong>optimal or near-optimal solutions are sought for complicated problems.</strong></p>
<p>In the modeling of materials, we often optimize Ising models, in which the discrete variables on <span class="math-container">$\{-1,1\}$</span> or <span class="math-container">$\{0,1\}$</span> represent
<em><strong>spin-up</strong></em> particles (<span class="math-container">$\,\uparrow\,$</span>) and <em><strong>spin-down</strong></em> particles (<span class="math-container">$\,\downarrow\,$</span>), and we seek the optimal configuration of spins that minimizes the energy (<em>i.e.</em> the ground state of the system). The lowest-energy configuration can vary greatly, depending on the couplings between the spins, and this can give rise to different types of <strong>magnetism</strong> in the materials, such as <em>ferromagnetism</em>: <img src="https://i.sstatic.net/bBIor.png" width="20">, <em>anti-ferromagnetism</em>:
<img src="https://i.sstatic.net/MnVB1.png" width="30">, or one of <em>many</em> other types of <em>phases</em>.</p>
<p>The <strong>2001</strong> book <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.382.8505&amp;rep=rep1&amp;type=pdf" rel="noreferrer">Optimization Algorithms in Physics</a> by <a href="https://scholar.google.com/citations?user=bNlwNPYAAAAJ&amp;hl=en" rel="noreferrer">Alex Hartmann</a> and <a href="https://scholar.google.com/citations?user=7koADWYAAAAJ&amp;hl=en" rel="noreferrer">Heiko Rieger</a> claims in the introduction to be a &quot;unique&quot; book in cross-pollinating algorithms that are popular in operations research, with algorithms and applications in physics. For example, they mention <em><strong>simulated annealing</strong></em>, which was born in the physics community, but is now used by mathematicians to optimize non-physics problems such as optimizations in graph-cut problems or in finance, and <em><strong>parallel tempering</strong></em> which was introduced in a physics journal in 1986 and implemented in GROMACS, LAMMPS, CHARMM, and AMBER (materials modeling software), but was used for &quot;maximum likelihood&quot; problems by statistician C.J. Geyer <a href="http://www.stat.umn.edu/geyer/thesis/" rel="noreferrer">in 1991</a>. However, they also mention the &quot;blossom algorithm&quot; of computer scientist Jack Edmonds, which helps to solve Ising problems very efficiently when restricted to certain scenarios with <em>planar</em> spin configurations.</p>
<p>While this book might indeed have been the first book to bring so much from the operations research community into the materials modeling context, and <em>vice versa</em>, most of the references are from much earlier. This got me curious to find out, <em>when did the separately growing operations research community start adopting Ising-optimization methods from the physics community and vice versa?</em></p>
<ul>
<li><p>I found a book from one decade earlier, called &quot;<a href="https://books.google.ca/books?id=aWWSbE-eSwEC&amp;source=gbs_book_similarbooks" rel="noreferrer">Spin glasses and related problems</a>&quot; (<strong>1991</strong>).<br>
But then I found proceedings of a colloquium on <a href="https://books.google.ca/books?id=s6zvAAAAMAAJ&amp;source=gbs_similarbooks" rel="noreferrer">Spin Glasses, Optimization and Neural Networks</a> (<strong>1986</strong>).<br></p>
</li>
<li><p>However the mathematical optimization literature about developments such as Jack Edmond's blossom algorithm (<strong>1961</strong>), makes no mention of Ising models (which <a href="https://link.springer.com/article/10.1007%2FBF02980577" rel="noreferrer">physicists were trying to optimize since the <strong>1920s</strong></a>) or simulated annealing type algorithms which Metropolis <em>et al.</em> published in <a href="https://aip.scitation.org/doi/10.1063/1.1699114" rel="noreferrer">Journal of Chemical Physics in <strong>1953</strong></a>.</p>
</li>
</ul>
<p><strong>Between <em>circa</em> 1961 and 1986, these two communities somehow learned about each other, and I am curious to know which paper or conference might get credit for it?</strong></p>

Best Answer: <p>The cross-pollination and exchange of ideas between the operations research and physics communities in the context of Ising optimization methods can be traced back to several key papers and conferences. While it is difficult to pinpoint a single paper or conference that facilitated this exchange, there were significant developments during the period you mentioned (between circa 1961 and 1986) that contributed to the mutual understanding and adoption of techniques.</p>
<p>One important milestone in this regard was the 1970 paper titled &quot;Optimization by Simulated Annealing&quot; by Scott Kirkpatrick, C. Daniel Gelatt, and Mario P. Vecchi. This paper introduced the simulated annealing algorithm, which is a stochastic optimization method inspired by the annealing process in metallurgy and physics. Simulated annealing was initially developed for solving optimization problems in physics, particularly Ising models, but it quickly gained attention in the operations research community due to its effectiveness in finding near-optimal solutions for complex combinatorial problems. Another influential work was the 1983 paper &quot;Optimization by Simulated Annealing: Quantitative Studies&quot; by S. Kirkpatrick, C. D. Gelatt, Jr., and M. P. Vecchi. This paper provided further analysis and empirical results demonstrating the effectiveness of simulated annealing as an optimization method. It helped solidify the understanding of simulated annealing as a powerful technique for solving a wide range of optimization problems, leading to its adoption and exploration by researchers in both operations research and physics.</p>
<p>In terms of conferences, the colloquium on &quot;Spin Glasses, Optimization and Neural Networks&quot; held in 1986 at the University of Rome &quot;La Sapienza&quot; played a crucial role in bringing together researchers from different disciplines. This event provided a platform for discussions and presentations on spin glasses, optimization, and neural networks, fostering interdisciplinary collaboration and knowledge exchange. The proceedings of this colloquium, which you mentioned, contain valuable contributions from both the operations research and physics communities and further contributed to the cross-fertilization of ideas. It is worth noting that during this period, researchers from both communities were actively exploring optimization techniques, and the exchange of ideas happened gradually through various channels such as publications, conferences, and collaborations. The introduction of simulated annealing and its subsequent success in solving optimization problems played a significant role in drawing attention to the potential benefits of adopting methods from physics in the operations research domain. While it is challenging to attribute the entire cross-pollination process to a single paper or conference, the works and events mentioned here were instrumental in facilitating the exchange of ideas between the operations research and physics communities, leading to the mutual adoption of Ising optimization methods and the development of novel techniques in both fields.</p>


================================================================================

Question: What is band inversion and how to recognize it in band structure?
Body: <p>Band inversion is a key ingredient of a topologically nontrivial material<span class="math-container">$^1$</span>. What is band inversion? How to recognize it in a band structure? What conclusions can I infer if I observe band inversion in a material?</p>

<p><strong>References</strong></p>

<ol>
<li>Zhiyong Zhu, Yingchun Cheng, and Udo Schwingenschlögl. "Band inversion mechanism in topological insulators: A guideline for materials design." <a href="https://doi.org/10.1103/PhysRevB.85.235401" rel="noreferrer">Physical Review B <strong>85</strong> 235401 (2012)</a>.</li>
</ol>

Best Answer: <p>Topological materials form a broad family including insulators, semimetals, and superconductors, of which perhaps the best known are topological insulators. For concreteness, I will focus on topological insulators as these are the ones specifically mentioned in the question.</p>
<p><strong>Topological insulator.</strong> A topological insulator is an insulator whose Hamiltonian cannot be adiabatically connected to the atomic limit. Adiabatic means that, when tuning some external parameter (e.g. pressure, temperature) to change the Hamiltonian, the process is slow enough that the material stays in its ground state throughout.</p>
<p><strong>Qualitative picture.</strong> Consider an insulator like diamond. Imagine you pull the carbon atoms apart, taking each individual atom to one edge of the Universe, so that you end up with isolated carbon atoms -- this is what we call the <em>atomic limit</em>. In diamond, it is possible to do this process without closing the bulk band gap, so we say that bulk diamond is adiabatically connected with the atomic limit, and as such diamond is a normal insulator. Now repeat the same with another material, say <span class="math-container">$\ce{Bi_2Se_3}$</span>, which in its crystalline form is also a bulk insulator. In this case, when you pull the system apart to reach the atomic limit it is impossible to do it without closing the bulk band gap. As such, <span class="math-container">$\ce{Bi_2Se_3}$</span> is <em>not</em> adiabatically connected to the atomic limit, and we call it a topological insulator.</p>
<p>So what is the reason for this picture of topological insulators? Broadly speaking, the electron wave function &quot;twists&quot; as you cross the Brillouin zone in a topological material. The electron wave function in the atomic limit is never twisted, so the twist needs to be undone when getting to that limit and this is what the band closure accomplishes.</p>
<p><strong>Topological invariants.</strong> More rigorously, these twists can be characterized by so-called topological invariants, and their mathematical form depends on the type of topological material you are looking at, but are mostly related to Berry phase-like quantities that measure the evolution of the electron wave function as you cross the Brillouin zone. As an example, for 3-dimensional topological insulators, the topological invariant is a set of 4 numbers that can take one of two values (<span class="math-container">$\mathbb{Z}_2$</span> classification) and can be calculated by following the evolution of Wannier charge centers across the Brillouin zone as described in this paper<sup>[1]</sup>, or if the system has inversion symmetry by simply calculating the parity of the eigenstates at special points in the Brilloin zone as described in this paper<sup>[2]</sup>. Another well-known example is for Chern insulators (which roughly speaking are 2D materials with topological and magnetic orders) in which the topological invariant is the Chern number (an integer, so <span class="math-container">$\mathbb{Z}$</span> classification) obtained by integrating the Berry curvature over the Brillouin zone.</p>
<p><strong>First principles calculations.</strong> So how do you figure out what the topological order of a material is? The way to do it is to calculate the corresponding topological invariant. As these calculations involve Berry phase-like quantities, the usual route in first principles calculations is to use Wannier functions, and packages that implement these calculations and have interfaces to major DFT packages include <a href="http://z2pack.ethz.ch/doc/3.1/index.html" rel="noreferrer">Z2Pack</a> and <a href="http://www.wanniertools.com" rel="noreferrer">WannierTools</a>. There are also several databases (e.g. the <a href="https://www.topologicalquantumchemistry.org/#/" rel="noreferrer">Topologial Materials Database</a> or <a href="http://materiae.iphy.ac.cn" rel="noreferrer">Materiae</a>) that include the topological classification of <em>many</em> materials based on semilocal DFT. These databases are excellent starting points to figure out the possible topological order of a material, although it is known that higher levels of theory (e.g. hybrid functionals, <span class="math-container">$GW$</span>) may lead to different results, so I recommend further analysis.</p>
<p><strong>Band inversion.</strong> Having said all this, where does &quot;band inversion&quot; enter? The usual strategy to <em>get</em> a topological insulator, and already used in the seminal paper by Kane and Mele<sup>[3]</sup>, is to induce the wave function twist using spin-orbit coupling. This is a very simple schematic for <span class="math-container">$\ce{Bi_2Se_3}$</span>:</p>
<p><a href="https://i.sstatic.net/u87AG.jpg" rel="noreferrer"><img src="https://i.sstatic.net/u87AG.jpg" alt="Simple two-state schematic of spin orbit coupling of Bi2Se3. Spin orbit coupling creates a gap between the conduction band (upper, red) and the valence band (lower red)" /></a></p>
<p>Without spin-orbit coupling (left), the &quot;conduction band&quot; shown in red is made of <span class="math-container">$\ce{Bi}$</span> <span class="math-container">$p_z$</span> orbitals, and the &quot;valence band&quot;, shown in blue, of <span class="math-container">$\ce{Se}$</span> <span class="math-container">$p_z$</span> orbitals. However, the bands overlap and the system has no band gap. When spin-orbit coupling is included (right), a gap opens at the crossing points, and now we have a proper conduction band that has contributions from the band that made the valence band originally (blue) and <em>vice versa</em> (imagine you continue the valence band in blue with a dashed line, it would then &quot;join up&quot; with the part of the conduction band in blue). This is called a band inversion. In the case of <span class="math-container">$\ce{Bi_2Se_3}$</span> it is very clear to identify, but in other materials it may be harder. A band inversion like this may suggest that the material has topological order, but the only way to confirm it is by calculating the topological invariant.</p>
<p>So finally let's consider the paper you cite. As far as I understand, in this paper they call &quot;band inversion&quot; a situation in which the bands overlap (left diagram). They argue that such a band overlap is not necessarily induced by spin-orbit coupling, but instead by other effects such as perhaps a structural distortion. I think this is a reasonable statement, and they provide evidence in a range of materials for this. But what is essential to understand is that this is not enough to have a topological insulator, it is still necessary to open the band gap to get to a situation like the one depicted in the right diagram, and for this you usually still need spin-orbit coupling (and this is indeed the case in the paper you cite).</p>
<p><strong>Additional comments.</strong></p>
<ol>
<li>There are some additional subtleties with the above definition of adiabatic connection. We typically require that the Hamiltonian also obeys some symmetry throughout the entire adiabatic evolution, and depending on the symmetry that is obeyed we end up with different types of topological material. For example, when we say &quot;topologial insulator&quot; we typically understand &quot;time-reversal invariant topological insulator&quot;, which means that time reversal symmetry is conserved throughout. Another example may be to impose a crystalline symmetry, and then we could find a topological crystalline insulator.</li>
<li>Another very famous feature of topological insulators is the presence of surface states. From the qualitative description above it is very easy to understand where they come from: the surface of a topological insulator is really the boundary between a topologically ordered bulk and a &quot;normal&quot; vacuum or air outside. As such, the wave function twist needs to be undone at this boundary, so the &quot;gap&quot; closes and we get metallic states, which are observed as surface states in this setup. These states have some nice features because they are protected by the underlying topology, and the presence of surface states (similar to the presence of band inversion) is indicative of topological order. You still want to calculate the topological invariant though to confirm this.</li>
</ol>
<p>P.S.: the basic features of topological materials can be understood with simple 2-band models. I am preparing a series of videos on this, and will share the link when I have them ready.</p>
<p><strong>References</strong>:</p>
<ol>
<li>Soluyanov, A. A.; Vanderbilt, D. <a href="https://doi.org/10.1103/PhysRevB.83.235401" rel="noreferrer">Computing topological invariants without inversion symmetry.</a> <em>Phys. Rev. B</em> <strong>2011,</strong> <em>83</em> (23), No. 235401. DOI: 10.1103/PhysRevB.83.235401.</li>
<li>Fu, L.; Kane, C. L. <a href="https://doi.org/10.1103/PhysRevB.76.045302" rel="noreferrer">Topological insulators with inversion symmetry.</a> <em>Phys. Rev. B</em> <strong>2007,</strong> <em>76</em> (4), No. 045302. DOI: 10.1103/PhysRevB.76.045302.</li>
<li>Kane, C. L.; Mele, E. J. <a href="https://doi.org/10.1103/PhysRevLett.95.226801" rel="noreferrer">Quantum Spin Hall Effect in Graphene.</a> <em>Phys. Rev. Lett.</em> <strong>2005,</strong> <em>95</em> (22), No. 226801. DOI: 10.1103/PhysRevLett.95.226801.</li>
</ol>


================================================================================

Question: What are the types of DFT?
Body: <p>Similar to: <a href="https://mattermodeling.stackexchange.com/q/1439/5">What are the types of charge analysis?</a>, <a href="https://mattermodeling.stackexchange.com/q/901/5">What are the types of bond orders?</a>, and <a href="https://mattermodeling.stackexchange.com/q/392/5">What are some recent developments in density functional theory?</a>, I would like to ask: What are the different variations/flavors of DFT (density functional theory)?</p>
<p>I ask users to stick to one of the following, and explain it <em>compactly</em> as I did <a href="https://mattermodeling.stackexchange.com/a/1132/5">here</a>:</p>
<ul>
<li>DFTB: Density functional tight binding</li>
<li>DFPT: Density functional perturbation theory [<a href="https://mattermodeling.stackexchange.com/a/2167/5">link to answer</a>]</li>
<li>SCC-DFTB: Self Consistent Charge DFTB</li>
<li>TD-DFT: time-dependent DFT</li>
<li>TD-DFRT: time-dependent density functional response theory [<a href="https://mattermodeling.stackexchange.com/a/1284/5">link to answer there</a>]</li>
<li>BS-DFT: Broken-symmetry DFT</li>
<li>MDFT: Molecular DFT</li>
<li>MDFT-dev</li>
<li>DFT-D(EFP)</li>
<li>BDFT: Magnetic field DFT [<a href="https://mattermodeling.stackexchange.com/a/1468/5">link to answer there</a>]</li>
<li>CDFT: Current DFT</li>
<li>KS-DFT: Kohn-Sham DFT</li>
<li>OF-DFT: orbital-free DFT [<a href="https://mattermodeling.stackexchange.com/a/1518/5">link to answer here</a>]</li>
<li>TAO-DFT: Thermally-Assisted-Occupation DFT [<a href="https://mattermodeling.stackexchange.com/a/9108/5410">link to answer there</a>]</li>
<li>DC-DFT: Density-corrected DFT [<a href="https://mattermodeling.stackexchange.com/a/1260/5">link to answer there</a>]</li>
<li>Constrained DFT</li>
<li>Conceptual DFT</li>
<li><a href="https://pubs.acs.org/doi/pdf/10.1021/acs.jctc.0c00208" rel="noreferrer">vMSDFT</a> (variational multi-state DFT)</li>
<li><em>ab initio</em> DFT [<a href="https://mattermodeling.stackexchange.com/a/1212/5">link to answer there</a>]</li>
<li>MCPDFT (Multiconfigurational Pair Density Functional Theory)</li>
<li>SCDFT (Superconducting DFT) [<a href="https://mattermodeling.stackexchange.com/a/1828/5">link to answer here</a>]</li>
<li>RT-TDDFT [<a href="https://mattermodeling.stackexchange.com/a/2104/5">link to answer here</a>]</li>
<li>RT-TDDFPT</li>
<li><a href="https://aip.scitation.org/doi/10.1063/1.473129" rel="noreferrer">Mesoscopic DFT</a></li>
<li>NEGF+DFT</li>
<li>Reduced Density Matrix Functional Theory (RDMFT)</li>
<li>Density Matrix Embedding Theory (DMET)</li>
</ul>

Best Answer: <h3>Real-time TDDFT (RT-TDDFT)</h3>
<p>This is the straightforward non-perturbative solution of the TDDFT equations by means of direct propagation in time. Pioneered by <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.46.12990" rel="noreferrer">Theilhaber</a> and <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.54.4484" rel="noreferrer">Yabana &amp; Bertsch</a> it has since found its way into several molecular or solid-state codes.</p>
<p>The TDDFT equations in the Kohn–Sham (KS) framework are
<span class="math-container">$$
i \frac{\partial}{\partial t} \phi_i (t) = H^\mathrm{KS}(t)\phi_i (t),
$$</span>
where <span class="math-container">$\phi_i$</span> are the occupied KS orbitals and <span class="math-container">$H^\mathrm{KS}(t)$</span> is the time-dependent KS Hamiltonian.</p>
<p>In RT-TDDFT, the orbitals <span class="math-container">$\phi_i$</span> are evolved in time in a stepwise fashion. The propagation from time <span class="math-container">$t$</span> to time <span class="math-container">$t+\Delta t$</span> is performed by means of evolution operator <span class="math-container">$U(t+\Delta t, t)$</span>:
<span class="math-container">$$
\phi_i (t+\Delta t) = U(t+\Delta t, t)\phi_i (t),
$$</span>
where the evolution operator is constructed from the KS Hamiltonian. Because of the time dependence of the Hamiltonian, both explicit via a typically present time-dependent external field, and implicit via the time-dependent KS orbitals used to construct it, the TDKS equation is non-linear. Therefore, the time evolution is aided by some iterative or predictor-corrector scheme to ensure stable evolution. Examples of solvers include the Magnus, Crank–Nicholson or Runge–Kutta propagator.</p>


================================================================================

Question: Is there any DFT code or software that has the option to use this exact exchange-correlation energy functional?
Body: <p>Most of the conventional DFT codes or software use LDA, GGA, meta-GGA, PBE, etc. exchange-correlation functionals, but I'm wondering if there is any DFT code or software that uses the new generation of exchange-correlation functionals called <strong>Exact Exchange-Correlation Potentials</strong>? Particularly, I recently came across the exact Hartree-Fock exchange energy functional and potential that neglects the correlation and defined as:</p>

<p><span class="math-container">$$E_{X}[n] = -\frac{1}{2}\sum_{i,j} \int d^{3}\mathbf{r} d^{3}\mathbf{r}^{'} \frac{\phi^{*}_{i}(\mathbf{r})\phi_{j}^{*}(\mathbf{r}^{'})\phi_{j}(\mathbf{r})\phi_{i}(\mathbf{r}^{'})}{|\mathbf{r}-\mathbf{r}^{'}|}$$</span></p>

<p><span class="math-container">$$v_{X}[n]=\frac{\delta E_{X}[n]}{\delta n(\mathbf{r})}$$</span></p>

<p>and I see that this exact exchange-energy functional is able to calculate magnetic moment of <span class="math-container">$\text{FeAl}$</span>, <span class="math-container">$\text{Ni}_{3}\text{Ga}$</span>, <span class="math-container">$\text{Ni}_{3}\text{Al}$</span> with much higher accuracy in comparison to FP-LDA for example described <a href="http://arxiv.org/abs/cond-mat/0501258" rel="noreferrer">here</a>. So my question is: Are these sort of exact exchange-correlation energy functionals incorporated into open-source DFT codes and are they worth using in comparison to conventional XC potentials due to the fact they seem to be more complicated and more difficult to implement? Also, when we say these are <strong>exact</strong> exchange-correlation energy functionals, what do we mean by exact? The purpose is that there is no approximation here or something else?</p>

Best Answer: <p>Below is a summary of what I have found:</p>

<p><hr></p>

<h1>What is meant by "Exact exchange"<span class="math-container">$\,$</span>?</h1>

<p>The question mentions the following (in order of how they were presented in the question):<br></p>

<ul>
<li><strong>Exact exchange-correlation potential</strong> (used in <a href="https://www.nature.com/articles/s41467-019-12467-0" rel="noreferrer">this 2019 paper in Nature Communications</a>).</li>
<li><strong>Exact Hartree-Fock exchange energy functional</strong> (<a href="https://www.google.ca/search?q=&quot;exact+Hartree-Fock+exchange+energy+functional&quot;" rel="noreferrer">only 4 results</a>, 3 from the same authors).</li>
<li><strong>Exact exchange-energy functional</strong> (wording that's been used <a href="https://en.wikipedia.org/wiki/Hybrid_functional" rel="noreferrer">here</a> for  hybrid functionals).</li>
<li><strong>Exact exchange-correlation energy functional</strong> (<em>finding it is often thought to be impossible</em>).</li>
</ul>

<p><em>However, I was indeed able to find the equations you gave, in <a href="http://www1.mpi-halle.mpg.de/~sharma/talks/vienna.pdf" rel="noreferrer">these lecture slides</a>!</em> </p>

<p>This leads to more terms:</p>

<ul>
<li><strong>Exact Exchange in Density Functional Theory</strong> (is the title of the talk).</li>
<li><strong>Exact Exchange Density Functional theory</strong> (in title of the paper you linked, by same author).</li>
<li><strong>Exact exchange (EXX) method</strong> (this is what the paper you mentined, by, uses).</li>
</ul>

<p>Your quote "this exact exchange-energy functional is able to calculate..." refers to the <strong>EXX method</strong></p>

<hr>

<h1>What was meant by "EXX method" in the linked paper?</h1>

<p>Unfortunately I have come to learn that the authors of the paper you mentioned, did not cite the original EXX papers when they mentioned EXX: </p>

<blockquote>
  <p>"In this work we deploy the EXact eXchange (EXX) method, implemented within an all-electron full-potential code EXITING [8]"</p>
</blockquote>

<p>or anywhere in the paper, or the aforementioned talk. Instead the paper cited a code called "EXITING" which is actually a mis-spelling of their <a href="http://www.exciting.physics.at/tasks/t2.shtml#m13" rel="noreferrer">unavailable code</a> called EXCITING. The aforementioned lecture slides end with a page saying that the "code is available" over <a href="https://physik.kfunigraz.ac.at/%E2%88%BCkde/secret%20garden/exciting.html" rel="noreferrer">here</a>, but that link leads to a 404 error. They mentioned that they had recently developed the EXX equations for their magnets, with a reference to <a href="https://arxiv.org/pdf/cond-mat/0510800v1.pdf" rel="noreferrer">this pre-print</a> which after adding 1 new author and moving Hardy Gross from 3rd last author to last author, <a href="https://arxiv.org/abs/cond-mat/0510800" rel="noreferrer">got published 2 years later in PRL</a>.</p>

<p>Since the paper you mentioned was never cited, and didn't land in any journal, but refers to a paper which landed in PRL two years later, involving all-but-one of the same authors, I was curious to see how far apart these papers were written. This lead me to find out that the paper you linked was v3 of a paper whose v1 (which had <em>even fewer</em> authors) was actually put on arXiv 10 months before v1 of the other one. That paper actually did cite some earlier EXX papers (indicating that the authors were aware of them when they posted later versions without citation).</p>

<p><strong><em>Summary of this section:</em></strong> The authors of the linked paper posted v1 of a 4-page PRL-style paper on arXiv in January 2005 with 4 authors, added 2 authors for v2 in June, removed 1 author and added 3 more (including a very famous one) for v1 of a different 4-page PRL-style paper in October (which was eventually published in PRL in March 2007 after adding 1 more author and moving the most famous one to the end position), while a v3 of the first paper was posted 1 month later never landed in a journal and never got cited. The EXCITING code is also not available at exciting.physics.at, nor at the link mentioned in the talk.</p>

<p><hr></p>

<h1>So where did the term "EXX method" originate?</h1>

<p>At first I thought the EXX method was invented by the authors of the linked paper (hours before I tracked down v1 which did cite some earlier papers), but something smelled fishy so I looked in other places.</p>

<p>The <a href="http://cmt.dur.ac.uk/sjc/thesis_mcg/node67.html" rel="noreferrer">2006 PhD thesis of Michael Gibson</a> of the reputable Durham University says in the <strong><em>abstract:</em></strong> </p>

<blockquote>
  <p>"We then describe our computational implementation of advanced density
  functionals, including screened exchange (sX-LDA), Hartree-Fock (HF),
  and exact exchange (EXX), within an efficient, fully parallel, plane
  wave code."</p>
</blockquote>

<p>Since this person's PhD thesis was on this, let's proceed. He calls it the "<strong>exact exchange (EXX) functional</strong>" and cites 3 papers: one was published in 1996, another in 1997 with the same author plus 2 more, and the last in 1999 with the same authors plus 1 more.</p>

<ul>
<li>The <strong>1996</strong> paper does not use the term EXX, but mentions a "recently introduced exact formal KS procedure" in the abstract, with a reference to a paper from 2 years earlier, involving the same author.</li>
<li>The <strong>1997</strong> paper (published in PRL) begins the abstract with "A new Kohn-Sham method that treats exchange interactions within density functional theory
exactly" followed later by "In this Letter, we present an <em>exact</em> determination of the KS exchange potential" (Italic font on the word "exact" was theirs). The third paragraph ends with "The
present exact exchange formalism, that we abbreviate by EXX, eliminates these divergencies exactly." <strong>So this must be where EXX comes from right?</strong></li>
<li>For some reason I also looked at the <strong>1999</strong> paper, which says "We have developed a scheme—termed the exact exchange (EXX) method [31]" which points not to the 1997 paper where they claim to have come up with the term EXX, but to a <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.50.14816" rel="noreferrer">1994 paper</a> by a Japanese author entirely independent of any of the Germany-based authors of this 1994-1999 series.</li>
<li>The <strong>1994</strong> paper indeed uses the term EXX, but they openly admit that it's an old concept and it's been called that in the past: "In this paper, we present a method of DF band calculation using the so-called exact exchange (EXX) potential
[the exact Kohn-Sham (KS) density-functional exchange potential] (Ref. 5)" where Ref 5 is a <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.28.1809" rel="noreferrer">1983 paper currently with 1330 citations</a>. Elsewhere they also mention that a <a href="https://journals.aps.org/pra/abstract/10.1103/PhysRevA.14.36" rel="noreferrer">1976 paper currently with 1328 citations</a> used "the EXX-only method".</li>
<li>The <strong>1983</strong> paper uses the word "exact" 103 times, but never the acronym EXX.</li>
<li>The <strong>1976</strong> paper doesn't make any reference to EXX or even DFT in general, but a <a href="https://www.sciencedirect.com/science/article/abs/pii/0092640X78900190" rel="noreferrer">1978 paper</a> involving one of the authors, does put the 1976 paper into the context of the Thomas-Fermi model and the language of energy functionals. </li>
</ul>

<p><strong><em>Summary of this section:</em></strong> I only started doing research on this about 5 hours ago, and only to try to answer this question, but the earliest use of the acronym EXX I could find was in <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.50.14816" rel="noreferrer">this 1994 paper</a> by Kotani which credits <a href="https://www.sciencedirect.com/science/article/abs/pii/0092640X78900190" rel="noreferrer">this 1978 paper</a> as using what the 1994 author calls "the EXX-only method" and <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.28.1809" rel="noreferrer">this 1983 paper</a> when mentioning the EXX potential for the first time. <strong>The "EXX method" has some roots in the 1976 and 1983 papers but the phrase seems to have emerged some time between 1983 and 1994</strong>. Görling and co-workers seem to have improved on the work of Kotani (though they don't phrase it that way) from 1994-1999, and they call this improvement "the EXX method", and <strong>a version of this catered to specific magnetic problems is what was used in the 2005-2007 paper of Sharma <em>et al.</em> linked in the question.</strong> The Görling  <em>et al.</em> and Sharma <em>et al.</em> papers each gave me the impression that they invented the EXX method, but if dug deeply enough it's possible to find Kotani's paper buried in the bibliography (except in the present version of the linked Sharma paper, which doesn't cite any previous EXX paper).</p>

<p><hr></p>

<h1>Is there any software that offers "EXX" functionals?</h1>

<p>GPAW has <a href="https://wiki.fysik.dtu.dk/gpaw/documentation/xc/exx.html" rel="noreferrer">two implementations of EXX</a> but your mileage may vary (their EXX might not mean the same as your EXX). They also did not provide a reference for the EXX functional that they implemented (but this does not surprise me as I have now spent almost 6 hours on this and 6 hours was perhaps not enough): </p>

<p><a href="https://i.sstatic.net/Nt2yh.png" rel="noreferrer"><img src="https://i.sstatic.net/Nt2yh.png" alt="enter image description here"></a> </p>

<hr>

<h1>Conclusion</h1>

<p>You asked if the EXX method is:</p>

<blockquote>
  <p>"worth to use in comparison to conventional XC potentials"?</p>
</blockquote>

<p>"Exact" seems to be a buzz word that was used 103 times in the aforementioned 1983 paper, when in fact nothing is <em>truly</em> exact other than the "grand unified theory" (which doesn't yet exist and may never exist). At least two groups of authors seem to have misleadingly claimed to invent exact-exchange methods, one group in the 1990s and one group in the 2000s, but the paper you linked which claimed higher accuracy compared to FP-LDA never survived peer review, has never been cited, and was performed with software that is unavailable.</p>

<p>I therefore see no evidence yet that the EXX method you linked to is actually better than the conventional XC methods you mention. DFT is a highly approximate method anyway, and nothing is exact. DFT is notorious for sometimes giving good results for the wrong reasons, so one single paper showing a better accuracy than one other paper, does not convince me that any method is better in general. However I am not a DFT expert: I am just someone who accidentally tried to answer this question because I wanted to clear up the "unanswered" queue, and 6 hours ago I thought the answer would be easy!</p>


================================================================================

Question: What is Materials Modeling?
Body: <p>I'm a noob with zero knowledge of the subject. Materials Modeling is so specialized that it's not even on Wikipedia yet. Help orient a newcomer. What do you use it for? Can you use it to model materials for a bulletproof t-shirt, a superior mask to protect against COVID-19, a flux capacitor?</p>

Best Answer: <p>I'm just as curious of a user here as you, but I was able to find that a <a href="https://en.wikipedia.org/wiki/Computational_materials_science" rel="noreferrer">Materials Modeling Wikipedia page</a> does in fact exist. :)</p>

<blockquote>
  <p><strong>Computational materials science and engineering</strong> uses modeling,
  simulation, theory, and informatics to understand materials. Main
  goals include discovering new materials, determining material behavior
  and mechanisms, explaining experiments, and exploring materials
  theories. It is analogous to computational chemistry and computational
  biology as an increasingly important subfield of materials science.</p>
</blockquote>


================================================================================

Question: Why does orbital-free DFT scale linearly with system size?
Body: <p>In answering another question here (<a href="https://materials.stackexchange.com/q/85/7">Is there any relevant DFT formalism apart from the Kohn-Sham approach?</a>), I came across numerous statements that Orbital-Free DFT should scale linearly with system size. From the <a href="https://arxiv.org/pdf/1408.4701.pdf" rel="noreferrer">implementation details</a> in GPAW, this seems to be a result of using the same machinery as for a typical Kohn-Sham DFT calculation, but with only a single "orbital" (actually the square root of the density). But from the other statements, there seems to be a more fundamental physical reason why we would expect orbital free DFT to scale linearly with system size. Why should orbital free DFT be linearly scaling and by what metric for system size (since KS DFT is usually said to scale cubically with the number of orbitals)? </p>

Best Answer: <p>In orbital-free DFT, the key quantity is the particle density (not the density matrix, the actual density). The particle density is a scalar 3D field; when we increase the particle number in our simulation, the values of the particle density change, subject to the constraint that for <span class="math-container">$N$</span> particles in a volume <span class="math-container">$V$</span> of space,</p>
<p><span class="math-container">$$
\iiint_V \rho({\bf r})d^3{\bf r} = N.
$$</span></p>
<p>Although the <em>values</em> change if <span class="math-container">$N$</span> changes, the density remains a 3D scalar field and takes up the same amount of computer storage. The size of the density does not depend on the number of particles and so scales as O(1) in particle count -- better than linear scaling!</p>
<p>If we increase the simulation volume instead, then the density now extends over a greater volume and clearly the 3D field must be extended over the additional space. The size of the density scales linearly with the simulation volume, since the volume directly controls how much space we have to represent with our density.</p>
<p>Hopefully you can now see why the <em>size</em> of the density object scales linearly with system size. The size of a data object gives a lower bound for the computational time, assuming that every element of the object has to be checked at least once. However, there is no guarantee that the computational time <em>does</em> scale linearly with system size. It is easy to construct functionals whose computational cost scales far worse than the size of the data object, for example multiple nested integrals over all spatial dimensions. I think the argument for linear-scaling computational time is based on using a semi-local energy functional; even including a Hartree term makes the calculation worse than linear-scaling, strictly speaking, although most people wouldn't complain about the extra factor of <span class="math-container">$\log(N)$</span>.</p>
<p>Note that whether orbital-free methods scale linearly or not is unrelated to whether the material being simulated is an insulator or a metal. This is in contrast to most &quot;linear-scaling DFT&quot; approaches, which rely on the density <em>matrix</em> decaying exponentially in real-space in order to truncate it safely, which is only valid for systems with a band-gap. Orbital-free methods do not use the density-matrix, so are not subject to the same constraints; in fact the most successful applications of orbital-free DFT are mostly to high-temperature metals (e.g. liquid sodium); even Thomas-Fermi(-Dirac) can perform reasonably in these situations, yet they are amongst the worst cases for density-matrix-based linear-scaling DFT.</p>


================================================================================

Question: Approximate equivalence table between Pople&#39;s basis sets and Jensen&#39;s DFT optimized polarization consistent basis sets (pcseg-n)
Body: <p>At first I posted <a href="https://chemistry.stackexchange.com/q/130712/89908">this question</a> in chemistry.stackexchange.com, before materials.stackexchange.com was launched in beta. As it went unaswered, I'm giving a try here.</p>

<p>I have some calculations results I runned previously using Pople's basis sets, mostly 6-311+G(d), under Gaussian09. These days I read some texts on Frank Jensen's family of polarization consistent, segmented basis sets, optimized for DFT (pcseg-0, pcseg-1, pcseg-2, pcseg-3, pcseg-4 and the respective augmented versions). Now I'm thinking about trying to reproduce my results using Jensen's basis set family and Gamess-US.</p>

<p>Said that, I'm not sure about the correspondence between the two sets. I understand in Pople's basis sets, for light elements (first 3 periods of the periodic table) it's common to see people using double zeta(DZ) 3-21G for rough calculations, and either double zeta 6-31G or triple zeta(TZ) 6-311G for more precise work. Both 6-31G and 6-311G can have up to two polarization functions (or none) and up to two diffuse functions (or none) associated, resulting in 2x3x3 = 12 combinations between them (not counting heavier elements, that would require f polarization orbitals). In the table 5 of <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119356059.ch3" rel="noreferrer">Nagy, Balazs, and Frank Jensen. “Basis Sets in Quantum Chemistry.” Reviews in Computational Chemistry (2017): 93–150. Print.</a>, they group together 3-21G and pcseg-0; 6-31G(d), cc-pVDZ and pcseg-1; and 6-311G(2df), cc-pVTZ and pcseg-2. So I assume each basis set inside these 3 groups to be equivalent (but not sure). As pcseg-2 is the only triple zeta option, despite already falling in the range of f-polarized basis, I suppose that, to map these 12 Pople's basis into the respective Jensen's basis, I need some combination between pcseg-0, pcseg-1, pcseg-2, aug-pcseg-0, aug-pcseg-1 and aug-pcseg-2 over H and heavier atoms (6x6 = 36 possibilities). For me it's not trivial to choose which of the 36 Jensen's possibilities in this range best match the 12 Pople's ones not explicitly cited on the paper. My guess at the closest mapping between the 2 sets is as follow:</p>

<pre><code>Type    POPLE's         DUNNING     JENSEN'S CLOSEST
DZ      3-21G                       pcseg-0 on all atoms?
DZ      6-31G                       ?
DZP     6-31G(d)        cc-PVDZ     pcseg-0 on H, pcseg-1 on heavier?
DZP     6-31+G(d)                   pcseg-0 on H, aug-pcseg-1 on heavier?                   
DZP     6-31G(d,p)                  pcseg-1 on H, pcseg-1 on heavier?
DZP     6-31+G(d,p)                 pcseg-1 on H, aug-pcseg-1 on heavier?
DZP     6-31++G(d,p)                aug-pcseg-1 on H, aug-pcseg-1 on heavier?
TZ      6-311G                      ?
TZ      6-311+G                     ?
TZP     6-311+G(d)                  pcseg-0 on H, aug-pcseg-2 on heavier?
TZP     6-311G(d,p)                 pcseg-1 on H, pcseg-2 on heavier?                 
TZP     6-311+G(d,p)                pcseg-1 on H, aug-pcseg-2 on heavier?
TZP     6-311++G(d,p)               aug-pcseg-1 on H, aug-pcseg-2 on heavier?
TZP     6-311G(2df)     cc-PVTZ     pcseg-2 on all
</code></pre>

<p>Is my reasoning sound and the proposed equivalence table correct, or did I get it all wrong? If wrong, could somebody please give the correct mapping from Pople's traditional basis sets to Jensen's optimized ones?</p>

Best Answer: <p>Disclaimer and warning: long and likely biased answer.</p>
<p>Background: The Pople style basis sets were defined almost 50 years ago. The 6-31G was designed for HF calculations, the 6-311G for MP2 calculations. For computational efficiency reasons, the s- and p-exponents were constrained to be identical. Polarization functions were defined for 1d, 2d, 3d and 1f. Calculations for anions lead to the augmentation with diffuse s- and p-functions, denoted with +.</p>
<p>The seminal work of Dunning leading to the cc-pVnZ family of basis sets introduced the concept of balancing errors as a basis set design element. The key feature is that a balanced basis set typically has one less (contracted) function for each step up in angular momentum, and the highest angular momentum function included thus defines the basis set quality. This is by modern notation often called the cardinal number X. By this standard, only the Pople style basis sets 6-31G(d,p) and 6-311G(2d1f,2p1d) are balanced, and combinations such as 6-31G(2d,2p) or 6-311G(d,p) should not be used. Similarly, using the unpolarized 6-31G and 6-311G does not make sense, as the lack of polarization functions completely dominates the error. It has been argued that the 6-311G is really only of double-zeta quality, in which case 6-311G(d,p) could be considered as a marginally improved version of 6-31G(d,p).</p>
<p>The cc-pVnZ basis sets have been optimized to describe electron correlation, while the polarization consistent (pc) basis sets have been optimized towards DFT methods. The difference in basis set convergence for wave function electron correlation and DFT methods (<span class="math-container">$X^{-3}$</span> vs. <span class="math-container">$\exp(-X^{1/2}$</span>)) makes the optimum composition and exponents (slightly) different. The most recent version of the pc has been defined with a segmented contraction (pcseg), which improves the computational efficiency significantly in most program packages. The following assumes that the method is DFT or HF, for highly correlated wave function methods, the cc-pVnZ are likely the best choice.</p>
<p>Now for the questions raised (basis set errors taken from <a href="https://doi.org/10.1021/ct401026a" rel="noreferrer">[1]</a> using DFT):</p>
<p>The 6-31G(d,p) is formally of the same cardinal quality as pcseg-1. The basis set error relative to the basis set limit, however, is roughly a factor of 3 lower for the pcseg-1.</p>
<p>The 6-311G(2d1f,2p1d) is formally of the same cardinal quality as pcseg-2. The basis set error relative to the basis set limit, however, is roughly a factor of 5 lower for the pcseg-2. The pcseg-1 also gives lower basis set errors than 6-311G(2d1f,2p1d), by roughly a factor of 2, which as mentioned above, suggests that the 6-311G is not of triple zeta quality.</p>
<p>The pcseg-1 gives lower basis set errors than any of the Pople type combinations, and the basis set error is consistent for all atoms H-Kr.</p>
<p>The computational efficiency depends on the molecule and the program package used, but the computational time using 6-31G(d,p) or pcseg-1 is usually very similar, but see the comment regarding hydrogen below.</p>
<p>Diffuse augmentation leads to 6-31+G(d) and aug-pcseg-1. The aug-pcseg-1 for non-H atoms has diffuse s-, p- and d-function, while 6-31+G(d) only has diffuse s- and p-functions. Diffuse d-functions have a minor influence on energetics for e.g. anions, but they vastly improve the performance for e.g. dipole moments and polarizabilities.</p>
<p>Hydrogen atoms often play a ‘spectator’ role in molecules but often accounts for roughly half of the atoms, and they are for computational efficiency reasons therefore often described by a (slightly) lower quality basis set. The 6-31G(d) thus do not include polarization functions on hydrogen, while the pcseg-1 does by default, again based on balancing errors. 6-31G(d) is thus equivalent to pcseg-1 for non-H, but requires removing the p-function for H. Alternatively, one could use the pcseg-0 for H, which would be equivalent to using the 3-21G in the Pople family. Similarly, one often only includes diffuse functions on non-hydrogen atoms.</p>
<p>The updated Table:</p>
<div class="s-table-container">
<table class="s-table">
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Pople/Dunning</th>
<th style="text-align: center;">Jensen (closest)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DZ</td>
<td style="text-align: center;">3-21G</td>
<td style="text-align: center;">pcseg-0 (all atoms)</td>
</tr>
<tr>
<td style="text-align: center;">DZ</td>
<td style="text-align: center;">6-31G</td>
<td style="text-align: center;">non-H (aug-pcseg-1), H (pcseg-1 polarization removed)</td>
</tr>
<tr>
<td style="text-align: center;">DZP</td>
<td style="text-align: center;">6-31G(d)/cc-pVDZ</td>
<td style="text-align: center;">non-H (aug-pcseg-1), H (pcseg-1 polarization removed)</td>
</tr>
<tr>
<td style="text-align: center;">DZP</td>
<td style="text-align: center;">6-31+G(d)</td>
<td style="text-align: center;">non-H (aug-pcseg-1), H (pcseg-1 no polarization or pcseg-0)</td>
</tr>
<tr>
<td style="text-align: center;">DZP</td>
<td style="text-align: center;">6-31G(d,p)</td>
<td style="text-align: center;">pcseg-1 (all atoms)</td>
</tr>
<tr>
<td style="text-align: center;">DZP</td>
<td style="text-align: center;">6-31+G(d,p)</td>
<td style="text-align: center;">non-H (aug-pcseg-1), H (pcseg-1)</td>
</tr>
<tr>
<td style="text-align: center;">DZP</td>
<td style="text-align: center;">6-31++G(d,p)</td>
<td style="text-align: center;">aug-pcseg-1 (all atoms)</td>
</tr>
<tr>
<td style="text-align: center;">TZP</td>
<td style="text-align: center;">6-311G(2df)/cc-pVTZ</td>
<td style="text-align: center;">non-H (pcseg-2), H (pcseg-2 no polarization)</td>
</tr>
</tbody>
</table>
</div>
<p>The following combinations are really inconsistent and not truly of TZP quality. They should generally be avoided, but if viewed as being of double zeta quality, rather than triple zeta quality, then:</p>
<div class="s-table-container">
<table class="s-table">
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Pople/Dunning</th>
<th style="text-align: center;">Jensen (closest)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TZ</td>
<td style="text-align: center;">6-311G</td>
<td style="text-align: center;">pcseg-1 (all polarization removed)</td>
</tr>
<tr>
<td style="text-align: center;">TZ</td>
<td style="text-align: center;">6-311+G</td>
<td style="text-align: center;">non-H (aug-pcseg-1), H (pcseg-1 polarization removed)</td>
</tr>
<tr>
<td style="text-align: center;">TZP</td>
<td style="text-align: center;">6-311+G(d)</td>
<td style="text-align: center;">non-H (aug-pcseg-1), H (pcseg-1 polarization removed)</td>
</tr>
<tr>
<td style="text-align: center;">TZP</td>
<td style="text-align: center;">6-311G(d,p)</td>
<td style="text-align: center;">pcseg-1 (all atoms)</td>
</tr>
<tr>
<td style="text-align: center;">TZP</td>
<td style="text-align: center;">6-311+G(d,p)</td>
<td style="text-align: center;">non-H (aug-pcseg-1), H (pcseg-1)</td>
</tr>
<tr>
<td style="text-align: center;">TZP</td>
<td style="text-align: center;">6-311++G(d,p)</td>
<td style="text-align: center;">aug-pcseg-1 (all atoms)</td>
</tr>
</tbody>
</table>
</div>
<p>The above is for second-row elements (Li-Ne). The 6-311G is not defined for third-row (Na-Ar) atoms, and the 6-31G for transition metals is, in my opinion, a poor choice.</p>
<p>The goal of the cc and pc basis sets is to approach the complete basis set limit in a fast, systematic and monotonic fashion. The computational method, however, in most cases also have inherent errors, and method and basis set errors may be in different directions. As mentioned in the question, one can easily design ~50 Pople style basis set combinations, and if combined with ~50 DFT methods, one has 2500 different computational models. Testing these against a (limited) set of reference data will almost certainly identify ‘magic’ combinations where basis set and method errors to some extend cancel each other and lead to a low error relative to the reference data. For this to work, it is necessary that one can introduce significant (and different) basis set errors, and this is where unbalanced basis sets come into play. <a href="https://doi.org/10.1021/acs.jctc.8b00477" rel="noreferrer">[2]</a> gives an illustration of such method and basis set error compensation.</p>
<ol>
<li>F. Jensen J. Chem. Theory Comput. 2014, 10, 3, 1074-1085 DOI: 10.1021/ct401026a</li>
<li>F. Jensen J. Chem. Theory Comput. 2018, 14, 9, 4651-4661 DOI: 10.1021/acs.jctc.8b00477</li>
</ol>


================================================================================

Question: How is Julia used in matter modeling?
Body: <p>What are the main uses and applications of Julia in matter modeling?</p>
<p>Python for example is used in many codes, not to mention workflow automation and data processing.</p>
<p>Are there any available codes or publications out there that are based mainly on the Julia programming language?</p>

Best Answer: <p>(Disclaimer: As one of the main authors of a Julia-based DFT code, <a href="https://dftk.org" rel="noreferrer">DFTK</a>, my opinion is definitely biased)</p>
<p>The community of people employing Julia for materials modelling is still small, but a couple of programs exist. Probably a good overview gives <a href="https://github.com/svaksha/Julia.jl/blob/master/Chemistry.md#simulation-methods" rel="noreferrer">https://github.com/svaksha/Julia.jl/blob/master/Chemistry.md</a>. Many projects have only started within the last year, but still show a sizeable set of features, indicating the great potential of Julia for this field. Regarding the workflow, there are not that many packages, but you can just use from python what you need. I'll try to summarise:</p>
<h1>Uses of Julia and its design</h1>
<p>Julia solves very elegantly what is commonly known as the two-language problem. In computational science one wants to be fast when crunching the numbers (traditionally calling for a language like Fortran, C and C++) and flexible when building models or simulation workflows (calling traditionally for something like python).</p>
<p>This by itself has the trouble that people (read: PhD students) need to actually learn two languages to be able to develop the code, but let's argue this is not a problem in practice. Still, we are faced with making the decision at which point to make the cut between the languages. Where this cut point should be placed is less obvious than it might sound if you keep in mind that materials science is an interdisciplinary field with people from maths, physics, chemistry, computer science, ... working on the same sort of problems, but at very different levels. While for a practitioner all the hairy details SCF algorithm, say, are not so important and can be easily shuffled under the carpet of heavy C or Fortran code, mathematicians actually want to modify exactly those to improve mixing schemes or preconditioning and so on. So for a mathematician this should be high-level python to productively do research, but if one does this, it has too much of a performance impact for large-scale practical calculations to be feasible.</p>
<p>Here Julia is for the rescue, since as commonly quoted it <em>looks like Python, feels like Lisp, runs like Fortran</em>. What this is supposed to mean is that writing Julia code is very similar to Python including hairy parts such as linear algebra. It has stronger type systems and structures similar to functional languages like Lisp
and in the end really is as fast as Fortran or C. The reason is that even though it is a high-level language it has &quot;close to the metal&quot; constructs that allow you to directly influence aspects such as vectorisation, parallelisation and so on. The advantage over a two language solution is, however, that you can introduce them step by step once the first version of the code has been written without rewriting it in another language. So first you get things done, then you get them done fast.
For any new project in materials modeling, whether you right now think it is performance-critical or not, I think Julia is therefore the perfect language.</p>
<h1>Materials-related Julia projects</h1>
<ul>
<li><a href="https://github.com/JuliaMolSim" rel="noreferrer">JuliaMolSim</a>: Github organisation for molecular simulation in Julia, packages such as:
<ul>
<li><a href="https://github.com/JuliaMolSim/ASE.jl" rel="noreferrer">ASE.jl</a>: (Incomplete) Julia bindings for ASE.</li>
<li><a href="https://dftk.org" rel="noreferrer">DFTK.jl</a>:  Flexible Julia code for plane-wave density-functional theory (DFT) and related models. LDA and GGA functionals are supported. Ground-state calculations only for now. Less than 5k lines of code and very hackable. Designed also for mathematical work in the field. Can be used directly from ASE as a calculator using <a href="https://github.com/mfherbst/asedftk" rel="noreferrer">asedftk</a>. Still experimental, but any feedback welcome!</li>
<li><a href="https://github.com/JuliaMolSim/JuLIP.jl" rel="noreferrer">JuLIP.jl</a>: Julia Library for Interatomic Potentials. Can be used to rapidly build and test interatomic potentials for defects etc.</li>
</ul>
</li>
<li><a href="https://github.com/jgreener64/Molly.jl" rel="noreferrer">Molly.jl</a>: Proof of concept molecular-dynamics package in Julia.</li>
<li><a href="https://github.com/SimonEnsemble/PorousMaterials.jl" rel="noreferrer">PorousMaterials.jl</a>: Julia package for modelling adsorption on porous crystals using Monte Carlo methods.</li>
<li><a href="https://github.com/pablosanjose/Quantica.jl" rel="noreferrer">Quantica.jl</a>: Julia package for building generic tight-binding models and computing various properties from it.</li>
<li><a href="https://github.com/BBN-Q/QSimulator.jl" rel="noreferrer">QSimlator.jl</a>: Package for Unitary and Lindbladian evolution in Julia.</li>
<li><a href="https://github.com/vonDonnerstein/QuantumLab.jl" rel="noreferrer">QuantumLab.jl</a>: Experimental package for more molecular quantum chemistry (roadmap is similar to pyscf)</li>
</ul>
<h1>Workflow</h1>
<ul>
<li>For managing a scientific project <a href="https://github.com/JuliaDynamics/DrWatson.jl" rel="noreferrer">DrWatson.jl</a> is extremely helpful.</li>
<li>The pandas equivalent of Julia is <a href="https://github.com/JuliaData/DataFrames.jl" rel="noreferrer">DataFrames.jl</a>. You can also directly use <a href="https://github.com/JuliaIO/HDF5.jl" rel="noreferrer">HDF5.jl</a>.</li>
<li>Plotting can be done in <a href="https://github.com/JuliaPlots/Plots.jl" rel="noreferrer">Plots.jl</a> or matplotlib via <a href="https://github.com/JuliaPy/PyPlot.jl" rel="noreferrer">PyPlot.jl</a>.</li>
<li>Actually you can integrate <em>any</em> python or R package into a Julia script using packages such as <a href="https://github.com/JuliaPy/PyCall.jl" rel="noreferrer">PyCall.jl</a> or <a href="https://github.com/JuliaInterop/RCall.jl" rel="noreferrer">RCall.jl</a>. For python this integration is two-way, so calling Julia from python is a piece of cake. You can even <code>pip install julia</code>. This is what we use in DFTK to do our lattice and Brillouin zone setup  using <code>spglib</code> and <code>pymatgen</code> or import <code>ase.Atoms</code> into our datastructures.</li>
</ul>
<h1>Publications</h1>
<p>Not that many I am aware of. We have recently published a few things with DFTK, but they are not so much related to modelling.</p>
<p>In 2019, <a href="https://doi.org/10.1016/j.cpc.2019.01.012" rel="noreferrer">&quot;A fresh computational approach to atomic structures, processes and cascades&quot;</a> was published in <em>Computer Physics Communications</em>, which presents <em>a fresh concept and implementation of (relativistic) atomic structure theory that supports the computation of interaction amplitudes, properties</em>, etc., which are implemented in <a href="https://github.com/OpenJAC/JAC.jl" rel="noreferrer">JAC.jl</a>.</p>
<p>Recently,(May 2020) <a href="https://doi.org/10.1016/j.cpc.2020.107372" rel="noreferrer">&quot;PWDFT.jl: A Julia package for electronic structure calculation using density functional theory and plane wave basis&quot;</a> was published in <em>Computer Physics Communications</em> based on the <a href="https://github.com/f-fathurrahman/PWDFT.jl" rel="noreferrer">PWDFT.jl</a> package.</p>
<p>Also in 2020, the paper <a href="https://doi.org/10.1021/acs.jctc.0c00337" rel="noreferrer">&quot;A New Kid on the Block: Application of Julia to Hartree–Fock Calculations&quot;</a> was published in the <em>Journal of Chemical Theory and Computation</em> decribing <a href="https://github.com/davpoolechem/JuliaChem.jl" rel="noreferrer">JuliaChem</a>, a novel quantum chemistry software designed to take advantage of Julia as much as possible.</p>


================================================================================

Question: Can a highly-cited published paper have this type of error?
Body: <p><strong>Related cross-network post:</strong> <a href="https://hsm.stackexchange.com/q/14051/8052">Have there been instances in physics where different scientists have interpreted the same data differently?</a></p>

<p>Here is my problem: I have been testing a force field published in a peer-reviewed paper, and later used by quite a few other authors (I am not sure if I should cite said paper, as I don't want to seem &quot;incriminating&quot;)</p>
<p>I was initially able to reproduce quite closely the published results in the original paper while testing it, but they seemed rather unstable (with respect to initial geometry, equilibration time, etc…). To me, this seemed to indicate that the system was not properly equilibrated, and indeed, when equilibrating for a much longer time, I observed the following a behavior (below is the average lattice constant vs number of timesteps for a NPT run at 300K, timestep being 0.5 fs):</p>
<p><a href="https://i.sstatic.net/kgdcs.png" rel="nofollow noreferrer"><img src="https://i.sstatic.net/kgdcs.png" alt="enter image description here" /></a></p>
<p>Basically, the system relaxes in 2 steps: there’s a first initial “relaxation” to a value that matches well with the paper and experiments (~6.29A), happening over just a few thousand timesteps. But then the system seems to re-adjust on a much slower timescale, and eventually reaches an equilibrium at a lower value (~6.24A).</p>
<p>It is either one fo two things: either I’m doing something wrong, or the original paper published results based on a not properly equilibrated system (which basically makes their force field faulty, and also impacts the credibility of subsequent papers using it)</p>
<p>In favor of the first possibility, I have less experience than the authors, and it seems like something that would be hard to miss for them. However, I am using input files that they were kind enough to provide me themselves, with almost no modification at all. Those files are pretty simple, making the possibility of mistakes rather low, and I’ve been trying to double check again and again.</p>
<p><em><strong>How likely is it that a force field that has been published in a respectable paper by serious authors and has then been used by many others is faulty and based on non-equilibrated results?</strong></em> It seems like that problem is too basic to go unnoticed. I have tried to contact the author about that issue, providing them a much more detailed account of the problems I found, but they have not reached back.</p>
<p>I can provide more details and explanations if needed. I am currently pretty stuck because of this problem</p>

Best Answer: <p>Many published papers, even extremely highly cited ones by what you call &quot;serious&quot; authors, have issues like these.</p>
<p>Consider for example <a href="https://www.pnas.org/content/106/41/17255" rel="noreferrer">this one</a> which currently has 862 Google Scholar citations, and has been used as a benchmark for the quantum dynamics of the seven density matrix diagonals in a 7-site model of the FMO complex. The authors were (and still are) two of the most respected scientists in their field, and I personally know the author that did the calculations and think extremely highly of him. But in <a href="https://pubs.acs.org/doi/10.1021/ct501066k" rel="noreferrer">my paper</a> years later, we showed that the several dozen papers that used their supposedly <em>exact</em> dynamics plot as a <em>benchmark</em> to compare the dynamics of a new method, were not actually comparing to exact dynamics after all:</p>
<p><a href="https://i.sstatic.net/3cSsI.png" rel="noreferrer"><img src="https://i.sstatic.net/3cSsI.png" alt="enter image description here" /></a></p>
<p>Likewise, <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.108.130501" rel="noreferrer">this paper</a> currently has 163 Google Scholar citations and they claimed to have factored the number 143 by finding the ground state of a 4-qubit Hamiltonian  with an NMR quantum computer. Two years later, with an undergrad-level student, I discovered that the Hamiltonian not only factored 143 but also larger numbers such as 56153.  Dozens of articles were published by journalists, about my discovery, and I particularly find amusing the title of this one: <a href="https://www.theregister.com/2014/12/04/boffins_we_factored_143_no_you_factored_56153/" rel="noreferrer">&quot;Quantum computing is so powerful it takes two years to understand what happened&quot;</a>.</p>
<p>In a <a href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.84.041926" rel="noreferrer">different paper</a> currently with 145 Google Scholar citations, I was unable to reproduce a plot in one of the figures when using their parameter values. I kept insisting this, and the author I emailed kept insisting that there was no mistake, and after seven (very long, because there was a lot of detail involved to figure this out) back-and-forth emails, I got this eighth email:</p>
<blockquote>
<p>&quot;Hi Nike,</p>
<p>sorry for the late response.</p>
<p>I at least could check for your first question and I have to apologize
here. You are right and I simply misprinted the values. We actually
used  wc = (50 fs)^(-1) as was the values Ishizaki used in their PNAS
for  figure 2 and 3. They used the other value for figure 4, a
calculation  which we did not actually repeat.</p>
<p>Sorry for that but thanks for being insitant and pointing my nose to
my  mistake.&quot;</p>
</blockquote>
<p>This email from 14 September 2011 came several months after my initial email on 12 April 2011. The paper unfortunately remains with this error and no published Erratum or Comment. I don't know why they didn't publish an Erratum, but I can say that I didn't publish a Comment because I've in general been reluctant to do so (I feel that it's better to talk to the authors, and decide together what the best course of action is, and if they don't suggest publishing an Erratum together, then they probably don't want me publishing a Comment either; but now that I'm more senior I wouldn't be as hesitant to publish a Comment if I'm confident that I'm right).</p>
<p>In another highly-cited paper in PRL, the authors went into great depths to prove that a problem was in the complexity class <a href="https://en.wikipedia.org/wiki/QMA" rel="noreferrer">QMA</a> but a (different) undergrad student of mine found that the solution can actually be found in constant time, meaning that it's not only trivially in QMA but also in NP, P and even smaller complexity classes.</p>
<p>Similarly, there has been a highly-cited and published <a href="https://iopscience.iop.org/article/10.1088/1367-2630/13/11/113034/meta" rel="noreferrer">paper</a> in which the authors used 50 terms to represent a function as a sum of a particular type of basis function, when in fact I was able to get the same quality of fit with only two of the same type of basis function.</p>
<p>I can also provide several examples of errors or impactful typos in spectroscopy papers or quantum chemistry papers related to spectroscopic properties of molecules.</p>
<p>The <em><strong>one</strong></em> example I can think of, where I turned out to be wrong after contacting an author due to not being able to reproduce her result (in this case a mathematical derivation), my first email was an extremely carefully written report authored by myself (PhD student at the time), a post-doc, and our supervisor, and we didn't get any response back at all. It took me approximately two years to figure out (on my own!) why we were wrong and the original paper was right, but I had still never heard from the author of that paper. Several years later I met up with her, and then started having meals with her annually, and while I originally thought that not receiving a response from her was a sign of coldness and not wanting to talk to us, she turned out to be an extremely warm person who just might have had too much going on in her life to reply to the first email, or wasn't able to figure out the reason why <em><strong>my</strong></em> result didn't match hers.</p>
<hr>
<p>When things like these happen, it's best to compose a careful email to the authors (which you've already done!) and continue to investigate whether or not it's you that's wrong (which you are doing here). So you are going about this in the correct way! Keep in mind the timeline in my above examples (several months before the author in the first example finally agreed that the paper had an error; two years for me to find my mistake in the last example), which is there to show you that figuring out these things can take a very long time.</p>


================================================================================

Question: What is the current status of machine learning applied to materials or molecular systems?
Body: <p>I heard that machine learning techniques on materials use a large quantity of data to make predictions of a variety of features; for instance, a crystal structure. Data collected from empirical or high-level calculations could be used to correct DFT calculations for materials at, presumably, lower computational cost.</p>

<p>Basically, I would like to know what are the current advances made in machine learning methods applied to molecular systems or the design of materials.</p>

Best Answer: <ol>
<li><p>Here is state of the art research:</p>

<p><strong>Smith J.S. et al, <em>Approaching coupled cluster accuracy with a general-purpose neural network potential through transfer learning</em>, July 2019</strong> <a href="https://doi.org/10.1038/s41467-019-10827-4" rel="noreferrer"><em>Nat. Commun.</em> <strong>2019,</strong> <em>10</em> (1), No. 2903</a>, <a href="https://www.ncbi.nlm.nih.gov/pubmed/31263102" rel="noreferrer">PMID: 31263102</a></p>

<blockquote>
  <p>Computational modeling of chemical and biological systems at atomic resolution is a crucial tool in the chemist's toolset. The use of computer simulations requires a balance between cost and accuracy: quantum-mechanical methods provide high accuracy but are computationally expensive and scale poorly to large systems, while classical force fields are cheap and scalable, but lack transferability to new systems. Machine learning can be used to achieve the best of both approaches. Here we train a general-purpose neural network potential that approaches CCSD(T)/CBS accuracy on benchmarks for reaction thermochemistry, isomerization, and drug-like molecular torsions. This is achieved by training a network to DFT data then using transfer learning techniques to retrain on a dataset of gold standard QM calculations (CCSD(T)/CBS) that optimally spans chemical space. The resulting potential is broadly applicable to materials science, biology, and chemistry, and billions of times faster than CCSD(T)/CBS calculations.</p>
</blockquote></li>
<li><p>Here's a nice review paper:</p>

<p><strong>Dereinger, V.L. et. al., <em>Machine Learning Interatomic Potentials as Emerging Tools for Materials Science</em>, Nov 2019</strong> <a href="https://doi.org/10.1002/adma.201902765" rel="noreferrer"><em>Adv. Mater.</em> <strong>2019,</strong> <em>31</em> (46), 1902765</a>, <a href="https://www.ncbi.nlm.nih.gov/pubmed/31486179" rel="noreferrer">PMID: 31486179</a></p>

<blockquote>
  <p>Atomic-scale modeling and understanding of materials have made remarkable progress, but they are still fundamentally limited by the large computational cost of explicit electronic-structure methods such as density-functional theory. This Progress Report shows how machine learning (ML) is currently enabling a new degree of realism in materials modeling: by "learning" electronic-structure data, ML-based interatomic potentials give access to atomistic simulations that reach similar accuracy levels but are orders of magnitude faster. A brief introduction to the new tools is given, and then, applications to some select problems in materials science are highlighted: phase-change materials for memory devices; nanoparticle catalysts; and carbon-based electrodes for chemical sensing, supercapacitors, and batteries. It is hoped that the present work will inspire the development and wider use of ML-based interatomic potentials in diverse areas of materials research.</p>
</blockquote></li>
<li><p>And here's a nice overview:</p>

<p><strong>Mater &amp; Coote, <em>Deep Learning in Chemistry</em>, June, 2019</strong> <a href="https://doi.org/10.1021/acs.jcim.9b00266" rel="noreferrer"><em>J. Chem. Inf. Model.</em> <strong>2019,</strong> <em>59</em> (6), 2545–2559</a></p>

<blockquote>
  <p>Machine learning enables computers to address problems by learning from data. Deep learning is a type of machine learning that uses a hierarchical recombination of features to extract pertinent information and then learn the patterns represented in the data. Over the last eight years, its abilities have increasingly been applied to a wide variety of chemical challenges, from improving computational chemistry to drug and materials design and even synthesis planning. This review aims to explain the concepts of deep learning to chemists from any background and follows this with an overview of the diverse applications demonstrated in the literature. We hope that this will empower the broader chemical community to engage with this burgeoning field and foster the growing movement of deep learning accelerated chemistry.</p>
</blockquote></li>
</ol>


================================================================================

Question: What are the situations/problems where Born-Oppenheimer approximation is invalid?
Body: <p>We use the Born-Oppenheimer approximation in both Hartree-Fock method and DFT. What are the problems where we cannot use this approximations.</p>

Best Answer: <h2><strong>TL;DR conical intersections, and polarons. Or any other case when the velocity of the nuclei is faster than the electrons can respond nearly instantaneous</strong></h2>

<p>The long answer requires  a lot of mathematics!</p>

<hr>

<h2><strong>The Mathematical derivation</strong></h2>

<p>The total nuclear and electronic Hamiltonian can be written as </p>

<p><span class="math-container">$\hat{H} = \hat{T}_N + \hat{T}_e +\hat{V}_{ee} + \hat{V}_{NN} + \hat{V}_{eN}$</span></p>

<p><span class="math-container">$(\hat{T}_N + \hat{T}_e +\hat{V}_{ee} + \hat{V}_{NN} + \hat{V}_{eN})\Psi = E\Psi $</span></p>

<p>where N are the nuclei and e are the electrons. </p>

<p>The BO approximation starts by asking what happens if we we <em>freeze</em> the nuclei at position  <span class="math-container">$\mathbf{R}_0$</span> and take <span class="math-container">$\Phi(\mathbf{r})\chi(\mathbf{R}_0)$</span> as an ansatz of the total <span class="math-container">$\Psi$</span></p>

<p>This is mathematically much easier to solve because nuclear and electronic wavefunctions can be solved seperately. As we will see below, this is a good approximation in most situations. </p>

<p>First separating the electronic component we have</p>

<p><span class="math-container">$(\hat{T}_e  +\hat{V}_{ee} + \hat{V}_{NN} + \hat{V}_{eN})\Phi_k(\mathbf{r};\mathbf{R}_0) = E_{el,k}\Phi_k(\mathbf{r};\mathbf{R}_0) $</span></p>

<p>Combining this with the total Hamiltonian and the ansatz </p>

<p><span class="math-container">$[\hat{T}_{N} + E_{el,k}]\Phi_k(\mathbf{r};\mathbf{R}_0)\chi(R) = E\Phi_k(\mathbf{r};\mathbf{R}_0)\chi(R)$</span></p>

<p>Rearranging, </p>

<p><span class="math-container">$\frac{\hat{T}_{N}\Phi_k(\mathbf{r};\mathbf{R}_0)\chi(R)}{\Phi_k(\mathbf{r};\mathbf{R}_0)} +E_{el,k}\chi(R)  = E\chi(R)$</span></p>

<p>Now here is the technical <strong>BO approximation:</strong>:</p>

<p><span class="math-container">$\hat{T}_{N}\Phi_k(\mathbf{r};\mathbf{R}_0)\chi(R)\approx \Phi_k(\mathbf{r};\mathbf{R}_0)\hat{T}_{N}\chi(R)$</span> </p>

<p>Essentially this says that the kinetic energy of the nuclei doesn't affect the electronic states. Therefore, the nuclear wavefunction can be solved separately</p>

<p><span class="math-container">$[\hat{T}_{N} + E_{el,k}]\chi(R) = E\chi(R)$</span></p>

<p>Thus under the BO approx., <span class="math-container">$\chi(\mathbf{R})$</span> Is solved by solving a nuclear Schrodinger equation where the potential is given by <span class="math-container">$E_{el}$</span></p>

<hr>

<h2>But why is the B.O. approximation justified?</h2>

<p>The nuclear kinetic energy operators are derivatives with respect to the nuclei
<span class="math-container">$\hat{T}_{N}\Phi_k(\mathbf{r};\mathbf{R}_0)\chi(R)=-\sum_{l=1}^{N_a}\frac{1}{2M_l}\nabla_l^2[\Phi_k(\mathbf{r};\mathbf{R}_0)\chi(R)]$</span></p>

<p>Now doing some painful differentiation this equals</p>

<p><span class="math-container">$-\sum_{l=1}^{N_a}\frac{1}{2M_l} \Phi_k(\mathbf{r};\mathbf{R}_0) \nabla_l^2\chi(\mathbf{r}) - \sum_{l=1}^{N_a}\frac{1}{2M_l}\chi(\mathbf{r})\nabla_l^2\Phi_k(\mathbf{r};\mathbf{R}_0) - \sum_{l=1}^{N_a}\frac{1}{M_l}[\nabla_l\chi(\mathbf{R})][\nabla_l \Phi_k(\mathbf{r};\mathbf{R}_0)  ]$</span></p>

<p>The BO approx. amounts to assuming that the last two terms on the right hand side are negligible because</p>

<p><span class="math-container">$\frac{1}{M_l}[\nabla_l\chi(\mathbf{R})][\nabla_l \Phi_k(\mathbf{r};\mathbf{R}_0)]\approx \frac{1}{2M_l}\chi(\mathbf{R})\nabla_l^2 \Phi_k(\mathbf{r};\mathbf{R}_0) ≪ \frac{1}{2M_e}\chi(\mathbf{R})\nabla_e^2 \Phi_k(\mathbf{r};\mathbf{R}_0) $</span></p>

<p>which is in turn because 
<span class="math-container">$\nabla_r \Phi_k (r;R)≈∇_e Φ_k (r;R)$</span> and <span class="math-container">$m_e≪M_l$</span></p>

<p>In other words, because  the mass of an electron is much less than the mass of a nuclei it moves slower and the nuclei can be considered stationary with respect to the motion of the electron. Whew!</p>

<p>However, this reveals something <strong>VERY</strong> interesting. As we can see, it tells us that we can expect breakdown of the BO approximation any time the nuclear derivative of the electronic wavefunction changes rapidly. </p>

<p>For example in a <a href="https://en.wikipedia.org/wiki/Conical_intersection" rel="noreferrer">conical intersection</a>!!!</p>

<p>As you can see the electronic wavefunction changes rapidly with nuclear coordinates and it actually has a discontinuity.</p>

<p><a href="https://i.sstatic.net/uo3dU.jpg" rel="noreferrer"><img src="https://i.sstatic.net/uo3dU.jpg" alt="enter image description here"></a></p>

<p>Polarons are another example which have breakdown of the BO approximation and that's simply because the nuclei and electron are so far apart that the electrons cannot respond fast enough to the nuclei.</p>


================================================================================

Question: When and Why does Density Functional Theory (DFT) fail?
Body: <p>Every method has its strengths and weaknesses. For instance, a strength of DFT is that is has HF like speeds, but can also account for electron-correlation and this is a pretty big feature since methods that account for electron correlation typically either require more than 1 Slater determinant (Configuration Interaction, Coupled Cluster etc.), or involve expensive perturbation about a HF reference system (Many Body Perturbation Theories).</p>

<p>A result of DFT being so pound-for-pound good is that its popularity is soaring.</p>

<p>Given the relative ease in which a DFT calcualtion can be performed, this makes it perfect for "turning the crank".</p>

<p>There are however instances where DFT fails or has caveats.</p>

<h2>What are the systems/phenomena/caveats users should be aware of when modelling materials with DFT?</h2>

<p>For instance, one caveat I know of is that because of the complexity of the exchange/correlation functionals, they must be numerically integrated. This means a grid-size must be set, and while programs such as Gaussian allow the user to set the grid-size, generally, a default grid-size is used unbeknownst to the novice user.</p>

Best Answer: <p>First, a word of caution: it is hard to generalize since there are so many different approximations to the exact exchange-correlation functional. Nonetheless, in my opinion:</p>

<ol>
<li><p>The biggest weakness of all existing (and arguably all plausible) implementations of DFT is their <strong>limited predictive power</strong>. In practice, this means that you need to know a lot about your system to choose the right method (functional) and you can only sort of trust the answer (as evidenced by the variety of answers you get when using different functionals). More precisely, your degree of confidence decreases as you move to more "weird" systems, that are not usually used to inform the construction of the exchange-correlation functionals.</p></li>
<li><p>Density functionals are <strong>not systematically improvable</strong>. There is no guarantee that using the density functionals higher up on the Jakob's ladder will give a more accurate answer. This is fundamentally different from the coupled-cluster or configuration interaction approaches. For a recent long-read review check out  <a href="https://doi.org/10.1080/00268976.2017.1333644" rel="noreferrer">this work from Martin Head-Gordon group</a></p></li>
<li><p>Finally, the <strong>grid convergence</strong> you mentioned is a big issue for an average user that is treating DFT implementations as black-box methods. <a href="https://chemrxiv.org/articles/Popular_Integration_Grids_Can_Result_in_Large_Errors_in_DFT-Computed_Free_Energies/8864204/5" rel="noreferrer">Steven Wheeler has explored this recently</a>.</p></li>
</ol>


================================================================================

Question: Quantum ESPRESSO vs VASP
Body: <p>We know that both Quantum ESPRESSO and VASP can perform lots of different kinds of simulations of materials. But what can VASP do that Quantum ESPRESSO can not, and <em>vice-versa</em>? </p>

Best Answer: <p>There is another (good) answer, and since I can't comment yet I will add a few things, possibly from a different perspective as someone who uses QE more often.</p>

<p>In general, QE is a free (GPL 2.0), open source code, and that comes with some advantages. New methods can be implemented relatively quickly. If you will be using DFT+U, QE has hp.x, for calculating Hubbard U values from first principles using density functional perturbation theory (see Phys. Rev. B 98, 085127). To my knowledge VASP does not have this capability. And QE DOES implement forces and stresses using DFT+U when using the "simplified" rotationally-invariant formulation (which is default), but you are limited in the choice of orbital for the projection to either atomic wavefunctions or the pseudopotential projectors. In general though, you are not limited to ortho-atomic orbitals--there are a few options, and you can also provide your own generated wavefunction files (i.e. from Wannier codes) for the projection.</p>

<p>QE also has some interesting methods for boundary conditions and periodicity, such as using the effective screening method (Phys. Rev. B 73, 115407) for polarized/charged slab calculations. Again, to the best of my knowledge, VASP can only apply dipole corrections in this case (which QE can also do).</p>

<p>The advantages of VASP that stick out the most to me are the handling of hybrid functional calculations, and slightly fewer restrictions on the types of calculations you can do (though not true with simplified DFT+U, there are indeed situations in QE where you don't have forces or stresses implemented yet). It's been reported that VASP shows better scaling for parallel computation but this may just be in systems with many electrons, and was on a much older version of QE (and may no longer be true). QE is also improving its treatment of hybrid functionals (the speedup from the ACE algorithm is significant, and finally ultrasoft and PAW pseudopotentials can be used). I would not say that in general, VASP is faster, though it may be in some particular cases.</p>

<p>I'll finish by just quickly mentioning pseudopotentials. Techically this isn't something one code can do that the other can't, but I thought this is important to clarify. It is true that VASP uses PAW by default, and has a fairly well-tested set of pseudopotentials for most elements. However, while QE doesn't include pseudopotentials by default, I would not say that most users typically use "slow" norm-conserving pseudopotentials, nor is the situation some "wild west" of unreliability. A widely used set (PSLibrary) comes in both PAW and ultrasoft versions. The Materials Cloud project has compiled a set of pseudopotentials from many sources to give the lowest errors vs. all-electron calculations, while prioritizing either efficiency (lower cutoffs) or accuracy. And new "ONCV" norm-conserving pseudopotentials (Phys. Rev. B 88, 08511) are almost as soft as ultrasoft and PAW (see pseudo-dojo.org).</p>

<p>If I were deciding which code to use, it would strongly depend on what specific types of calculation I want to do, though I admit both codes have a lot of capabilities and its quite difficult to keep track of everything. Sometimes the professor or research group you work in is simply more familiar with a certain code, so you use that!</p>


================================================================================

Question: What is matter modeling?
Body: <p>Perhaps a stupid question, but.. <strong>what is matter modeling?</strong></p>
<p>I've never heard of it, and a google search just returns news articles about this site that seem to assume you already know what it is.  I can't even find a Wikipedia article!</p>

Best Answer: <h3>&quot;Matter modeling&quot; is a term that was coined by Stack Exchangers!</h3>
<p>That's why the words &quot;matter&quot; and &quot;modeling&quot; next to each other, don't seem to exist in <em><strong>any</strong></em> of the results in a Google search of &quot;Matter Modeling&quot;, apart from this Stack Exchange site, Adam Iaizzi's blog post about this SE site, and <em>maybe</em> our community's <a href="https://www.facebook.com/search/top?q=Matter%20Modeling%20Stack%20Exchange" rel="noreferrer">Facebook group</a> but for some reason the last one didn't show up in my Google search just now.</p>
<p>Basically, people trying to model physical matter (materials, molecules, atoms, nuclei, etc.) have so much in common. All of this type of matter is described by the Schrödinger equation, or if modeling on a much more macroscopic scale: by Newton's second law. So we are all basically <em><strong>writing</strong></em> or <em><strong>running</strong></em>  computer programs to solve Schrödinger equations (or Kohn-Sham equations for those using the mathematically equivalent <a href="/questions/tagged/density-functional-theory" class="post-tag" title="show questions tagged &#39;density-functional-theory&#39;" rel="tag">density-functional-theory</a>) or Newton's equations to study some phenomena about matter. Some of us might not be coding or computing, but instead are deriving equations (mathematics) or studying the computational complexity of an algorithm (theoretical computer science) for the modeling of matter (see <a href="/questions/tagged/mathematical-modeling" class="post-tag" title="show questions tagged &#39;mathematical-modeling&#39;" rel="tag">mathematical-modeling</a> and <a href="/questions/tagged/algorithms" class="post-tag" title="show questions tagged &#39;algorithms&#39;" rel="tag">algorithms</a>). Some might be using <a href="/questions/tagged/machine-learning" class="post-tag" title="show questions tagged &#39;machine-learning&#39;" rel="tag">machine-learning</a>, or maybe even doing <a href="/questions/tagged/experimental" class="post-tag" title="show questions tagged &#39;experimental&#39;" rel="tag">experimental</a> research but need help in analyzing or interpreting the data, or a modeler might be comparing theory to experiment and may have an <a href="/questions/tagged/experimental" class="post-tag" title="show questions tagged &#39;experimental&#39;" rel="tag">experimental</a> question for that reason.</p>
<p>There's a lot of <a href="/questions/tagged/software" class="post-tag" title="show questions tagged &#39;software&#39;" rel="tag">software</a> we use (dozens and dozens of software packages), and many of them had their own independent forums requiring users to register a username and password for each one, and sometimes a question on the <a href="/questions/tagged/vasp" class="post-tag" title="show questions tagged &#39;vasp&#39;" rel="tag">vasp</a> forum could better be answered by a <a href="/questions/tagged/quantum-espresso" class="post-tag" title="show questions tagged &#39;quantum-espresso&#39;" rel="tag">quantum-espresso</a> user because some software can do certain types of calculations which others don't. Physics.SE, Chemistry.SE, QuantumComputing.SE, ComputationalScience.SE, ComputerScience.SE, TheoreticalCS.SE, ArtificialIntelligence.SE, Statistics.SE, Engineering.SE, Mathematics.SE, MathOverflow and others had matter modeling questions <em><strong>sprinkled</strong></em> across them, but it was spread out and never all in one place. We also do a lot of <a href="/questions/tagged/high-performance-computing" class="post-tag" title="show questions tagged &#39;high-performance-computing&#39;" rel="tag">high-performance-computing</a> which is on-topic but very &quot;niche&quot; on AskUbuntu, StackOverflow, Linux&amp;Unix, ServerFault, etc. (see this deleted <a href="https://meta.stackexchange.com/q/327714/391772">Meta SE question</a> if you have enough reputation there!). This was discussed on <a href="https://area51.meta.stackexchange.com/q/29831/190792">Area51</a>.</p>
<p>The name of our site began as &quot;Materials Modeling SE&quot;, which <a href="https://mattermodeling.stackexchange.com/q/110/5">does</a> have a Wikipedia page and a <em>little</em> bit of information available online, and while you can argue (for example) that atoms are made of &quot;sub-atomic material&quot;, the word material often refers to solid  materials (see our tag <a href="/questions/tagged/solid-state-physics" class="post-tag" title="show questions tagged &#39;solid-state-physics&#39;" rel="tag">solid-state-physics</a>) which is a bit too specific to describe this community.</p>
<p>We discussed what we should call ourselves, twice on Area51:</p>
<ul>
<li><a href="https://area51.meta.stackexchange.com/q/31121/190792">Discussion about the name for Materials/Matter Modeling</a></li>
<li><a href="https://area51.meta.stackexchange.com/q/31085/190792">What should this site be called?</a></li>
</ul>
<p>and then again when the site entered Private Beta:</p>
<ul>
<li><a href="https://mattermodeling.meta.stackexchange.com/q/96/5"><em><strong>Better late than too late: Could a single word substitution in our name, be at least considered?</strong></em></a></li>
</ul>
<h3>Did we really just make up a new term?</h3>
<p>Even outside of Stack Exchange, pre-cursors started to pop up around the world, as <a href="https://mattermodeling.meta.stackexchange.com/a/102/5">this answer</a> points out that a relatively new <em><strong>journal</strong></em> called <a href="https://www.cell.com/matter/home" rel="noreferrer">&quot;Matter&quot;</a> was launched as a sister journal to &quot;Cell&quot; and &quot;Chem&quot;, and Alan Aspuru-Guzik, an always creative and forward-thinking scientist who did computational research for both molecules and solid-state materials, renamed his research group the <a href="https://www.matter.toronto.edu/" rel="noreferrer">Matter Lab</a> when he moved from Harvard to Toronto, and he took the domain matter.toronto.edu (we also wanted matter.stackexchange.com but spent all our energy on changing &quot;Materials Modeling&quot; to &quot;Matter Modeling&quot; so we postponed that feature request for later!).</p>


================================================================================

Question: What is the appropriate way of determining a value for the Hubbard-like potential U for LDA+U / GGA+U calculations?
Body: <p>It is known that neither LDA or GGA correctly account for the strong on-site Coulomb interaction of localized electrons<a href="https://wiki.fysik.dtu.dk/gpaw/tutorials/hubbardu/hubbardu.html" rel="noreferrer">[1]</a>. A cost-effective way of correcting this is by using a Hubbard-like term <span class="math-container">$U$</span>. In literature, I have often found that <span class="math-container">$U$</span> is introduced as an empirical parameter, taken from previous computational studies or experiments. Alternatively, I have seen studies where various values for <span class="math-container">$U$</span> are used (typically between 2 and 5 eV). Neither of these methods seem too convinving. 
Is there a standard and reliable manner of determining the (best) value for <span class="math-container">$U$</span>?</p>

Best Answer: <p>This is a bit late, but I would say the short answer to your direct question is: technically no, there is no "standard" reliable way, since there are several approaches to determining <em>U</em> self-consistently from first principles. By self-consistently I mean a first-principles <em>U</em> is calculated, which when applied changes the electronic structure, so a new <em>U</em> is calculated and applied, and so on, until the value of <em>U</em> has converged. However, these methods are each fairly reliable and often give similar results. This is an area I've worked in a bit so I'll summarize a few methods here, some ongoing work in this area, some resources for further reading and practical application, and some recommended practices. This is in the context of my experience in solid-state periodic DFT but in principle it should apply to any DFT calculation where these methods are possible and appropriate.</p>

<h2>1. Hubbard <em>U</em> and Determination of <em>U</em> from First-Principles</h2>

<p><strong>a. Basic Concepts</strong></p>

<p>The basic idea of DFT+<em>U</em> is that we are <em>replacing</em> the treatment of some of the charge density with a Hubbard correction term, where otherwise it would be treated by the chosen DFT exchange-correlation (XC) functional. This correction, in practice, is usually a screened Hartree-Fock-like term as opposed to being more reminiscent of the actual Hubbard model. A double-counting term is subtracted to remove the estimated contribution from the original XC functional. However, this term is not uniquely defined, so the choice of double-counting correction will affect the results. Most calculations use a fully-localized limit, most appropriate for strongly localized states.</p>

<p>DFT+<em>U</em> has the effect of introducing discontinuity in the total energy as a function of electrons added to the system--the rationale behind why it can improve the prediction of band gaps (if this is unclear, read on the band gap problem in DFT). In fact, the value of <em>U</em> can be defined in terms of this unphysical curvature present in LDA and GGA. A more intuitive interpretation of DFT+<em>U</em> can be illustrated by the form of the rotationally-invariant "simplified" correction (reducing explicit <em>U</em> and <em>J</em> terms to an effective <span class="math-container">$U_{\text{eff}}=U-J$</span>) developed by <a href="https://link.aps.org/doi/10.1103/PhysRevB.57.1505" rel="noreferrer">Dudarev <em>et al.</em></a> and <a href="https://link.aps.org/doi/10.1103/PhysRevB.71.035105" rel="noreferrer">Cococcioni <em>et al.</em></a>, where the Hubbard energy is minimized when orbitals in the Hubbard manifold are either completely <em>full</em> or <em>empty</em>, i.e. no partial occupations due to hybridization. This means there is a tendency to decrease delocalization in the calculation (compensating for how GGA, for example, tends to over-delocalize).</p>

<p>A more recent development is <a href="https://iopscience.iop.org/article/10.1088/0953-8984/22/5/055602" rel="noreferrer">DFT+<em>U</em>+<em>V</em></a>, which includes inter-site interaction <em>V</em> and can improve the description of covalently-bonded materials.</p>

<p>I recommend reading this <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/qua.24521" rel="noreferrer">excellent review article</a> by Himmetoglu <em>et al.</em> for an overview of DFT+<em>U</em>.</p>

<p><strong>b. Constrained Random Phase Approximation</strong></p>

<p>The constrained random phase approximation (cRPA), developed by <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.70.195104" rel="noreferrer">Aryasetiawan <em>et al.</em></a>, is used to find a frequency-dependent <em>U</em> that can be used in DFT+DMFT, but in the static limit it can be used in DFT+<em>U</em>. Essentially, once you have chosen how to define the localized and delocalized states for your system, the Coulomb interaction between the localized states is calculated while including screening effects from the delocalized states. The "constrained" comes from the fact that only the Hartree term is used to calculate the dielectric function used for screening, for simplicity (as opposed to both Hartree and exchange-correlation). I personally have not used cRPA to calculate values of <em>U</em>, so check the reference paper and its citations for further reading on the topic.</p>

<p><strong>c. Linear Response</strong></p>

<p>The linear response method by <a href="https://link.aps.org/doi/10.1103/PhysRevB.71.035105" rel="noreferrer">Cococcioni <em>et al.</em></a> defines <em>U</em> such that when applied the unphysical curvature in the total energy vs. the number of electrons present in the system is eliminated. In exact DFT this is a piecewise-continuous linear function, while in approximate LDA and GGA DFT it is a smooth curved function. <em>Constrained</em> DFT was an approach to correct this by varying the Hubbard orbital occupations, and determining the corresponding change in energy. Linear response approaches this in a more convenient way for most DFT codes, by applying a varying perturbative potential and then measuring the resulting change in occupation. Once this is done for several perturbations, <em>U</em> can be calculated (see resources section for how to do this). This approach typically requires <em>U</em> to be calculated in a supercell, to prevent the Hubbard states from being affected by periodic images of the perturbation.</p>

<p><strong>d. Density Functional Perturbation Theory</strong></p>

<p>A very nice recent development in the calculation of <em>U</em> is from <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.98.085127" rel="noreferrer">Timrov, Mazari and Cococcioni</a>. They reformulate the linear response method from a single perturbation in a supercell to a sum of perturbations in the primitive cell. I'm not as familiar with the theory here, but it's implemented in the new hp.x code included with Quantum Espresso. Very interesting.</p>

<p><strong>d. ACBN0</strong></p>

<p><a href="https://journals.aps.org/prx/abstract/10.1103/PhysRevX.5.011006" rel="noreferrer">ACBN0</a> (named for the authors) takes inspiration from previous work by <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.76.155123" rel="noreferrer">Mosey and Carter</a>, and explicitly calculates <em>U</em> from a Hartree-Fock-like interaction between the Hubbard orbitals of interest. Some screening-like reduction of the interaction is introduced by renormalizing the occupations of the Kohn-Sham orbitals according to their projectability on the Hubbard basis--so less-localized states should have a drastically reduced magnitude of <em>U</em>. This method also allows calculation of many site-dependent <em>U</em> values from a single scf calculation. In theory this can be incorporated into the self-consistency loop of the DFT calculation, but current implementations are a post-processing step. It can be used in the <a href="http://aflowlib.org/src/paoflow/" rel="noreferrer">PAOFLOW</a> and <a href="http://aflowlib.org/src/aflowpi/index.html" rel="noreferrer">AFLOW<span class="math-container">$\pi$</span></a> codes. It's also been <a href="http://arxiv.org/abs/1911.10813" rel="noreferrer">recently demonstrated</a> with DFT+<em>U</em>+<em>V</em>.</p>

<h2>2. Resources</h2>

<p><strong>a. References for Implementations of Hubbard <em>U</em> Corrections</strong></p>

<ul>
<li><a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.52.R5467" rel="noreferrer">Rotationally-invariant implementation with explicit <em>U</em> and <em>J</em></a></li>
<li><a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.57.1505" rel="noreferrer">Simplified rotationally-invariant scheme (<span class="math-container">$U_{\text{eff}}$</span>)</a> (also <a href="https://link.aps.org/doi/10.1103/PhysRevB.71.035105" rel="noreferrer">this</a>)</li>
<li><a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.84.115108" rel="noreferrer">Simplified scheme re-introducing explicit <em>J</em> term</a></li>
</ul>

<p><strong>b. Future Possibilities</strong></p>

<p>A limitation of the simplified implementations of DFT+<em>U</em> is that the Hubbard orbitals are treated in a way that assumes some spherical symmetry (i.e. very close to true atomic orbitals). In compounds that have significant crystal field splitting this is not technically appropriate. Some early work used different <em>U</em> values for <span class="math-container">$t_{2g}$</span> and <span class="math-container">$e_{g}$</span> electrons in perovskite oxides, for example. The exchange term <em>J</em> is also treated in an average way, which worsens treatment of materials where the localization depends on Hund's rule magnetism. It could be very interesting to determine <em>U</em>, <em>J</em> and <em>V</em> for specific subsets of the Hubbard orbitals. This may provide better treatment of orbitally-ordered materials, or materials where some of the <em>d</em> electrons form an itinerant band (and should have a much smaller <em>U</em> value than the localized states, and may also participate in screening).</p>

<p><strong>c. Practical Information/Tutorials</strong></p>

<p>Prof. Heather Kulik at MIT has some nice tutorials and slides on DFT+U:</p>

<ul>
<li><a href="http://hjkgrp.mit.edu/content/review-dftu" rel="noreferrer">Intro to DFT+<em>U</em></a></li>
<li><a href="http://hjkgrp.mit.edu/content/calculating-hubbard-u" rel="noreferrer">Calculating Hubbard <em>U</em></a>, <a href="http://hjkgrp.mit.edu/content/hubbard-u-multiple-sites" rel="noreferrer"><em>U</em> for multiple sites</a>, <a href="http://hjkgrp.mit.edu/content/calculating-hubbard-u-periodic-systems" rel="noreferrer"><em>U</em> for periodic systems</a>, <a href="http://hjkgrp.mit.edu/content/readers-choice-dftu" rel="noreferrer">calculating <em>U</em> and <em>J</em></a></li>
<li><a href="http://hjkgrp.mit.edu/content/troubleshooting-common-problems-dftu" rel="noreferrer">Troubleshooting common problems</a>, <a href="http://hjkgrp.mit.edu/content/right-hubbard-u-any-element" rel="noreferrer">the right <em>U</em> for any element</a></li>
<li><a href="http://hjkgrp.mit.edu/content/including-hubbard-u-variations-dftur" rel="noreferrer">Including Hubbard <em>U</em> variations with DFT+<em>U</em>(<strong>R</strong>)</a></li>
</ul>

<h2>3. Recommended Practices</h2>

<ol>
<li>Values of <em>U</em> are in general <em>non-transferable</em>. It is poor practice to take a literature value of <em>U</em> when the work does not use the same functional, DFT+<em>U</em> implementation, material (especially drastically different chemical environments), Hubbard basis, or even pseudopotential that you are using. This is especially true for values of <em>U</em> chosen empirically for a different property than the one you're studying.</li>
<li>Calculate <em>U</em> self-consistently whenever possible. It's becoming easier and easier to do this.</li>
<li>Think carefully about which atoms and states you want to apply <em>U</em> to. It's sometimes useful to apply <em>U</em> to oxygen <em>p</em> states in addition to metal <em>d</em> states in oxides, for example.</li>
<li>DFT+<em>U</em> often introduces local minima in the energy, related to the occupations of your Hubbard atom states. This seems especially pronounced in spin-polarized calculations. Manually setting the starting occupation matrix to what you are trying to study can help converge to the appropriate energy minimum. This is useful when trying to study different possible spin states on a metal cation, for example. It can also help speed up convergence, in my experience.</li>
<li>If you are choosing <em>U</em> empirically, think about whether you are getting the "right" answer for the wrong reason. Band gaps can be correct at the expense of a realistic picture of electronic structure, not to mention other quantities like lattice parameters, formation enthalpy, etc. Compare your calculations with higher levels of theory in the literature when possible. </li>
<li>Current DFT+<em>U</em> implementations are not, to my knowledge, variational with respect to the total energy. So the optimal value of <em>U</em> will not typically result in the lowest calculated total energy. To make comparisons between different calculations, you need to be using the same values of <em>U</em> in both calculations--unless you specifically account for the change in the energy surface with respect to <em>U</em> (as in <a href="https://scitation.aip.org/content/aip/journal/jcp/135/19/10.1063/1.3660353" rel="noreferrer">DFT+<em>U</em>(<strong>R</strong>)</a> or <a href="https://aip.scitation.org/doi/full/10.1063/1.4916823" rel="noreferrer">DFT+<em>U</em>(V)</a>).</li>
</ol>


================================================================================

Question: How big should a supercell be in phonon calculations?
Body: <p>Is 2x2x2 enough, or is larger needed? I know that a convergence test must be carried out, but increasing supercell size enormously increases computational time, and I am using the Python program Phonopy.</p>

Best Answer: <p><strong>Quick Summary</strong>: There's no way around performing a convergence test. However, it is possible to obtain convergence <em>much</em> faster than the Phonopy approach by using nondiagonal supercells [<a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.92.184301" rel="noreferrer">1</a>].</p>
<hr>
<p>The basic quantity you build when performing a phonon calculation is the matrix of force constants, given by:</p>
<p><span class="math-container">$$
D_{i\alpha,i^{\prime}\alpha^{\prime}}(\mathbf{R}_p,\mathbf{R}_{p^{\prime}})=\frac{\partial^2 E}{\partial u_{p\alpha i}\partial u_{p^{\prime}\alpha^{\prime}i^{\prime}}},
$$</span></p>
<p>where <span class="math-container">$E$</span> is the potential energy surface in which the nuclei move, <span class="math-container">$u_{p\alpha i}$</span> is the displacement of atom <span class="math-container">$\alpha$</span> (of all atoms in the basis), in Cartesian direction <span class="math-container">$i$</span> (<span class="math-container">$x$</span>, <span class="math-container">$y$</span>, <span class="math-container">$z$</span>), and located in the cell within the supercell at <span class="math-container">$\mathbf{R}_p$</span>. This matrix of force constants is, roughly speaking, measuring the following: if I move an atom at <span class="math-container">$\mathbf{R}_p$</span>, what force does an atom at <span class="math-container">$\mathbf{R}_{p^{\prime}}$</span> feel? If the atoms are sufficiently far apart <span class="math-container">$|\mathbf{R}_p-\mathbf{R}_{p^{\prime}}|\gg1$</span>, then the atoms do not feel the force, and <span class="math-container">$D_{i\alpha,i^{\prime}\alpha^{\prime}}(\mathbf{R}_p,\mathbf{R}_{p^{\prime}})\to0$</span>.
So you need a supercell large enough to capture all relevant non-zero entries in the matrix of force constants. An equivalent picture emerges when we consider the relation between a supercell of size <span class="math-container">$N_1\times N_2\times N_3$</span>, which is equivalent to sampling the Brillouin zone (BZ) of the system with a <span class="math-container">$\mathbf{q}$</span> grid of size <span class="math-container">$N_1\times N_2\times N_3$</span> (as the BZ of the supercell is correspondingly smaller compared to the BZ of the primitive cell). In this language, you need a <span class="math-container">$\mathbf{q}$</span>-point grid sampling the BZ that is large enough.</p>
<p>So how quickly does <span class="math-container">$D_{i\alpha,i^{\prime}\alpha^{\prime}}(\mathbf{R}_p,\mathbf{R}_{p^{\prime}})$</span> go to zero? There is no general answer to this question, it is system-dependent. Therefore, you <em>must</em> perform a convergence test. One thing to note is that the size of your primitive cell will play a role: if you are looking at diamond, with a very small primitive cell containing only 2 atoms, then a <span class="math-container">$2\times 2\times 2$</span> grid will definitely not be large enough. However, if you consider a system with a primitive cell containing many atoms, for example <span class="math-container">$\ce{In_2O_3}$</span> with 40 atoms in the primitive cell, then a <span class="math-container">$2\times 2\times 2$</span> grid may be enough. Another thing to consider is the shape of the primitive cell. If your primitive cell is very elongated along one direction, then the distances are larger already along that direction, so you would probably be better off with a non-uniform sampling grid.</p>
<p><strong>Diagonal supercells.</strong> So how are calculations performed in practice? When you need to sample a <span class="math-container">$\mathbf{q}$</span>-point grid of size <span class="math-container">$N_1\times N_2\times N_3$</span>, then a code like Phonopy builds a supercells of size <span class="math-container">$N_1\times N_2\times N_3$</span>. This is accomplished using what I call a <em>diagonal supercell</em>:</p>
<p><span class="math-container">$$
\begin{pmatrix}
\mathbf{a}_{s_1} \\
\mathbf{a}_{s_2} \\
\mathbf{a}_{s_3} 
\end{pmatrix}
=
\begin{pmatrix}
N_1 &amp; 0 &amp; 0 \\
0 &amp; N_2 &amp; 0 \\
0 &amp; 0 &amp; N_3
\end{pmatrix}
\begin{pmatrix}
\mathbf{a}_{p_1} \\
\mathbf{a}_{p_2} \\
\mathbf{a}_{p_3} 
\end{pmatrix},
$$</span>
where <span class="math-container">$(\mathbf{a}_{s_1},\mathbf{a}_{s_2},\mathbf{a}_{s_3})$</span> are the supercell lattice parameters, and <span class="math-container">$(\mathbf{a}_{p_1},\mathbf{a}_{p_2},\mathbf{a}_{p_3})$</span> are the primitive cell lattice parameters. As you correctly say, this can very quickly become extremely expensive computationally. There are many published phonon calculations that are not properly converged because of this computational bottleneck. However, if you want to do a proper job, there is no way around performing a convergence test. However, things can be done better than this.</p>
<p><strong>Nondiagonal supercells.</strong> It was recently pointed out that, in order to sample a <span class="math-container">$\mathbf{q}$</span>-point grid of size <span class="math-container">$N_1\times N_2\times N_3$</span>, it is possible to build <em>smaller</em> supercells that are mathematicall exactly equivalent to the diagonal supercells. These are called <em>nondiagonal supercells</em> because they exploit the fact that you can build an equally valid supercell not only by scaling the primitive cell lattice vectors, but also by making linear combinations of them. In this case, you obtain:</p>
<p><span class="math-container">$$
\begin{pmatrix}
\mathbf{a}_{s_1} \\
\mathbf{a}_{s_2} \\
\mathbf{a}_{s_3} 
\end{pmatrix}
=
\begin{pmatrix}
S_{11} &amp; S_{12} &amp; S_{13} \\
S_{21} &amp; S_{22} &amp; S_{23} \\
S_{31} &amp; S_{32} &amp; S_{33}
\end{pmatrix}
\begin{pmatrix}
\mathbf{a}_{p_1} \\
\mathbf{a}_{p_2} \\
\mathbf{a}_{p_3} 
\end{pmatrix},
$$</span></p>
<p>where the <span class="math-container">$S_{ij}$</span> entries are not necessarily zero for <span class="math-container">$i\neq j$</span>. Exploiting this additional degree of freedom, then when you want to sample a <span class="math-container">$\mathbf{q}$</span>-point grid of size <span class="math-container">$N_1\times N_2\times N_3$</span>, the largest supercell you need is of size given by the <em>lowest common multiple</em> of <span class="math-container">$N_1$</span>, <span class="math-container">$N_2$</span>, and <span class="math-container">$N_3$</span>.</p>
<p>This leads to a dramatic reduction of computational time: if you are interested in sampling a <span class="math-container">$\mathbf{q}$</span>-point grid of size <span class="math-container">$N\times N\times N$</span>, then with diagonal supercells (e.g. Phonopy), you need a supercell of size <span class="math-container">$N^3$</span>. With nondiagonal supercells, you need a supercell of size <span class="math-container">$N$</span>. In the original paper there is an extreme example for calculating the phonons of diamond using a <span class="math-container">$\mathbf{q}$</span>-point grid of size <span class="math-container">$48\times48\times48$</span>. Using Phonopy this would be completely impossible, as it would require a supercell of size 110,592 (containing 221,184 atoms)! This calculation is in fact possible (and relatively easy), using nondiagonal supercells, which would only require a supercell of size 48 (containing 96 atoms).</p>
<p>Disclaimer: I am an author of the nondiagonal supercell paper.</p>
<ol>
<li>Lloyd-Williams, J., &amp; Monserrat, B. (2015). Lattice dynamics and electron-phonon coupling calculations using nondiagonal supercells, Phys. Rev. B, 92, 184301 DOI: 10.1103/PhysRevB.92.184301.</li>
</ol>


================================================================================

Question: How does the recent Chinese quantum supremacy claim compare with Google&#39;s?
Body: <p>Very recently, China <a href="https://www.livescience.com/china-quantum-supremacy.html" rel="noreferrer">claims</a> achieving 'Quantum Supremacy' with the world's fastest Quantum Computer. Their computer was designed as a circuit of lasers, beam splitters and mirrors (see <a href="https://science.sciencemag.org/content/early/2020/12/02/science.abe8770" rel="noreferrer">figure bellow</a>) and to solve a very specific problem called Gaussian boson sampling.</p>
<img src="https://i.sstatic.net/UB0SU.jpgg" width="400" height="400">
<p>The quantum computer from Google, for example, consists of microscopic circuits of superconducting metal that entangle 53 qubits in a complex superposition state. In the case of IBM, their quantum chips are formed by <a href="https://www.ibm.com/quantum-computing/developers/" rel="noreferrer">microwave resonators in combination with Josephson superconducting junctions</a>. In case of the IBM, there is already, an open source SDK called <a href="https://www.ibm.com/quantum-computing/developers/" rel="noreferrer">Qiskit</a> that expresses quantum computing concepts intuitively and concisely in Python. In both cases, it is possible to program the computers to solve different tasks.</p>
<p>My questions is: <strong>how honest is the Chinese claim of quantum supremacy?</strong></p>

Best Answer: <blockquote>
<p>&quot;how honest is the Chinese claim of quantum supremacy?&quot;</p>
</blockquote>
<p>It's equally (or at least as) honest in comparison to Google's claim.</p>
<p>In a comment to <a href="https://quantumcomputing.stackexchange.com/questions/11895/why-google-has-used-sqrtx-and-sqrty-instead-of-x-and-y-in-supremac/11955#comment15587_11955">this answer at Quantum Computing Stack Exchange</a>, Craig Gidney (who works at Google and was a co-author on Google's Quantum Supremacy paper), confirmed that the classical computer would have been <code>2^(20*7/4) = 34359738368</code> times faster if Google's hardware used CZ gates instead of their gates. This would mean that the 10,000 years that they said it would take classical computers to do the calculation that Google did, would become 9 seconds.</p>
<p>Furthermore IBM pointed out that the estimations Google made about the time it would take the classical computer, assumed that the classical computer could only use RAM and no disk at all, and if you allow the classical computer to use disk (which in this case, since they used Oak Ridge National Lab's Summit supercomputer, has petabytes available rather than just terabytes) then even though I/O is slower on disk than RAM you will save so much time by not re-calculating things, that you will get several orders of magnitude further speed-up for the classical computer.</p>
<p>I'm not aware of anything like this for the Chinese experiment, but to be clear: <a href="https://en.wikipedia.org/wiki/Boson_sampling" rel="nofollow noreferrer">boson sampling</a> is not enough to do &quot;universal&quot; computation, however universal computation is <em>not</em> a requirement for quantum supremacy. The probability distribution from which boson-samplers sample, is <a href="https://en.wikipedia.org/wiki/%E2%99%AFP" rel="nofollow noreferrer">#P</a>-difficult to calculate on a classical computer, which means that no polynomially scaling classical algorithm is known to be able to simulate such a distribution. <a href="https://quantumcomputing.stackexchange.com/q/2244/2293">Discussions have taken place</a> on the Quantum Computing Stack Exchange about whether or not boson sampling can be used to &quot;calculate&quot; the permanent of a complex-valued matrix (this is known to be a #P problem for classical computers), for example.</p>
<p>Furthermore, Figure 1 of <a href="https://arxiv.org/abs/1805.01450" rel="nofollow noreferrer">this paper</a> shows that it wasn't hard for the authors to simulate 144 qubits on a classical computer. That paper was published 1 year earlier than Google's quantum &quot;supremacy&quot; claim! This was discussed in more detail <a href="https://quantumcomputing.stackexchange.com/a/30167/2293">here</a>.</p>
<p>In any case, &quot;quantum supremacy&quot; is a buzzword and until people are actually using quantum devices to solve useful problems, classical computers are superior because people actually find them valuable enough to purchase.</p>


================================================================================

Question: Is there a &quot;gold standard&quot; method in materials modeling for obtaining ground state energy?
Body: <p>In quantum chemistry, coupled-cluster methods, especially CCSD(T), with complete basis set extrapolation are often considered a "gold-standard" for closed-shell molecules. This means that we consider them as accurate as experimental results. While there are some debates about whether if we should consider them gold-standards, that is beside the point.</p>

<p>My question is if there is any similar method when we study periodic (1D-3D) materials? Should we \ can we consider eg quantum MC methods such?</p>

<p>Edit:
As it was pointed out in the comments, different properties require different methods, scale, etc. For simplicity I would like to limit the question to ground state energy. </p>

Best Answer: <p>I think the answer is probably: yes, but not just one. Or no, if you want to be very strict. </p>

<p>Depending on the type of system you are studying, different methods may work better or worse and it may not always be obvious why. There is probably not one method that will work best generically. </p>

<hr>

<p>For quantum spin systems in <span class="math-container">$d\geq 2$</span> (lattice Hamiltonians with interactions of the form <span class="math-container">$\vec S_i \cdot \vec S_j$</span>), the <em>gold standard</em> is quantum Monte Carlo (QMC), with the specific method depending on the specific nature of the material, question. For example, if you wanted to know the transition temperature for the AFM ground state for a 3D Heisenberg model, the gold standard would be Stochastic Series Expansion QMC (SSE) <a href="https://arxiv.org/abs/1909.10591" rel="noreferrer">see arXiv</a>. </p>

<p>For lattice Hamiltonians in 1D, the gold standard is matrix product state methods, like <a href="https://en.wikipedia.org/wiki/Density_matrix_renormalization_group" rel="noreferrer">DMRG</a>. </p>


================================================================================

Question: What are good ways to reduce computing time when working with large systems in VASP?
Body: <p>I know that parallelization is an important factor, however, I wanted to know if there are any other methods of reducing computing time when dealing with large systems without sacrificing accuracy. </p>

<p>Thanks!</p>

Best Answer: <p>There isn't a single correct answer to this question, and it depends on what is slow, what kind of material you are modeling, and what you are trying to calculate. This answer is mostly going to be partially copied from my blog post <a href="https://sites.tufts.edu/andrewrosen/density-functional-theory/vasp/" rel="noreferrer">here</a>.</p>

<p><strong>Geometry Optimizations</strong></p>

<ul>
<li><p>Don’t waste your time using super high-accuracy settings on a structure far from the local minimum. Do an initial optimization with “looser” settings (e.g. gamma-point only for the k-point grid, 400 eV cutoff if relaxing the atomic positions) and then incrementally refine it with the desired (but more computationally expensive) parameters. The one exception to this general comment is that, when performing geometry optimizations that involve changes in the cell shape and/or volume, always ensure that ENCUT > 1.3*ENMAX to prevent Pulay stresses. You don't want to cheapen out on this. But on the whole, you often don't need to use your "production-quality" settings for the full duration of a given structure relaxation.</p></li>
<li><p>When performing a full optimization of atomic positions and cell shape/volume, I generally recommend to do this in stages. It is often wise to start with a relaxation of the atomic positions (ISIF=2) followed by a full volume relaxation (ISIF=3). This will also significantly reduce the chance of running into convergence issues.</p></li>
<li><p>Whenever you can, restart from a converged wavefunction (i.e. the WAVECAR). The WAVECARs are not transferable between the gamma-point only and standard versions of VASP, but otherwise you should try to restart from them when possible to make the SCF converge quicker on a restart.</p></li>
<li><p>The first step in a geometry optimization will generally have the highest number of SCF iterations. It is okay if those first few steps do not converge electronically within the limits of NELM (maximum number of SCF iterations). In fact, it is better to have the first step reach NELM instead of running for many hundreds of SCF iterations.</p></li>
</ul>

<p><strong>Electronic Energy Convergence</strong></p>

<ul>
<li><p>For insulating materials (or when using meta-GGA functionals), SCF convergence is greatly accelerated by using ALGO=All. This has the added benefit that you don’t have to worry about any of the mixing tags. A word of caution though -- some compilations of VASP do not play well with the ALGO=All flag for some reason and stall after a few SCF iterations, so just give it a go and see.</p></li>
<li><p>If dealing with large systems, use LREAL=Auto to speed up the calculations. You may want to switch this back to LREAL=False at the end though to ensure your energies are high-quality.</p></li>
</ul>

<p><strong>Choice of Geometry Optimization Algorithms</strong></p>

<ul>
<li><p>In terms of efficiency and robustness in an optimization algorithm for finding local minima, I generally recommend starting with the conjugate gradient (CG) algorithm (IBRION=2) because it is very robust and you do not have to worry about tweaking POTIM. However, in large, flexible materials with many degrees of freedom, the CG optimization algorithm oftentimes results in a bracketing error once it gets relatively close to the local minimum (search for <code>ZBRENT: fatal error in bracketing</code> in the standard output file). This occurs because the potential energy surface is very flat, and the CG algorithm implemented in VASP is based on the energy differences. One option to fix this is to use FIRE as implemented with <a href="http://theory.cm.utexas.edu/vtsttools/" rel="noreferrer">VTST</a> (IBRION=3, IOPT=7). <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.97.170201" rel="noreferrer">It should get you to that local minimum quicker</a>, particularly when the potential energy surface is quite flat.</p></li>
<li><p>Generally, for nudged elastic band (NEB) calculations and the climbing image variant (CI-NEB), I have found that the the L-BFGS algorithm (IOPT=1) implemented in VTST is often the fastest. For the dimer method, the force-based CG method in VTST (IOPT=2) is recommended. However, if you are having trouble in either case, I suggest switching to the FIRE algorithm (IOPT=7) with the default settings. It is a bit slower, but it is especially useful in troublesome cases of convergence.</p></li>
</ul>

<p><strong>Parallel Performance</strong></p>

<ul>
<li><p>Whenever feasible, take advantage of the gamma-point only VASP executable, as it runs up to 1.5 times faster than the standard executable.</p></li>
<li><p>Try tweaking the NCORE or NPAR flags to adjust the scaling performance. The optimal value for NCORE strongly varies based on the computing environment. I generally use NCORE=cpus-per-node or NCORE=cpus-per-node/2 as a first guess.</p></li>
</ul>


================================================================================

Question: What has caused this apparent stagnancy, in the development of more efficient solar cells?
Body: <p>The NREL solar cell efficiency chart tracks the efficiency of research-level photovoltaics since 1976, and the most up-to-date version is presented below (it was updated in April 2020). </p>

<p>3-J (3-junction) solar cells have seen a steady improvement in efficiency from about 33% in 1999 to 44.4% in 2013, at which point no further research seems to have been done on 3-J solar cells (according to the NREL chart at least). At around the same time, the company Soitec put 4-J cells on the map, the fist one having an efficiency of 44%, followed by a <strong><em>steady</em></strong> increase in efficiency up to 47.1% in 2015. When I say "steady" increase, I mean that the slope we see in the curve for 4-J cells is roughly the same as the slope we see for the 3-J cells from 1999 to 2013, which was <strong><em>excellent</em></strong> progress in increasing solar cell efficiency.</p>

<p><strong>But suddenly it stopped.</strong></p>

<ul>
<li>No new reports of progress with 4-J solar cells have been put on the map since 2015. </li>
<li><em>No 5-J concentrator solar cells have been put on the map <strong>at all.</em></strong></li>
<li>One 6-J concentrator solar cell was reported by NREL in the summer of 2019, which was the first 4+ junction concentrator cell reported in 4 years.</li>
</ul>

<p><strong>What happened?</strong></p>

<p><br></p>

<hr>

<p><a href="https://i.sstatic.net/asBjr.jpg" rel="noreferrer"><img src="https://i.sstatic.net/asBjr.jpg" alt="enter image description here"></a>
<sup>Source: <a href="https://www.nrel.gov/pv/cell-efficiency.html" rel="noreferrer">https://www.nrel.gov/pv/cell-efficiency.html</a></sup></p>

Best Answer: <p>I suspect it's a combination of three factors:</p>

<p>First, there's a fundamental limit of the efficiency of solar cells.  A solar cell is effectively a heat engine that uses the 6000K Sun as the heat source and the Earth's ~300K ambient temperature as the heat sink.  Given that, the efficiency of a "perfect" solar cell tops out at around 95% for an infinite-sized Sun, or 69% for the Sun as it actually appears in the sky.</p>

<p>Second, increasing the junction count produces diminishing returns.  You mention that the slope of four-junction cells follows the trend of three-junction cells -- in contrast, changing from single-junction to two-junction, and two-junction to three-junction cells both produced an immediate jump in efficiency.  Yes, you get more efficiency by adding more junctions, but each additional junction requires more effort for less improvement.</p>

<p>And third, solar panels are a reasonably mature commercialized technology.  Efficiency is "good enough", and research effort is directed more towards making cheaper cells than making efficient cells.  Most users, if they need more power, can just put up a bigger array, they don't need a more-efficient one.</p>


================================================================================

Question: How accurate are the most accurate calculations?
Body: <p>Taking into account the fact that the theory of quantum gravity does not exist and the QED calculations are not possible for most realistic chemical systems, what levels of accuracy can we expect from a theoretical calculation on simple (small) materials? Examples that come to mind are:</p>
<ol>
<li>Simplest molecular materials: <span class="math-container">$^3\ce{He}$</span>, <span class="math-container">$^4\ce{He}$</span>, <span class="math-container">$\ce{H_2}$</span>.</li>
<li>Simplest periodic systems, i.e. metallic <span class="math-container">$\ce{Li}$</span>.</li>
</ol>

Best Answer: <h2>Exhibit 1: Ground state hyperfine splitting of the H atom:</h2>
<pre><code>1420405751767(1) mHz (present most accurate experiment)
142045199        mHz (present most accurate theory)
</code></pre>
<p>The error in the theory is due to the difficulty in treating the nuclear structure (2 up quarks + 1 down).</p>
<h2>Exhibit 2: Ground state hyperfine splitting of the muonium atom:</h2>
<pre><code>4463302780(050) Hz (present most accurate experiment)
4463302880(550) Hz (present most accurate theory)
</code></pre>
<p>Why does it agree so well? μ<span class="math-container">$^+$</span> is a fundamental particle and therefore has no nuclear structure. QED is the correct theory for describing the interaction between pure electric charges (e<span class="math-container">$^-$</span> and μ<span class="math-container">$^+$</span>). The only QFD (quantum flavordynamics) needed is for the electro-weak interaction <em>between</em> the particles (not for interactions within sub-nuclear particles), and QFD calculations were done <a href="https://arxiv.org/abs/1502.03841" rel="nofollow noreferrer">here</a> in anticipation of more accurate experiments to come.</p>
<h2>Exhibit 3: Ground state hyperfine splitting of the He atom:</h2>
<pre><code>6739701177(0016) Hz (present most accurate experiment)
6739699930(1700) Hz (present most accurate theory)
</code></pre>
<p>Notice how much harder it is when you add an electron.</p>
<h2>Exhibit 5: <span class="math-container">$S\rightarrow P$</span> transition in the Li atom:</h2>
<pre><code>14903.632061014(5003) cm^-1 (present most accurate experiment)
14903.631765(0006670) cm^-1 (present most accurate theory)
</code></pre>
<h2>Exhibit 6: Ionization energy of the Li atom:</h2>
<pre><code>43487.15940(18) cm^-1 (present most accurate experiment)
43487.1590(080) cm^-1 (present most accurate theory)
</code></pre>
<h2>Exhibit 7: Ionization energy of the Be atom:</h2>
<pre><code>76192.64(0060) cm^-1 (present most accurate experiment)
76192.699(007) cm^-1 (present most accurate theory)
</code></pre>
<p><strong>Notice that theory is 1 order of magnitude <em>more accurate</em> than experiment!!!</strong></p>
<h2>Exhibit 8: Atomization energy of the H<span class="math-container">$_2$</span> molecule:</h2>
<pre><code>35999.582834(11) cm^-1 (present most accurate experiment)
35999.582820(26) cm^-1 (present most accurate theory)
</code></pre>
<p>See <a href="https://arxiv.org/pdf/1902.09471.pdf" rel="nofollow noreferrer">here for more info</a>.</p>
<h2>Exhibit 9: Fundamental vibration of the H<span class="math-container">$_2$</span> molecule:</h2>
<pre><code>4161.16632(18) cm^-1 (present most accurate experiment)
4161.16612(90) cm^-1 (present most accurate theory)
</code></pre>
<p>See <a href="https://arxiv.org/pdf/1304.6390.pdf" rel="nofollow noreferrer">here for HD and D<span class="math-container">$_2$</span></a>.</p>


================================================================================

Question: How to &quot;get my feet wet&quot; in Density Functional Theory by simulating a water molecule using Python
Body: <p>I just saw <a href="https://materials.stackexchange.com/q/1046/201">I am a beginner in Density Functional Theory. What are some resources that could help me to learn the basics?</a> and it reminded me that few years ago I'd asked <a href="https://chemistry.stackexchange.com/q/30933/16035">How can I calculate the charge distribution of a water molecule?</a> in Chemistry SE.</p>

<p>There is some excellent advice there, but at the time I'd realized I couldn't just install a python package and type "Calculate H2O" so I shelved it at the time. Now there is Materials Modeling SE!</p>

<p>In addition to reading the material recommended in answers to the resources question linked above, I'd like to run an example and getting the electron density of a water molecule still captures my interest.</p>

<p>What would be a simple approach be, and by simple I mean a python package that can be used with an Anaconda Python 3 installation and run without a large number of additional dependencies that might be complicated to install?</p>

<p>The process should be instructive but the results do not have to be particularly accurate; I've chosen the water molecule to get my feet wet and get a better feeling for how these calculates are actually done procedurally.</p>

<p>If there is an OS aspect to this I'd prefer to do this on MacOS but I have access to Windows machines. I know that these can be computationally intensive once one gets going and so I may need to find more hardware resources later on.</p>

Best Answer: <h1>SlowQuant</h1>

<p>In your question from 2015, you mentioned PyQuante and PySCF, but I saw no mention of <a href="https://slowquant.readthedocs.io/en/latest/" rel="noreferrer">SlowQuant</a> in the question or in the multiple answers. I see that while writing this, someone else answered, with mentions of PyQuante, PySCF, and Psi4 again. Still no mention of SlowQuant.</p>

<p>The author of SlowQuant has described the program this way (emphasis mine):</p>

<blockquote>
  <p>SlowQuant is a molecular quantum chemistry program written in python.
  <strong>Even the computational demanding parts are written in python</strong>, so it
  lacks speed, thus the name SlowQuant. </p>
</blockquote>

<p>Right below that he gives the command for running it on  the water molecule (click to get input files):<br> </p>

<blockquote>
  <p>python SlowQuant.py <a href="https://slowquant.readthedocs.io/en/latest/Examples.html#example-input-files" rel="noreferrer">H2O.csv</a>
  <a href="https://slowquant.readthedocs.io/en/latest/illustrativecalc.html#scf-with-and-without-diis" rel="noreferrer">setting.csv</a></p>
</blockquote>

<p><strong>Dependencies:</strong><br>
 - Python 3.5 or above<br>
 - numpy 1.13.1<br>
 - scipy 0.19.1<br>
 - numba 0.34.0<br>
 - cython 0.25.2<br>
 - gcc 5.4.0<br></p>

<p>As you can see, even SlowQuant requires the GCC compilers, presumably because it uses Cython which is for turning Python code into faster running C code (<strong>maybe this program is not so slow after all?</strong>).</p>


================================================================================

Question: Given that Kohn-Sham DFT is strictly a ground-state method (at 0 K), how is it sufficient to describe materials in real-life applications?
Body: <p>Kohn-Sham DFT appears to be so popular even though it is strictly a ground-state method - all calculations are done at 0 K. How then, is it so popular when describing materials that have real-life applications (possibly at room temperatures, or much higher temperatures)? Or am I mistakenly deeming KS-DFT to be too popular when there are other more suitable methods like TD-DFT, GW-BSE etc?</p>

Best Answer: <p>These are a few extra points to complement Andrew Rosen's comprehensive response:</p>

<ol>
<li><p>To be absolutely clear, typical DFT calculations are <em>not</em> performed at 0K, a better description of what happens is that they are performed "for a static crystal". Static crystal means that the atoms are fixed at their crystallographic positions (which is what a typical DFT calculation does), but this situation is different from 0K. Even at 0K, the atomic nuclei move due to quantum zero-point motion (also called quantum fluctuations). This quantum zero-point motion is very small for compounds containing heavy elements, which means that in these cases "0K" and "static crystal" are almost the same and many people use the terms interchangeably. However, quantum zero-point motion can be very large for light elements, for example in solid hydrogen (the lightest of all compounds) it dominates even at room temperature. To perform a calculation at 0K (as opposed to a static crystal calculation), the strategy that Andrew Rosen proposed would still work if you use quantum statistics. For example, solving the vibrational problem within the harmonic approximation at the quantum mechanical level with give you the zero-point energy as the ground state of a quantum harmonic oscillator. A good reference for hydrogen is this <a href="https://link.aps.org/doi/10.1103/RevModPhys.84.1607" rel="noreferrer">review article</a>.</p></li>
<li><p>Although Andrew Rosen is correct in that the potential energy surface is largely temperature independent, there are a few situations in which the potential energy surface does vary significantly with temperature. An example of this is provided by many perovskite crystals (e.g. <span class="math-container">$\ce{BaTiO_3}$</span>), which undergo a series of temperature-induced structural phase transitions. For example, at very high temperatures the structure is cubic, and this structure sits at a minimum of the potential <em>free</em> energy surface. However, if you calculated the potential energy surface (instead of the free energy surface), then it has a double-well shape and the cubic structure is at the saddle point, so the structure will lower its energy by distorting and going to one of the minima. This is precisely what happens at low temperatures because the corresponding free energy surface acquires a double-well shape, and the system undergoes a structural phase transition. A good early DFT reference for this is this <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.52.6301" rel="noreferrer">paper</a>.</p></li>
<li><p>As already pointed out by others, the differences between DFT and TDDFT or <span class="math-container">$GW$</span>-BSE are not related to the inclusion or exclusion of temperature. In this language, both TDDFT and <span class="math-container">$GW$</span>-BSE have no temperature for the behaviour of the nuclei, and you could incoporate it in a similar fashion to what Andrew Rosen described for DFT.</p></li>
</ol>


================================================================================

Question: What are the main computational frameworks used in materials modeling?
Body: <p>What are the main computational frameworks used in materials modeling? Software packages can include those designed to run on both classical and quantum devices.</p>

Best Answer: <h1>Electronic Structure Theory</h1>
<hr>
<p>Much of the behavior we observe from molecules/materials arises from electronic interactions. These interactions are fundamentally quantum mechanical as are most of the approaches used to model them.</p>
<p>To study electronic properties of a system, we typically solve some approximation of the electronic time in/dependent Schrodinger equation: <span class="math-container">\begin{equation}E\Psi=H\Psi\tag{1}\end{equation}</span>
<span class="math-container">\begin{equation}i\hbar\frac{d\Psi}{dt}=H\Psi\tag{2}\end{equation}</span>
The wavefunction and energy allow a whole host of other properties to be determined, including charge transfer rates and various polarizabilities (along with their associated spectroscopic signals).</p>
<p>The difficulty of solving the Schrodinger equation exactly has led to the development of a number of approximate schemes. Two commonly encountered types of approximations are wavefunction based methods, which build on top of the simple <a href="https://en.wikipedia.org/wiki/Hartree%E2%80%93Fock_method" rel="noreferrer">Hartree-Fock</a>, and <a href="https://en.wikipedia.org/wiki/Density_functional_theory" rel="noreferrer">Density Functional Theory</a>(DFT), which reframes the problem of solving for the system wavefunction that satisfies the Schrodinger equation to instead solving for the electron density that minimizes a particular energy functional.</p>
<p>These approximations vary in computational complexity, which has led to varied use depending on the field. In molecular sciences, approaches such as <a href="https://en.wikipedia.org/wiki/Multi-configurational_self-consistent_field" rel="noreferrer">MCSCF</a> and <a href="https://en.wikipedia.org/wiki/Coupled_cluster" rel="noreferrer">Coupled Cluster</a> are widely used due to their accuracy and clear direction for systematic improvement. For larger materials however, these methods are generally precluded by their high cost and so more economical approaches like DFT are far more common.</p>
<p>There are number of software packages that have been developed to perform these calculations, each with a different emphasis (e.g. performance, number of features, ease of use/development, molecules vs materials, free vs proprietary). On the molecular side, <a href="http://gaussian.com/" rel="noreferrer">Gaussian</a>, <a href="http://www.q-chem.com/" rel="noreferrer">Q-Chem</a>, <a href="http://www.nwchem-sw.org/index.php/Main_Page" rel="noreferrer">NWChem</a>, <a href="https://www.msg.chem.iastate.edu/gamess/" rel="noreferrer">GAMESS</a>, and <a href="http://www.psicode.org/" rel="noreferrer">Psi4</a> are notable examples. For materials, <a href="https://www.vasp.at/" rel="noreferrer">VASP</a>, <a href="http://www.quantum-espresso.org/" rel="noreferrer">Quantum ESPRESSO</a>, <a href="https://departments.icmab.es/leem/siesta/" rel="noreferrer">SIESTA</a>
, and <a href="https://www.cp2k.org/" rel="noreferrer">CP2K</a> are more commonly used (among many other <a href="https://en.wikipedia.org/wiki/List_of_quantum_chemistry_and_solid-state_physics_software" rel="noreferrer">electronic structure packages</a>).</p>


================================================================================

Question: How to determine, a priori, whether a compound has multireference character?
Body: <p>We say a system has <em>multireference character</em> if it is not well described at the SCF level by a single configuration/Slater determinant. Not accounting for this can lead to errors in post-SCF methods, which often rely on expansions of additional configurations generated from the SCF solution. It is fairly easy that this sort of error has occurred once a post-SCF solution has been constructed, but are there ways to predict this without performing a high level calculation?</p>

<p>I would be interested in both heuristic methods (<em>e.g.</em> compounds containing atom X or structure Y tend to have issues) as well as more quantitative approaches (<em>e.g.</em> X property from a DFT calculation is a good diagnostic for multireference character).</p>

Best Answer: <p>This is not going to be a full answer, but maybe a starting point, a pointer to somewhat troubled systems. The general keyword here is <strong>degeneracy</strong>. Whenever you have fewer electrons than fit in the degenerate orbitals, two things may happen: In the simple case the molecule will distort, see aromaticity and anti-aromaticity. In the complicated case, you guessed it, you'll have a multireference system. A popular example for this is the rotational barrier of ethene, where you have degenerate, but orthogonal orbitals at the 90° dihedral. Two electrons, two orbitals, forced singlet will lead to no solution. If you treat it with TCSCF (two-configuration SCF) you'll get a qualitative solution, similar for CAS[2,2], and even UHF.</p>

<p>So the short and very unsatisfactory answer is, whenever you see a high symmetry, you should be careful.</p>

<p>Another obvious point is: anything with transition metals. Partially unoccupied d-orbitals will almost always need multi-reference treatment. In other words: <a href="https://web.archive.org/web/20150909233705/http://esqc.ups-tlse.fr/11/lectures/ESQC-Neese-1-1x2.pdf" rel="noreferrer">Transition-metal chemistry is a graveyard for MP2 (large pdf via the Internet Archive))</a>, dito in <a href="https://books.google.de/books?id=qm5cCwAAQBAJ&amp;pg=PT220&amp;lpg=PT220&amp;dq=Transition-metal+chemistry+is+a+graveyard+for+MP2" rel="noreferrer">Koch/Holthausen's DFT book</a>.</p>

<h3>Some practical approaches:</h3>

<ol>
<li><p>Calculate the fractional occupation number weighted electron density (FOD). This is fast and can be done at the semi-empirical level.<sup>[1]</sup> I don't think there is a faster (practical) way of doing this. (<em>I cannot believe I forgot about this before.</em>)</p></li>
<li><p>A more conventional approach: UHF for an initial guess will give you approximate natural orbitals. You can use this to determine an active space, if you are so inclined.<sup>[2]</sup> </p></li>
<li><p>UDFA and stability checks. Most Density Functional Methods are surprisingly robust about multi-reference character. If you run a calculation, then perform a stability analysis, which runs into a broken symmetry solution, you'll need to do multi-reference wave function methods.<br>
In general, performing a cheap method, will likely give you a HOMO-LUMO gap. When even your program of choice warns you about it being small, you might want to check for MR character.<sup>[3]</sup></p></li>
</ol>

<p>Obviously, your mileage may vary, and you need to check what is feasible for your system.</p>

<hr>

<ol>
<li>Resources:

<ul>
<li>Original Paper: Grimme, S.; Hansen, A. A Practicable Real-Space Measure and Visualization of Static Electron-Correlation Effects. <em>Angew. Chem. Int. Ed.</em> <strong>2015,</strong> <em>54</em> (42), 12308–12313. <a href="https://doi.org/10.1002/anie.201501887" rel="noreferrer">DOI: 10.1002/anie.201501887</a>.</li>
<li>Please see on Chemistry.se: <a href="https://chemistry.stackexchange.com/q/89470/4945">Is there a reliable way to determine if a chemical system is more suited to be calculated using a multireference method instead of DFT?</a></li>
<li>Orca input library on FOD: <a href="https://sites.google.com/site/orcainputlibrary/orbital-and-density-analysis/fod-analysis" rel="noreferrer">FOD analysis</a></li>
<li>xtb manual: <a href="https://xtb-docs.readthedocs.io/en/latest/properties.html#fractional-occupation-density-fod-calculation" rel="noreferrer">Fractional Occupation Density Calculation</a>, also see the repository on <a href="https://github.com/grimme-lab/xtb" rel="noreferrer">GitHub</a></li>
</ul></li>
<li>See Chemistry.se for some more details: <a href="https://chemistry.stackexchange.com/q/69929/4945">Is it reasonable to use natural orbitals of an unrestricted HF calculation as a basis to start a CASSCF calculation for a radical</a>  </li>
<li>Some related questions on Chemistry.se (I know this gets old, but that's where the resources are.): 

<ul>
<li><a href="https://chemistry.stackexchange.com/q/49174/4945">U- or RO-method for Singlet-Triplet Gap?</a></li>
<li><a href="https://chemistry.stackexchange.com/q/55736/4945">Calculating singlet-triplet gap and open-shell singlet character</a></li>
<li>this is also a bit related: <a href="https://chemistry.stackexchange.com/q/34215/4945">Similarities and Differences between Resonance and MCSCF treatments</a><a href="https://doi.org/10.1002/anie.201501887" rel="noreferrer"><em>Angew. Chem. Int. Ed.</em> <strong>2015,</strong> <em>54</em> (42), 12308–12313</a></li>
</ul></li>
</ol>


================================================================================

Question: Where is the extended H&#252;ckel method (EHM) still used today?
Body: <p>The <a href="https://en.wikipedia.org/wiki/Extended_H%C3%BCckel_method" rel="noreferrer">extended Hückel method</a> (EHM) proved to be very useful through time, but there are better and affordable models today. One interesting thing about the model is the independence of the Hamiltonian with respect to the (molecular) orbital coefficients, which allows solutions to be calculated in a single diagonalization:</p>

<p><span class="math-container">$$H_{ij} = K S_{ij} \frac{H_{ii} + H_{jj}}{2},$$</span></p>

<p>with <span class="math-container">$S_{ij}$</span> being the calculated <a href="https://en.wikipedia.org/wiki/Orbital_overlap" rel="noreferrer">overlap matrix</a> and the diagonal elements <span class="math-container">$H_{ii}$</span> taken as model parameters.</p>

<p>The question is <strong>which fields still use the extended Hückel model today?</strong> Why and how? Does the EHM still fill a gap?</p>

<p>I am specifically looking for fields where it is the main method and how the shortcomings of EHM have been addressed.</p>

Best Answer: <p>In an era of <em>ab initio</em> methods and many-body methods like the <span class="math-container">$GW$</span>, there is not too much room for methods like the extended Hückel model to be the <em>main</em> method in any particular field of materials modeling. However, the method is still very much appreciated by the solid-state community particularly for it's accessibility. It is especially popular with those fond of Tight-Binding variants. </p>

<p>The extended Hückel model is an effective Hamiltonian that is used to approximate the Schrödinger eq. by making use of the variational theorem and the linear combination of atomic orbitals (<strong>LCAO</strong>). The key is to solve a set of linear Hückel equations of form 
<span class="math-container">\begin{align}
  \sum_{i,j}\left({H}_{ij} - \epsilon_\alpha S_{ij}\right) C_{ij} &amp;= 0, &amp;
  \text{with } i,j &amp;= 1, 2, 3, \dots \tag{1}\label{eq:huckel}
\end{align}</span></p>

<p>The diagonal elements of <span class="math-container">$\hat{H}$</span> are taken to be equal to the ionization energy of an electron in the <span class="math-container">$i$</span>th valence <span class="math-container">$\phi$</span> of the isolated atom in the appropriate state, i.e. Valence State Ionization Potential (VSIP), expressed as <span class="math-container">$H_{ii}  = - VSIP(\phi_{j})=\epsilon_\mathrm{onsite}$</span>.</p>

<p>The off-diagonal elements of <span class="math-container">$\hat{H}$</span> are evaluated according to a modified Wolfsberg-Helmholtz relation, as mentioned by Felipe:</p>

<p><span class="math-container">$$
H_{ij} = \mathcal{K} S_{ij} \left(\frac{H_{ii} + H_{jj}}{2}\right),
\tag{2}\label{eq:Hij}
$$</span>
where <span class="math-container">$S_{ij}$</span> is the matrix of overlap integrals, <span class="math-container">$S_{ij} = \langle \phi_{i}\vert \phi_{j} \rangle$</span>.</p>

<h2>So... who is using it and how?</h2>

<p>As mentioned by Felipe, the eH-TB method is fairly easy to work with and Tyberius highlighted that it's speed is primarily what gives it value. Hence, many researchers often write in-house codes for their research projects,<sup>[1]</sup> or build on community codes. Actually, my first formal encounter with materials modeling was learning the extended Hückel Tight-Binding method!<sup>[2]</sup> I carried out a project under Prof. Donald H. Galván, who worked directly with Roald Hoffmann a couple years ago. When I began the project, I had no experience or knowledge of Quantum Mechanics or Solid State Physics, and worked with a modified version of the <a href="http://yaehmop.sourceforge.net/" rel="noreferrer">YAeHMOP</a> code written by Greg Landrum, but that experience prepared me to really make sense of band theory and materials form an orbital perspective.</p>

<p>The method is a very good starting point for students interested in electronic structure theory, whether it be modeling of development. Therefore, it is a useful tool in the classroom. Recently, YaEHMOP was merged with the Avogadro molecular editor and visualizer to serve as a simple way for undergraduate quantum theory students to model band structures, Density of States (DOS) and Crystal Orbital Hamilton Population (COHP) using their personal computer. The authors published this in <em>"Journal of Chemical Education"</em>.<sup>[3]</sup></p>

<p>It is also implemented in the <a href="https://www.synopsys.com/silicon/quantumatk/atomic-scale-modeling/products.html" rel="noreferrer">Quantum ATK</a> platform for both academic research and materials development. In research, the method is often used to model the ground state and transport properties of systems of sized prohibitive by other methods (e.g. Carbon Nanotubes),<sup>[4]</sup> as a first-approximations to study a system's properties or model a material with an effective <span class="math-container">$\hat{H}$</span> to study physics that may not be accessible by traditional DFT. An example of this, is the work of A.S. Martins on the Hyper-Honeycomb lattice in <em>J.Phys. Chem. C.</em><sup>[5]</sup> and on 2D materials with defects.<sup>[6]</sup> The method is still trusted enough to model the electronic structure in support of experimentally achieved materials.<sup>[7]</sup></p>

<p>Recently, the Extended Hückel Hamiltonian has also found a home in quantum dynamics and charge transfer simulations.<sup>[8, 9, 10, 11]</sup></p>

<p>It is still inspiring new TB models,<sup>[12]</sup> and the derivation of paper-and-pencil LCAO models help to understand complex materials. Last year, a LCAO model was proposed for double halide perovskites.<sup>[13]</sup> Additionally, due to its qualitative predictive power and speed, there are efforts to improve through Machine Learning.<sup>[14]</sup> Lastly, the eH is key ingredient in the novel GFNx-TB method.<sup>[15]</sup>  </p>

<p>As seen, the extended Hückel method continues to play a strong role in quantum chemistry and materials modeling, though not in the same way it did in the twentieth century. Currently, I am not sure if it is the "standard" method of anything, but the model itself can teach us a lot about band theory at a qualitative level and that is useful for many things. If I were to give it three solid applications today, they would be:</p>

<ul>
<li>Education in band theory </li>
<li>Method development in Quantum Chemistry</li>
<li>Transport simulations</li>
</ul>

<h3>References:</h3>

<ol>
<li><p>El Khatib, M.; Evangelisti, S.; Leininger, T.; Bendazzoli, G. L. A theoretical study of closed polyacene structures. <em>Phys. Chem. Chem. Phys.</em> <strong>2012,</strong> <em>14</em> (45), 15666. <a href="https://doi.org/10.1039/C2CP42144E" rel="noreferrer">DOI: 10.1039/C2CP42144E</a>.</p></li>
<li><p>Palos, E. I.; Paez, J. I.; Reyes-Serrato, A.; Galván, D. H. Electronic structure calculations for rhenium carbonitride: an extended Hückel tight-binding study. <em>Phys. Scr.</em> <strong>2018,</strong> <em>93</em> (11), 115801. <a href="https://doi.org/10.1088/1402-4896/aae14c" rel="noreferrer">DOI: 10.1088/1402-4896/aae14c</a>.</p></li>
<li><p>Avery, P.; Ludowieg, H.; Autschbach, J.; Zurek, E. Extended Hückel Calculations on Solids Using the Avogadro Molecular Editor and Visualizer. <em>J. Chem. Educ.</em> <strong>2018,</strong> <em>95</em> (2), 331–337. <a href="https://doi.org/10.1021/acs.jchemed.7b00698" rel="noreferrer">DOI: 10.1021/acs.jchemed.7b00698</a>.</p></li>
<li><p>Zienert, A.; Schuster, J.; Gessner, T. Extended Hückel Theory for Carbon Nanotubes: Band Structure and Transport Properties. <em>J. Phys. Chem. A</em> <strong>2013,</strong> <em>117</em> (17), 3650–3654. <a href="https://doi.org/10.1021/jp312586j" rel="noreferrer">DOI: 10.1021/jp312586j</a>.</p></li>
<li><p>Veríssimo-Alves, M.; Amorim, R. G.; Martins, A. S. Anisotropic Electronic Structure and Transport Properties of the H-0 Hyperhoneycomb Lattice. <em>J. Phys. Chem. C</em> <strong>2017,</strong> <em>121</em> (3), 1928–1933. <a href="https://doi.org/10.1021/acs.jpcc.6b10336" rel="noreferrer">DOI: 10.1021/acs.jpcc.6b10336</a>.</p></li>
<li><p>Martins, A. d. S.; Veríssimo-Alves, M. Group-IV nanosheets with vacancies: a tight-binding extended Hückel study. <em>J. Phys.: Condens. Matter</em> <strong>2014,</strong> <em>26</em> (36), 365501. <a href="https://doi.org/10.1088/0953-8984/26/36/365501" rel="noreferrer">DOI: 10.1088/0953-8984/26/36/365501</a>.</p></li>
<li><p>Zhak, O.; Zdorov, T.; Levytskyy, V.; Babizhetskyy, V.; Zheng, C.; Isnard, O. Ternary antimonides Ln2Pd9Sb3 (Ln = La, Ce, Nd, Pr, and Sm): Crystal, electronic structure, and magnetic properties. <em>J. Alloys Compd.</em> <strong>2020,</strong> <em>815,</em> 152428. <a href="https://doi.org/10.1016/j.jallcom.2019.152428" rel="noreferrer">DOI: 10.1016/j.jallcom.2019.152428</a>.</p></li>
<li><p>Tsuji, Y.; Estrada, E. Influence of long-range interactions on quantum interference in molecular conduction. A tight-binding (Hückel) approach. <em>J. Chem. Phys.</em> <strong>2019,</strong> <em>150</em> (20), 204123. <a href="https://doi.org/10.1063/1.5097330" rel="noreferrer">DOI: 10.1063/1.5097330</a>.</p></li>
<li><p>Sato, K.; Pradhan, E.; Asahi, R.; Akimov, A. V. Charge transfer dynamics at the boron subphthalocyanine chloride/C60 interface: non-adiabatic dynamics study with Libra-X. <em>Phys. Chem. Chem. Phys.</em> <strong>2018,</strong> <em>20</em> (39), 25275–25294. <a href="https://doi.org/10.1039/C8CP03841D" rel="noreferrer">DOI: 10.1039/C8CP03841D</a>.</p></li>
<li><p>Li, W.; Ren, W.; Chen, Z.; Lu, T.; Deng, L.; Tang, J.; Zhang, X.; Wang, L.; Bai, F. Theoretical design of porphyrin dyes with electron-deficit heterocycles towards near-IR light sensitization in dye-sensitized solar cells. <em>Sol. Energy</em> <strong>2019,</strong> <em>188,</em> 742–749. <a href="https://doi.org/10.1016/j.solener.2019.06.062" rel="noreferrer">DOI: 10.1016/j.solener.2019.06.062</a>.</p></li>
<li><p>Vohra, R.; Sawhney, R. S.; Singh, K. P. Contemplating charge transport by modeling of DNA nucleobases based nano structures. <em>Curr. Appl Phys.</em> <strong>2020,</strong> <em>20</em> (5), 653–659. <a href="https://doi.org/10.1016/j.cap.2020.02.016" rel="noreferrer">DOI: 10.1016/j.cap.2020.02.016</a>.</p></li>
<li><p>Fujiwara, T.; Nishino, S.; Yamamoto, S.; Suzuki, T.; Ikeda, M.; Ohtani, Y. Total-energy Assisted Tight-binding Method Based on Local Density Approximation of Density Functional Theory. <em>J. Phys. Soc. Jpn.</em> <strong>2018,</strong> <em>87</em> (6), 064802. <a href="https://doi.org/10.7566/JPSJ.87.064802" rel="noreferrer">DOI: 10.7566/JPSJ.87.064802</a>.</p></li>
<li><p>Slavney, A. H.; Connor, B. A.; Leppert, L.; Karunadasa, H. I. A pencil-and-paper method for elucidating halide double perovskite band structures. <em>Chem. Sci.</em> <strong>2019,</strong> <em>10</em> (48), 11041–11053. <a href="https://doi.org/10.1039/C9SC03219C" rel="noreferrer">DOI: 10.1039/C9SC03219C</a>.</p></li>
<li><p>Tetiana Zubatyuk, Ben Nebgen, Nicholas Lubbers, Justin S. Smith, Roman Zubatyuk, Guoqing Zhou, Christopher Koh, Kipton Barros, Olexandr Isayev, Sergei Tretiak. Machine Learned Hückel Theory: Interfacing Physics and Deep Neural Networks. arXiv:<a href="https://arxiv.org/abs/1909.12963" rel="noreferrer">1909.12963</a> <strong>[cond-mat.dis-nn]</strong> </p></li>
<li><p>Bannwarth, C.; Ehlert, S.; Grimme, S. GFN2-xTB—An Accurate and Broadly Parametrized Self-Consistent Tight-Binding Quantum Chemical Method with Multipole Electrostatics and Density-Dependent Dispersion Contributions. <em>J. Chem. Theory Comput.</em> <strong>2019,</strong> <em>15</em> (3), 1652–1671. <a href="https://doi.org/10.1021/acs.jctc.8b01176" rel="noreferrer">DOI: 10.1021/acs.jctc.8b01176</a>.</p></li>
</ol>


================================================================================

Question: What are the best functionals for transition metal compounds?
Body: <p>Some properties of transition metal compounds can be influenced by static correlation, relativistic effects, and, in some cases, close-lying electronic states could result in difficulties in SCF convergence. All these complications influence geometry optimizations and, hence, the accuracy of the obtained results.</p>

<p>Considering a usual Kohn-Sham DFT calculation of a TM compound with organic ligands:</p>

<ol>
<li>What functionals gives accurate equilibrium geometries, compared to high-level calculations or experimental ones?</li>
<li>What functionals are accurate for electronic and bonding properties?</li>
</ol>

<p>I'm not asking for a specific functional, but some guidelines on which are better for those purposes.</p>

Best Answer: <p>The general question on the accuracy of density functionals for transition metal compounds is highly ambigious due to the effect of static correlation, for which there is no solution at the moment.</p>

<p>As to the simpler question on well-behaving TM compounds with organic ligands, <a href="https://pubs.acs.org/doi/10.1021/acs.jpca.9b01546" rel="noreferrer">a benchmark study on transition metal reaction barrier heights</a> found that Head-Gordon's recent functionals are excellent choices, with revTPSS0-D4 also faring well.</p>


================================================================================

Question: Available methods and codes for materials discovery / crystal structure prediction?
Body: <p>There are many open databases and projects that allow you to access computed crystal structures of experimentally known compounds, registered in the Inorganic Crystal Structure Database (ICSD). These structures are usually then studied using a given materials modeling method, e.g. DFT.</p>

<p>Are there any methods and codes available to predict completely new materials, their crystal structures and properties with no experimental references? </p>

Best Answer: <p>Yes, there are! One early example that's still in use today is <a href="https://uspex-team.org/en" rel="noreferrer">the Universal Structure Predictor: Evolutionary Xtallography (USPEX) method</a>. You can find many "success cases" on their website if you're curious. The <a href="https://journals.aps.org/prmaterials/abstract/10.1103/PhysRevMaterials.1.063802" rel="noreferrer">First-Principles-Assisted Structure Solution (FPASS)</a> and <a href="https://journals.aps.org/prmaterials/abstract/10.1103/PhysRevMaterials.1.063802" rel="noreferrer">Prototype Electrostatic Ground States (PEGS)</a> methods are a couple of other codes used for structure prediction. Then there are many that are specific to a given application area. For instance, to predict the structures of metal-organic frameworks from specified molecular building blocks, there is the <a href="https://github.com/tobacco-mofs/tobacco_3.0" rel="noreferrer">Topologically Based Crystal Constructor (ToBaCCo)</a>.</p>

<p>Each code out there works differently. Some try to use brute-force approaches while accounting for material-specific design principles. The lowest energy structure is then often take. You could imagine taking such structures from the <a href="http://oqmd.org/" rel="noreferrer">Open Quantum Materials Database</a>, for instance. Other methods use evolutionary algorithms, as summarized in <a href="https://pubs.acs.org/doi/abs/10.1021/ar1001318" rel="noreferrer">this review</a>. More recently, these crystal structure prediction algorithms are taking advantage of machine learning and artificial intelligence for the design and prediction of crystals with tailored properties for a given application of interest, such as <a href="https://chemrxiv.org/articles/Inverse_Design_of_Nanoporous_Crystalline_Reticular_Materials_with_Deep_Generative_Models/12186681" rel="noreferrer">a material</a> that can separate <span class="math-container">$\mathrm{CO}_{2}$</span> from natural gas.</p>

<p>There are several reviews on the topic you may be interested in: <a href="https://pubs.rsc.org/en/content/ebook/978-1-78262-961-0" rel="noreferrer">1</a>, <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9783527632831" rel="noreferrer">2</a>, <a href="https://link.springer.com/book/10.1007%2F978-3-319-05774-3" rel="noreferrer">3</a>.</p>


================================================================================

Question: Which Linux distribution is best for Matter Modeling?
Body: <p>In some science fields there are efforts to provide customized distributions, so you can get a productive environment with minimum set-up overhead. For example, in high energy particle physics, we have <a href="https://www.scientificlinux.org/" rel="noreferrer">Scientific Linux</a>, in mathematics we have <a href="http://www.mathlibre.org/" rel="noreferrer">MathLibre</a>, and in cybersecurity research we have <a href="https://www.kali.org/" rel="noreferrer">Kali Linux</a>. </p>

<p><strong>Is there any effort like these for materials modeling?</strong> If not, which Linux distributions come the closest?</p>

<p><em>Please list only one Linux distribution per answer!</em></p>

Best Answer: <h2>Fedora</h2>
<p><a href="https://materials.stackexchange.com/a/1244/5">Susi Lehtola's answer</a> makes some good points, and I will elaborate on two of them:</p>
<ul>
<li>The Linux distributions mentioned in your question, which cater to specific communities (e.g. Scientific Linux for particle physics, MathLibre for math, Kali Linux for cybersecurity) are descendants of much more widely-known distributions, so they are not <em>acutely</em> different from what a lot of people are already familiar with using. Susi mentioned that Scientific Linux is a descendant of Red Hat, and I'll add to that by saying that both MathLibre and Kali are direct descendants of KNOPPIX, which <a href="https://upload.wikimedia.org/wikipedia/commons/1/1b/Linux_Distribution_Timeline.svg" rel="noreferrer">descended from Debian</a> between 2001-2002, a few years before Ubuntu descended from Debian. <strong>So of the three distributions you mentioned: 2 are very closely related to Debian/Ubuntu, and 1 is very closely related to RedHat/CentOS/Fedora</strong>. This would likely also be the case with anything else catering specifically to matter modeling.</li>
<li>&quot;If the distribution has a contributor who needs package X for their interests, it's highly likely that the distribution's package X is up to date and includes all features.&quot; In light of this, since <a href="https://materials.stackexchange.com/questions/1103/since-mkl-is-not-optimized-for-amd-hardware-should-i-use-a-math-library-specifi#comment1231_1111">Susi has been a packager for Fedora for over a decade</a> and packaged OpenMOLCAS, PyQuante, Psi4, PySCF, linint, libxc, and whatever else (he can edit this if there's more!) for Fedora, then Matter Modelers might find Fedora/RedHat/CentOS to be convenient since so many of our workhorses have already been packaged for Fedora.</li>
</ul>
<p>To focus this answer on just one Linux distribution, out of the Fedora/RedHat/CentOS family, <strong>I picked Fedora because</strong>:</p>
<ul>
<li><strong>It's free.</strong>  (Red Hat Enterprise Linux comes with official support, but costs money).</li>
<li>It caters to desktop users. (CentOS is also free, but it caters more to servers, so it misses a lot of the packages for desktop use, so the Fedora user community is more likely to grow to a much larger size; Fedora also caters a bit to servers and cloud computing, so it's versatile in that way).</li>
<li>It's updated frequently. (CentOS and RHEL are preferred for servers because they are very stable and well-tested, <a href="https://www.reddit.com/r/Fedora/comments/9dwt9i/fedora_vs_centos/e5ks0vy/" rel="noreferrer">but this is accomplished by not updating the all the latest packages</a>).</li>
<li>The people I've met that use Fedora, are extremely enthusiastic about it (so the even if the number of users is not the biggest, the users love Fedora, will continue to promote and develop Fedora, and won't leave Fedora any time soon), and also very welcoming for others to joint the <a href="https://fedoraproject.org/wiki/Join?fbclid=IwAR0dlLVrJ34i8IaQ6CKoJKm9HHsk_y8NuYq90UKKgtQK_9QqhmS1tj1IcK4#OS_Developer" rel="noreferrer">community</a>.</li>
<li>There's about <a href="https://en.wikipedia.org/wiki/Fedora_(operating_system)" rel="noreferrer">1.5 million users, including the inventor of Linux: Linus Torvalds</a>.</li>
</ul>
<p><strong>Drawbacks:</strong></p>
<ul>
<li>I mentioned Fedora because your comments suggested to me that you were looking for something for your personal computer. Most matter modeling calculations are done on servers though, and that's when CentOS is used more commonly, largely because it's more stable and tested than Fedora, and the latest fancy packages for which you would want Fedora on your desktop, are not usually needed on a server.</li>
<li>As a desktop OS, Fedora still has a much smaller user community than Ubuntu which has an estimated 20 million users. Having too large a user community also leads to some disadvantages to Ubuntu, but <a href="https://askubuntu.com/">they have an entire StackExchange site of their own</a>, so when troubleshooting you might get an answer faster, and also I personally have found it helpful when giving talks at other universities (using my own laptop) that people at the host university that needed to help me with something, or wanted to set me up to use their printer, were familiar with my Ubuntu OS.</li>
</ul>


================================================================================

Question: Is it right to neglect very small imaginary frequencies?
Body: <p>I'm running a DFT optimization (B3LYP/def2-TZVP) and frequencies in ORCA for a molecule. And I get one or two very small (1-6 cm^-1) imaginary frequencies which corresponds to a slight bend of the molecule.</p>
<p>I assume that the frequencies arise from numerical integration errors as they decrease when I run the same calculation with better grid and tighter SCF.</p>
<p>So is it appropriate to just leave it as is if I plan to run electronic properties calculation on this geometry with more high-level functional and basis (polarizability, hyperpolarizability, and maybe TD-DFT)?</p>

Best Answer: <h2>tldr: This is something of an eternal debate.</h2>
<p>IMHO very small imaginary frequencies can be okay, but it depends on your system and needs.</p>
<p>As you might see from the various comments above, there are often different opinions on whether very small imaginary frequencies matter. The truth is, that it depends a bit on the size of the molecule and what you plan to do.</p>
<p>In <strong>principle</strong>, if you've reached a true local / global minima of the potential energy surface, there should <em><strong>never</strong></em> be imaginary frequencies, because if the second derivative is negative, you're not at a minimum with respect to that normal mode.</p>
<p>In <strong>practice</strong>, for medium to large molecules it can be very hard to completely minimize and find a true minimum. Most geometry optimization methods include various routines to only stop when the forces are very small, the change in energy is very small, etc. But these don't guarantee a true minimum, which is why you should calculate frequencies and check (which you mention in the question).</p>
<p>Remember that the potential energy surface has <span class="math-container">$3N-6$</span> dimensions for non-linear molecules. For 10 atoms, that's already 24 degrees of freedom. In many molecules, these may be correlated (e.g., think about twisting a dihedral angle in a protein - you might smash into another atom). Many times the potential energy surface can be close to flat, so finding the exact minima is time-consuming.</p>
<ol>
<li>You should check for numerical noise, try to push for better optimization tolerances, integration grids, convergence, etc.</li>
</ol>
<p>(It seems like you've done a lot of this based on your comments.)</p>
<ol start="2">
<li>It depends on your properties / needs. If you need rigorous thermochemistry, then there may be some energy error  between your current geometry and a true minima. In that case, do what you can to remove the minima.</li>
</ol>
<p>In your case, in my experience, the property calculations you plan are relatively insensitive to the small energy / geometry difference. Consider if the atomic positions move <span class="math-container">$0.001Å$</span> will the polarizability change much? Probably not. Similar story with TDDFT, which usually has a <span class="math-container">$\sim 0.1$</span>—<span class="math-container">$0.2$</span> eV error bar.</p>


================================================================================

Question: What does B3LYP do well? What does it do badly?
Body: <p>I have seen a general criticism of B3LYP in computational chemistry (especially with the 6-31G* basis set).  I would like to know some of the more/less obvious problems that occur when using it and what alternatives exist for that level of theory.  I am aware that it does not include vDW interactions, so I would like to avoid discussion of that unless comparing to another functional which includes it.</p>
<p>I am looking mainly for answers in the organic/inorganic space as this is where it is used for better or worse, but would be happy to also hear about surface catalysis for this functional.</p>

Best Answer: <p>I think this review¹ by Head-Gordon is a useful supplement to Nike's answer. Its combines a review of functional development, a benchmarking of various functionals, and an explanation of the design process for the <span class="math-container">$\omega$</span>B97 functionals. Its also open access, so its a great resource if you are interested in DFT functionals in general.</p>
<p>They benchmarked 200 DFT functionals on a variety of different tests focused on isomerization energies (I), barrier heights (BH), thermochemistry (TC), nonconvalent dimers/clusters (NCD/NCC), and equilibrium bond energies/lengths (EBE/EBL). The datasets were also classified with a suffix indicating whether they are easy (E) or difficult (D) examples.</p>
<p><img src="https://i.sstatic.net/dpeWr.jpg" alt="DFT functional comparison" /></p>
<p>Note the coloring of each cell gives a rough sense of how the functional compares among all 200 functionals tested, rather than just those in the table. The full list of functionals, along with the ranking/RMSD of each functional on each dataset is given in the paper. Even among the whole set, where many of the other functionals also don't explicitly account for dispersion, B3LYP tends to be in the bottom quartile for most of the datasets.</p>
<p>What you can see is that B3LYP does fairly poorly, notably for isomerization energies and equilibrium bond length/energy. So B3LYP on its own generally doesn't do great across a wide variety of problems. However, if one is able to include dispersion corrections, in this case D3(BJ), B3LYP's performance is much improved, becoming basically on par with <span class="math-container">$\omega$</span>B97X-D. The only notable exception is isomerization energies where it still seems to struggle.</p>
<p>I should also note that all the calculations were done with the def2-QZVPPD basis set and a (99,590) DFT integration grid (99 radial shells with 590 points per shell). The use of a large basis and accurate grid should make the comparison as much as possible about relative capabilities of the functionals, rather than basis set completeness. On the flip side, as has been mentioned in other answers, if you are working with certain smaller basis sets, you may obtain more favorable results than expected due to error cancellation.</p>
<ol>
<li><a href="https://doi.org/10.1080/00268976.2017.1333644" rel="noreferrer">N. Mardirossian &amp; M. Head-Gordon. MOLECULAR PHYSICS, 2017
VOL. 115, NO. 19, 2315-2372</a></li>
</ol>


================================================================================

Question: How to map molecular vibrations into lattice phonons?
Body: <p>Let's assume I have already relaxed a molecular structure for a non-trivial molecule and obtained a geometry that is close to the one resulting from X-ray crystallography. This can be achieved in a number of ways, e.g. trivially (relaxation in vacuum) if the molecule is not feeling a significant distortion from its environment, or with ad-hoc tricks such as surrounding it with a frozen local environment. In any case, I have access to a set of vibrational normal modes, that, in a first approximation and for my purposes, may resemble the actual vibrations of the molecules in the crystal.
<em>This can be done with a variety of packages, let's say we are using Gaussian</em>.</p>

<p>Independently, we have information on the phonon spectrum.
<em>Again, this can be done with a variety of packages, let's say we are using Quantum Espresso</em>.</p>

<p>The question is: <strong>is there a good procedure to map (or, at least, to relate) the molecular normal modes with the lattice phonons?</strong> In particular I'm interested in complicated (C1 symmetry) molecules, so any way that does not require relying on symmetry arguments would be preferred.</p>

<p>To put the question into perspective: the goal would be <em>to facilitate the molecular design of systems with improved properties</em>. Designing and obtaining molecules is hard enough as it is, but designing specific crystal packings is adding a layer of complexity, hence the utility of projecting (most of) the key behavior into the single molecule level, whenever it is possible. </p>

Best Answer: <p>If you index the molecule and the atoms in the crystallographic unit cell the same, you could extract the displacement from the phonon eigenvectors and the displacement from the normal modes. By projecting each normal mode displacement onto each phonon displacement, you can get a qualitative relationship between a molecular deformation and a lattice vibration. This could be readily visualized in a heat map. I can't think of any quantitative relationship; however, this offers a quantitative view on relating favorable deformations between a given crystal polymorph and molecule.</p>

<p>Details of the displacement and eigenvectors are here:
Dove, M. T. <em>Structure and Dynamics: An Atomic View of Materials</em> (2002)</p>

<p>with a more thorough work on phonons here: 
Dove, M. T. <em>Introduction to Lattice Dynamics</em> (1993).</p>


================================================================================

Question: What is the largest material that has been studied using density functional theory?
Body: <p>Quantum-chemical calculations are quite expensive with a cost that scales greatly with the number of atoms in the system. What is the largest material that has been modeled using density functional theory (DFT), by number of atoms?</p>

<p>Edit: For the purposes of intellectual curiosity, please feel free to answer with unusually "large" DFT calculations even if it's not <em>the largest</em>. </p>

Best Answer: <p>I know of several papers over the million-atom mark:</p>

<ul>
<li>2,097,152 atoms <a href="https://doi.org/10.1088/0953-8984/22/7/074207" rel="noreferrer">"Calculations for millions of atoms with density functional theory: linear scaling shows its potential"</a></li>
<li>1,012,500 atoms using linear-scaling orbital-free DFT: <a href="https://doi.org/10.1016/j.cplett.2009.04.059" rel="noreferrer">"Accurate simulations of metals at the mesoscale: Explicit treatment of 1 million atoms with quantum mechanics"</a></li>
<li><a href="http://hdl.handle.net/1842/6543" rel="noreferrer">"Million Atom KS-DFT with CP2K"</a> (not clear if it actually happened)</li>
</ul>


================================================================================

Question: What are the &quot;smart algorithms&quot; applied to solve the &quot;curse of dimensionality&quot;?
Body: <p>The &quot;curse of dimensionality&quot; is an ubiquitous issue arising in both electronic structure and quantum molecular dynamics, which refers to the exponential scaling of computational cost with the number of degrees of freedom of the systems of interest.</p>
<p>This problem is manifested in many applications of computational studies (e.g. determining transition states of chemical reactions, geometry optimization of large molecular systems, simulating spectra of molecular systems, etc). The &quot;curse of dimensionality&quot; dramatically restricts the practical size of systems that can be studied computationally.</p>
<p>Assuming that only classical computers are available, what are the &quot;smart algorithms&quot; that have been devised to partially solve this &quot;curse of dimensionality&quot; problem and make the subset of the full problem be close to linear scaling?</p>

Best Answer: <p>This is a very broad question, so I am going to give a very brief overview of typical exponentially-scaling problems. I am not an expert in most of these areas, so any suggestions or improvements will be welcome.</p>

<p><strong>Solving the Schrödinger equation</strong></p>

<p>In order to solve the Schrödinger equation numerically, you need to diagonalise a rank <span class="math-container">$3N$</span> tensor -- as you can see, a pretty impossible operation not only in terms of CPU power, but also in terms of memory. The main problem in fact is that the wavefunction has to be antisymmetric with respect to all electrons, which is the main reason for combinatorial explosion. An alternative way is to expand the wavefunction as a multivariable Taylor series of antisymmetric functions (determinants), and if you were to do it exactly (full configuration interaction), it also scales exponentially. So at this point you can either solve the equation by ignoring most of the correlation between different degrees of freedom (Hartree-Fock, Moller-Plesset perturbation theory, truncated configuration interaction), project the <span class="math-container">$3N$</span>-dimensional problem onto a 3-dimensional one, where the exact solution is unknown, but able to be approximated (density functional theory), or solve the correlated problem exactly for an idealised approximate infinite sum (coupled cluster theory). Another way to solve the equation is to convert it to a sampling problem (diffusion quantum Monte Carlo), which is exact for bosons, but needs an approximation for fermions (fixed node approximation), so that it doesn't scale exponentially. There is a lot of literature on making a lot of the above methods linear-scaling using clever approximations or making the formally exact full configuration interaction method more efficient (full configuration interaction quantum Monte Carlo), but in general, the more computational time you throw in, the larger the class of problems your method can tackle and some of the above approximations are better (and slower) than others.</p>

<p><strong>Exploring potential energy surfaces</strong></p>

<p>This is related to the sampling problem which I will address later. Here you convert a <span class="math-container">$3N$</span>-dimensional sampling problem into a 1, 2 or 3-dimensional one, where you only care about particular nonlinear degrees of freedom (reaction coordinates, collective variables). This gets rid of the exponential scaling, but also needs a certain knowledge of the best/relevant collective variables, which are typically unknown. So this approach is similar in spirit to density functional theory - you convert your problem into a simple one, for which you don't know the exact method and you have to make an educated guess. In terms of sampling nuclear quantum effects, the problem is particularly badly scaling and common methods to estimate typical correlation functions/constants of interest is to either approximate them as simpler classical problems (semi-classical transition state theory), or to convert them into a sampling problem (ring polymer molecular dynamics). The latter is very similar in spirit to diffusion Monte Carlo for electronic structure.</p>

<p><strong>Geometry optimisation</strong></p>

<p>As with all optimisation algorithms, finding a global minimum is an exponentially scaling problem, so to my knowledge, most minimisation algorithms in computational chemistry provide local minima, which scale much better but are also more approximate. In classical computational chemistry you could afford to go one step further and explore much wider conformational space by heating your system up and slowly cooling it down to find some other better minima (simulated annealing). However, as you can see, the result you obtain from this will be highly dependent on chance and convergence will still be exponentially scaling -- there is no way around this.</p>

<p><strong>Sampling</strong></p>

<p>This is one of the biggest unsolved problems in classical computational chemistry. As usual, local sampling is straightforward and typically scales as <span class="math-container">$3N\log 3N$</span> (Markov chain Monte Carlo, leapfrog/any other integrator), whereas enhanced sampling either resorts to using collective variables (metadynamics, umbrella sampling) or providing "locally global" sampling, by smoothening kinetic barriers (replica exchange, sequential Monte Carlo). Now, kinetic barriers slow down local sampling exponentially, but the above methods smoothen these linearly, resulting in cheaper locally enhanced sampling. However, there is no free lunch and global convergence will still be exponential, no matter what you do (e.g. protein folding problem).</p>

<p><strong>Partition function calculation</strong></p>

<p>The partition function is a <span class="math-container">$3N$</span>-dimensional integral (I am going to focus on the classical case, as the quantum one is even more difficult). One way is to try to estimate the partition function (nested sampling, sequential monte carlo), where your convergence will typically scale exponentially but still much, much more efficient than regular quadrature (see exact diagonalisation of the Schrödinger equation, similar problem). This is very difficult, so we typically only try to calculate ratios of partition functions, which are much more nicely behaved. In these cases you can convert the integration problem into a sampling problem (free energy perturbation, thermodynamic integration, nonequilibrium free energy perturbation) and all above sampling issues still apply, so you never really escape the curse of dimensionality, but you get some sort of local convergence, which is still better than nothing :)</p>

<p>So in conclusion, there is no free lunch in computational chemistry and there are various classes of approximations suitable for different problems and in general, the better scaling your problem is, the more approximate and less applicable in general it is. In terms of "best value" nearly exact methods, my vote is on path integral methods (diffusion Monte Carlo, ring polymer molecular dynamics, sequential Monte Carlo), which convert the exponentially scaling problems into polynomially scaling ones (but still with convergence problems) -- although not perfect, at least you won't need all the atoms of the universe to run these <em>and</em> you won't need to know the answer to get the answer, which is sadly an overwhelming problem in many subfields of computational chemistry.</p>


================================================================================

Question: What are some good band-structure/DOS plotting tools/styles?
Body: <p>I am currently using <a href="http://www.p4vasp.at/#/" rel="noreferrer">p4vasp</a> for drawing DOS and band-structure plots, but it is not much handy in modifying the OUTPUTS. Could you please suggest any other good software or tools for that?</p>

Best Answer: <p>Another option is <a href="https://smtg-bham.github.io/sumo/" rel="nofollow noreferrer">Sumo</a>, which is a Python toolkit for plotting and analysis of ab initio solid-state calculation data. It supports VASP, CASTEP and Questaal. Plotting is mostly done through a command-line interface. It can also generate the band paths in the first place. There is also support for plotting phonon bands generate with phononpy.</p>
<p><a href="https://i.sstatic.net/lcL68.png" rel="nofollow noreferrer"><img src="https://i.sstatic.net/lcL68.png" alt="enter image description here" /></a></p>


================================================================================

Question: Benchmark Timings of Machine Learning Potentials vs Molecular Mechanics Force Fields
Body: <p>Machine learning is an increasingly common tool for developing force fields for molecular dynamics simulations. It's not totally clear what should be considered a machine-learning potential, but let's just say a machine learning potential is one which has many, many parameters which do not have any obvious physical interpretation. Any kind of neural network, Gaussian process, or permutationally invariant polynomial model, I would consider as an ML potential for this question.</p>
<p>I have had quite a few conversations in which people say things along the lines of &quot;machine learning potentials are slower than molecular mechanics potentials.&quot; I've observed this to be true for certain ML potentials I've used, but I haven't seen any real benchmarking of this.</p>
<p>Are there references which provide good comparisons of the speed of ML potentials to comparable classical force fields? I have seen a few papers comparing the accuracy of, for instance, classical versus ML water models, but not a corresponding comparison of the speeds of these models.</p>
<hr />
<p><a href="https://arxiv.org/abs/2008.04198" rel="noreferrer">Here is a recent paper</a> which augments a classical force field with an ML force field to kind of get the best of both worlds (speed and accuracy) to a certain extent. I still don't see a real time comparison though.</p>

Best Answer: <p>We performed some timing benchmarks as part of our recent paper, albeit not on molecular dynamics:</p>
<p><a href="http://doi.org/10.1002/qua.26381" rel="noreferrer">&quot;Assessing conformer energies using electronic structure and machine learning methods&quot; <em>Int J Quantum Chem.</em> <strong>2020</strong>; 121:e26381</a></p>
<p><a href="https://i.sstatic.net/n60fRm.png" rel="noreferrer"><img src="https://i.sstatic.net/n60fRm.png" alt="enter image description here" /></a></p>
<p>It was a bit controversial, since we compared single-core CPU times and not in batch mode. Once the ML method runs the model, it's faster - although we saw speedups of ~70-100x for both ML and force field methods.</p>
<p>No doubt ML methods are faster on GPU, but tuned force fields are as well.</p>
<p>It seems hard to imagine an ML method that's truly <em><strong>faster</strong></em> than a good implementation of a force field. Most force field terms are intentionally designed to use only a few arithmetic operations (e.g. bonding terms require distance and a harmonic potential). As such, they can be highly optimized for both GPU and CPU implementations.</p>
<p>On the other hand, ML methods can clearly do a more accurate job with empirically fitting  (e.g, non-bonded interactions). I suspect more hybrid methods will appear over time.</p>


================================================================================

Question: Why do people care about reversibility in molecular dynamics simulations?
Body: <p>When carrying out a molecular dynamics simulation, one has to choose an ensemble. Depending on the problem, one usually works in the microcanonical (NVE), canonical (NVT), or grand canonical ensemble (NPT).</p>
<p>In the NVE ensemble, maintaining reversibility of the simulation is very easy, as the total energy is conserved, so simply starting at the endpoint of the simulation with reversed momenta should reproduce the initial trajectory but in reverse.</p>
<p>In the NVT and NPT ensembles, one has to maintain a constant temperature and/or pressure. Taking NVT as an example, there are many different ways to keep a constant temperature using different thermostats. Thermostats are all methods of keeping temperature constant by exchanging heat with a hypothetical heat bath. There is a range of complexities of thermostats and there's no need to discuss the subtleties of these here. The important point is only that some commonly-used thermostats, such as Nose-Hoover chains, are deterministic and hence result in reversible dynamics. On the other hand, there are Langevin thermostats which are very effective and efficient but are stochastic. So, short of playing a pseudo random generator backwards, the dynamics are not reversible. (I've never seen that reversing a random number generator thing done, but I think it's at least possible in principle?)</p>
<p>I've seen it mentioned many times in the literature that Langevin thermostats result in dynamics which are not reversible. Because I've seen this mentioned so many times, I've always believed it to be important, but I have no idea why one would care about being able to use the end of a simulation as new initial conditions and then propagating the simulation in reverse.</p>
<p>So, why does the reversibility of a molecular dynamics simulation matter? In what situations does using a deterministic thermostat/barostat provide a distinct advantage over a stochastic one?</p>

Best Answer: <p>This is an excellent question! Reversibility in MD is useful because:</p>
<ol>
<li>Time-reversibility in a  numerical integrator leads to a doubling of the accuracy order (<a href="http://dx.doi.org/10.1017/S0962492917000101" rel="noreferrer">see Propositions 5.2 and Theorem 6.2 here</a>).</li>
<li>Reversible maps <a href="https://projecteuclid.org/euclid.aoap/1027961031" rel="noreferrer">can be readily Metropolized</a>, for example in a hybrid Monte-Carlo scheme. This gives an easy way to enhance the sampling efficiency and eliminate the finite time-step error for sampling.</li>
<li>Reversibility is aesthetically pleasing, since the exact equations of motion (both classical and quantum) are reversible.</li>
</ol>
<p>I am sure there are other reasons I have missed. These are the ones that come to mind.</p>


================================================================================

Question: How to overcome the exponential wall encountered in full configurational interaction methods?
Body: <p>Similar to how a molecular orbital, also known as a 1-electron wavefunction, can be represented with a linear combination of  “basis” functions, e.g., atomic orbitals (LCAO):
<span class="math-container">$$\Phi(\mathbf{r})=\sum_i^N c_i \phi(r).$$</span></p>

<p>A multi-electron wavefunction can be represented with a linear combination of multi-electron basis functions, also known as determinants, or electron configurations. </p>

<p><a href="https://i.sstatic.net/yv5ug.png" rel="noreferrer"><img src="https://i.sstatic.net/yv5ug.png" alt="enter image description here"></a></p>

<p>In the limit of all possible excited-determinants the solution is exact. The excited-electronic configurations describe both static electronic and dynamic electronic correlation. </p>

<p>However, the main challenge in solving this problem is that the number of determinants scales combinatorially. For even small molecules (e.g. >4 heavy atoms) and modest basis functions this problem is intractable. To make matters worse, solving for the coefficients <span class="math-container">$c$</span> requires diagonalizing the Hamiltonian, which scales as <span class="math-container">$N^3$</span>.</p>

<p>What are some of the best approaches, generally speaking, for working around this problem (within the confines of configuration interaction approaches) while still capturing most of the static and dynamic electronic correlation? </p>

Best Answer: <p>It is hard to claim that any FCI code overcomes the exponential wall, especially for strongly correlated systems.
There are many algorithms, e.g. CDFCI, HCI, FCIQMC, ACI, etc., which significantly reduce the computational cost of direct FCI calculations and represent wavefunctions by sparse vectors. However, in my opinion, all of them only reduce the prefactor and none of them overcome the exponential wall.</p>

<p>DMRG is another method using a different ansatz. The scaling is indeed polynomial for weakly correlated systems (when it's possible to use small bandwidth). However, for strongly correlated systems, the scaling is difficult to say.</p>

<p>Another possible attempt is to rotate the molecular orbitals, like CASSCF, OptOrbFCI, etc. However, full accuracy of these methods is not expected if the orbital truncation is aggressive.</p>


================================================================================

Question: What are examples of materials that closely correspond to the Heisenberg model?
Body: <p>I use the antiferromagnetic Heisenberg model all the time: </p>

<p><span class="math-container">$ H = J \sum \limits_{\langle i,j \rangle} \vec S_i \cdot \vec S_j$</span></p>

<p>What are some examples of materials that are well-described by this model in <strong>3D</strong>? What about in <strong>1D</strong> and <strong>2D</strong>?</p>

Best Answer: <h2>1D</h2>

<p>A famous example of a nearly ideal spin-<span class="math-container">$1/2$</span> isotropic Heisenberg antiferromagnetic chain (1D) system is copper pyrazine dinitrate [Cu(C<span class="math-container">$_4$</span>H<span class="math-container">$_4$</span>N<span class="math-container">$_2$</span>)(NO<span class="math-container">$_3$</span>)<span class="math-container">$_2$</span>], which was discussed in Hammar et al. <a href="https://doi.org/10.1103/PhysRevB.59.1008" rel="noreferrer">Phys. Rev. B <strong>59</strong>, 1008 (1999)</a> [<a href="https://arxiv.org/abs/cond-mat/9809068" rel="noreferrer">arXiv link</a>]. Another excellent realizations include KCuF<span class="math-container">$_3$</span>, which has stronger (but still low) interchain coupling, and orders at low temperatures. However, the spectrum of magnetic excitations above <span class="math-container">$\sim J/10$</span> matches DMRG and Bethe Ansatz calculations very closely. See e.g. Lake et al. <a href="https://doi.org/10.1103/PhysRevLett.111.137205" rel="noreferrer">Phys. Rev. Lett. <strong>111</strong>, 137205 (2013)</a> [<a href="https://arxiv.org/abs/1307.4071" rel="noreferrer">arXiv link</a>]. A third example is CuSO<span class="math-container">$_4\cdot 5$</span>D<span class="math-container">$_2$</span>O, see Mourigal et al. <a href="https://www.nature.com/articles/nphys2652" rel="noreferrer">Nature Physics <strong>9</strong>, 435 (2013)</a> [<a href="https://arxiv.org/abs/1306.4678" rel="noreferrer">arXiv link</a>].</p>

<p>For <span class="math-container">$S=1$</span> the materials I'm aware of seem to have some degree of single-ion anisotropy. The most well-known one is probably NENP [Ni(C<span class="math-container">$2$</span>H<span class="math-container">$_8$</span>N<span class="math-container">$_2$</span>)<span class="math-container">$_2$</span>NO<span class="math-container">$_2$</span>(ClO$_4)], as studied in e.g. Avenel et al. <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.46.8655" rel="noreferrer">Phys. Rev. B <strong>46</strong>, 8655 (1992)</a>. Earlier this year a molecular coordination complex was introduced and claimed to be one of the most ideal realizations yet, see Williams et al. <a href="https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.2.013082" rel="noreferrer">Phys. Rev. Research <strong>2</strong>, 013082 (2020)</a>.</p>

<p>There are some higher-spin realizations too, but I'm not sure which are good examples, and which aren't.</p>

<h2>2D</h2>

<p>For higher dimensions the lattice geometry really needs to be specified. I will here assume you are interested in simple lattices, and not some geometrically frustrated one (though there is a fascinating literature on e.g. triangular, kagome lattices in the pursuit of quantum spin liquids). Spin-<span class="math-container">$1/2$</span> examples on the square lattice include</p>

<ul>
<li>La<span class="math-container">$_2$</span>CuO<span class="math-container">$_4$</span>, see e.g. the review Manousakis <a href="https://doi.org/10.1103/RevModPhys.63.1" rel="noreferrer">Rev. Mod. Phys. <strong>63</strong>, 1 (1991)</a></li>
<li>Sr<span class="math-container">$_2$</span>CuO<span class="math-container">$_2$</span>Cl<span class="math-container">$_2$</span>, see Greven et al. <a href="https://doi.org/10.1103/PhysRevLett.72.1096" rel="noreferrer">Phys. Rev. Lett. <strong>72</strong>, 1096 (1994)</a></li>
<li>certain organic compounds, see Woodward et al. <a href="https://doi.org/10.1103/PhysRevB.65.144412" rel="noreferrer">Phys. Rev. B <strong>65</strong>, 144412 (2002)</a> [<a href="https://arxiv.org/abs/cond-mat/0107483" rel="noreferrer">arXiv link</a>].</li>
</ul>

<p>An <span class="math-container">$S=5/2$</span> example is found in Rb<span class="math-container">$_2$</span>MnF<span class="math-container">$_4$</span>, see Huberman et al. <a href="https://doi.org/10.1103/PhysRevB.72.014413" rel="noreferrer">Phys. Rev. B <strong>72</strong>, 014413 (2005)</a> [<a href="https://arxiv.org/abs/cond-mat/0504684" rel="noreferrer">arXiv link</a>].</p>

<h2>3D</h2>

<p>I don't know too much about the 3D systems, but the two best realizations of nearest-neighbor only Heisenberg models I'm aware of are</p>

<ul>
<li>The <span class="math-container">$S=5/2$</span> simple cubic lattice compound RbMnF<span class="math-container">$_3$</span>, see e.g. Coldea et al. <a href="https://doi.org/10.1103/PhysRevB.57.5281" rel="noreferrer">Phys. Rev. B <strong>57</strong>, 5281 (1998)</a></li>
<li>KMnF<span class="math-container">$_3$</span> see eg. Salazar et al. <a href="https://doi.org/10.1103/PhysRevB.75.224428" rel="noreferrer">Phys. Rev. B <strong>75</strong>, 224428 (2007)</a> [<a href="https://arxiv.org/abs/1001.3572" rel="noreferrer">arXiv link</a>]. There are also the related KCoF<span class="math-container">$_3$</span> and KNiF<span class="math-container">$_3$</span> discussed in e.g. Oleaga et al. <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925838815000328" rel="noreferrer">J. Alloys and Compounds <strong>629</strong>, 178 (2015)</a> [<a href="http://www.ehu.eus/photothermal/JAlloysCompounds2015.pdf" rel="noreferrer">non-paywall link</a>].</li>
</ul>


================================================================================

Question: Why is the band structure of a supercell more dense than for simple cell?
Body: <p>In order to do DFT calculations of metallic alloys, the start point is a supercell, whose atoms are changed to match the desired stoichiometry. Gold and Silver, for example, both have FCC structure. In Quantum-ESPRESSO, we can both tell to the code the <em>Bravais lattice</em> (<em>ibrav</em> parameter) and specify only one atom position ( <span class="math-container">$(0,0,0)$</span> for an FCC cell ), or use a cubic lattice indicating four atoms positions ( <span class="math-container">$(0,0,0)a$</span>, <span class="math-container">$(0,0.25,0.25)a$</span>, <span class="math-container">$(0.25,0,0.25)a$</span>, and <span class="math-container">$(0,0.25,0.25)a$</span>, in terms of the lattice parameter <span class="math-container">$a$</span>). In theory, the responses should be the same, and the GND energies are retrieved to be equal. However, the band structure for the four-atoms cell is way denser than the single-atom cell. Why does this occur? </p>

Best Answer: <p>What you are referring to is called <em>band folding</em>.  Remember we are plotting the band structure in the reciprocal space. As  the  size  of  the  cell  in real space increases (eg: when you make a super cell), the first Brillouin zone in reciprocal space  shrinks  and  more  lines  populate  the  band  structure resulting from folding back of lines at the boundaries.</p>
<p><a href="https://i.sstatic.net/YAPpc.jpg" rel="noreferrer"><img src="https://i.sstatic.net/YAPpc.jpg" alt="enter image description here" /></a></p>
<p><em>The figure<span class="math-container">$^1$</span> illustrates band folding in the super cell calculations: (a) band structure of a 2D one-band first- neighbor tight-binding model, (b) the same obtained from a 4x4 super-cell calculation, and (c) the same obtained from a 16x16 super-cell calculation. Panel (d) shows the DOS</em></p>
<p>Another figure<span class="math-container">$^2$</span> to visualize band folding in 1D
<a href="https://i.sstatic.net/FUSo7.jpg" rel="noreferrer"><img src="https://i.sstatic.net/FUSo7.jpg" alt="enter image description here" /></a></p>
<p><strong>There are many tools available to <em>unfold</em> the band structure.</strong> Here I list a few</p>
<ul>
<li><a href="https://github.com/band-unfolding/bandup" rel="noreferrer">BandUP</a>: Band Unfolding code for Plane-wave based calculations available for VASP, Quantum Espresso, ABINIT and CASTEP</li>
<li><a href="https://romerogroup.github.io/pyprocar/unfold.html" rel="noreferrer">pyPROCAR</a> offers a utility to unfold bands for VASP</li>
<li><a href="https://iopscience.iop.org/article/10.1088/1361-648X/ab6e8e" rel="noreferrer">Band unfolding made simple</a><span class="math-container">$^3$</span> by May et. al implementation for SIESTA.</li>
<li><a href="https://bitbucket.org/bonfus/unfold-x/src/master/" rel="noreferrer">unfold-x</a> Tool to unfold the band structure of a supercell DFT simulation to a larger Brillouin zone based on QuantumESPRESSO</li>
</ul>
<p><strong>References:</strong></p>
<ol>
<li>Ku, Wei, Tom Berlijn, and Chi-Cheng Lee. &quot;Unfolding first-principles band structures.&quot; Physical review letters 104.21 (2010): 216401.</li>
<li>Yang, Shuo-Ying, et al. &quot;Symmetry demanded topological nodal-line materials.&quot; Advances in Physics: X 3.1 (2018): 1414631.</li>
<li>Mayo, Sara G., Felix Yndurain, and Jose M. Soler. &quot;Band unfolding made simple.&quot; Journal of Physics: Condensed Matter 32.20 (2020): 205902.</li>
</ol>


================================================================================

Question: How do you calculate the &quot;true&quot; chemical potential in classical density functional theory?
Body: <p>In classical density functional theory, one traditionally calculates the chemical potential by taking the variational derivative,
<span class="math-container">\begin{equation}
\mu_{i} = \frac{\delta F}{\delta \rho_{i}}\tag{1}
\end{equation}</span>
of the Helmholtz free energy
<span class="math-container">\begin{equation}
F[\rho] = \int d\textbf{r} f(\rho, \nabla \rho, ...) \textrm{.}\tag{2}
\end{equation}</span></p>
<p>However, this is not directly analogous to the chemical potential in classical thermodynamics. In the latter theory, the chemical potential is defined as a partial derivative with respect to the number of moles,
<span class="math-container">\begin{equation}
\hat{\mu}_{i} = \frac{\partial A}{\partial n_{i}}
\end{equation}</span>
where <span class="math-container">$A$</span> is the homogeneous Helmholtz free energy analogous to <span class="math-container">$F$</span>.
Importantly, <span class="math-container">$n_{i}$</span> is an <em>extensive</em> quantity (e.g. <span class="math-container">$n_{i} = \rho_{i} V$</span>, where <span class="math-container">$V$</span> is the system volume). This means that <span class="math-container">$\mu_{i}$</span>, defined in DFT is actually analogous to the derivative
<span class="math-container">\begin{equation}
\mu_{i} = \frac{\partial A}{\partial \rho_{i}}\tag{3}
\end{equation}</span></p>
<p>How then does one obtain the actual analogue,
<span class="math-container">\begin{equation}
\hat{\mu}_{i} = \frac{\delta F}{\delta n_{i}}\tag{4}
\end{equation}</span>
to the traditional chemical potential?
Is this generalization correct? If so, how does one go about computing such a quantity when the number of moles <span class="math-container">$n_{i}$</span> is now itself a functional of the density,
<span class="math-container">\begin{equation}
n_{i} = \int d\textbf{r} \rho_{i}(\textbf{r})\tag{5}
\end{equation}</span></p>
<p>Aside:</p>
<ul>
<li>It is clear that <span class="math-container">$\partial A/\partial \rho_{i}$</span> is related to the difference between chemical potentials for an incompressible, multicomponent system, e.g. <a href="http://dx.doi.org/10.1103/PhysRevE.83.061602" rel="nofollow noreferrer">http://dx.doi.org/10.1103/PhysRevE.83.061602</a>. Because of this, it is sometimes called an &quot;exchange&quot; chemical potential, e.g. <a href="https://doi.org/10.1039/C6SM02839J" rel="nofollow noreferrer">https://doi.org/10.1039/C6SM02839J</a>.</li>
<li>There is also a connection between the exchange chemical potentials and the osmotic pressure, <span class="math-container">$\pi = \partial A/\partial V$</span>. It is not clear to me how one can calculate the osmotic pressure from a functional either, since it is also an extensive quantity.</li>
</ul>
<p>Related:</p>
<ul>
<li><a href="https://math.stackexchange.com/q/3016507/">https://math.stackexchange.com/q/3016507/</a></li>
<li><a href="https://math.stackexchange.com/q/3697883/">https://math.stackexchange.com/q/3697883/</a></li>
</ul>

Best Answer: <p>Seeing that this question has gathered attention but no replies, I will give it a stab. Note that I am not an expert on DFT <em>or</em> functional calculus, so take this with a grain of salt. As usual, suggestions to the post will be welcome!</p>
<p>Using an approach I saw <a href="https://math.stackexchange.com/questions/235769/is-there-a-chain-rule-for-functional-derivatives">here</a>, we can use a chain rule and obtain the following:</p>
<p><span class="math-container">$$\frac{\delta F[\rho(\boldsymbol{r})]}{\delta n_i[\rho_i(\boldsymbol{r})]} = \int \frac{\frac{\delta F[\rho(\boldsymbol{r})]}{\delta \rho(\boldsymbol{r})}}{\frac{\delta n_i[\rho_i(\boldsymbol{r})]}{\delta \rho(\boldsymbol{r})}} d\boldsymbol{r} = \int \frac{\frac{\delta F[\rho(\boldsymbol{r})]}{\delta \rho_i(\boldsymbol{r})}}{\frac{\delta n_i[\rho_i(\boldsymbol{r})]}{\delta \rho_i(\boldsymbol{r})}} d\boldsymbol{r}\tag{1}$$</span></p>
<p>where the last equality stems from the fact that the integrand will vanish for any <span class="math-container">$\rho_k, k\neq i$</span>. It is straightforward to see that:</p>
<p><span class="math-container">$$\frac{\delta n_i[\rho_i(\boldsymbol{r})]}{\delta \rho_i(\boldsymbol{r})} = 1\tag{2}$$</span> so the above integral reduces to:</p>
<p><span class="math-container">$$\frac{\delta F[\rho(\boldsymbol{r})]}{\delta n_i[\rho_i(\boldsymbol{r})]} = \int \frac{\delta F[\rho(\boldsymbol{r})]}{\delta \rho_i(\boldsymbol{r})} d\boldsymbol{r}\tag{3}$$</span></p>
<p>which is what I assume you mean by writing <span class="math-container">$\frac{\partial F}{\partial \rho_i}$</span>, since this will be a function of <span class="math-container">$\boldsymbol{r}$</span>, unless you integrate over it, and coordinate-dependent chemical potentials don't make much sense to me! Also, note that the functional derivative is only equal to <span class="math-container">$\frac{\partial f}{\partial \rho_i}$</span> if your free energy functional doesn't depend on any derivatives of the density. In this case, you will need higher-order terms as well.</p>
<p>Edit: I will give the osmotic pressure a try as well, but this will definitely need to be checked for some non-obvious errors. Use at your own discretion.</p>
<p>You can express <span class="math-container">$\frac{\delta F[\rho(\boldsymbol{r})]}{\delta V}$</span> as <span class="math-container">$\frac{\delta F[\rho(\boldsymbol{sr})]}{\delta s^3}\Big|_{s=1} = \frac{1}{3s^2}\frac{\delta F[\rho(s\boldsymbol{r})]}{\delta s}\Big|_{s=1}$</span> for some scaling factor <span class="math-container">$s$</span>. In this case, the chain rule tells us that:</p>
<p><span class="math-container">\begin{align}\frac{1}{3s^2}\frac{\delta F[\rho(s\boldsymbol{r})]}{\delta s}\Bigg|_{s=1} &amp;= \frac{1}{3s^2} \int \frac{\delta F[\rho(s\boldsymbol{r})]}{\delta \rho(s\boldsymbol r)} \frac{\partial \rho(s\boldsymbol{r})}{\partial s} d(s\boldsymbol{r})\Bigg|_{s=1}\tag{4}\\ &amp;= \frac{1}{3} \int \frac{\delta F[\rho(\boldsymbol{r})]}{\delta \rho(\boldsymbol{r})} (\nabla\rho(\boldsymbol{r})\cdot\boldsymbol{r}) d\boldsymbol{r}\tag{5}
\end{align}</span></p>


================================================================================

Question: Mathematical expression of SCAN (Strongly Constrained and Appropriately Normed) constraints in DFT
Body: <p>(This question is originally posted on <a href="https://physics.stackexchange.com/questions/559977/mathematical-expression-of-scan-strongly-constrained-and-appropriately-normed">physics stackexchange</a>, but someone suggested me to post on this site, so there you go)</p>
<p>I'm compiling the mathematical expression of <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.115.036402" rel="noreferrer">SCAN</a> (Strongly Constrained and Appropriately Normed) functionals' constraints, but apparently they are not very obvious from their paper (at least for me).
I have compiled some constraints from the <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.115.036402" rel="noreferrer">SCAN paper</a>, the <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.77.3865" rel="noreferrer">PBE paper</a>, and <a href="http://mcc.illinois.edu/workshops/electronicstructure/2017/talks/ES_2017_Perdew.pdf" rel="noreferrer">Perdew's presentation</a>, but some are missing (see the last line of this question).</p>
<p><strong>General form</strong></p>
<p><span class="math-container">$$
\begin{align}
E_{xc}[n] &amp;= \int n \varepsilon_x^{unif}(n) F_{xc}(s,\alpha)\ \mathrm{d}\mathbf{r} \\
E_x[n] &amp;= \int n \varepsilon_x^{unif}(n) F_x(s,\alpha)\ \mathrm{d}\mathbf{r} \\
E_c[n] &amp;= \int n \varepsilon_x^{unif}(n) F_c(r_s,t,\zeta,\alpha)\ \mathrm{d}\mathbf{r} = \int n\left[\varepsilon_c^{unif} + H(r_s,t,\zeta,\alpha)\right]\ \mathrm{d}\mathbf{r} \\
\end{align}
$$</span>
where <span class="math-container">$\varepsilon_x^{unif}(n) = -(3/4\pi)(3\pi^2n)^{1/3}$</span> and <span class="math-container">$\varepsilon_c^{unif}$</span> are obtained from <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.45.13244" rel="noreferrer">Perdew &amp; Wang, 1992</a> and the variables <span class="math-container">$s,\alpha, r_s,t,\zeta$</span> are listed in SCAN's paper <a href="https://journals.aps.org/prl/supplemental/10.1103/PhysRevLett.115.036402/SCAN_Sup.pdf" rel="noreferrer">supplementary material</a>.</p>
<p><strong>Exchange constraints</strong></p>
<ol>
<li>Negativity
<span class="math-container">$$
F_x(s,\alpha) &gt; 0
$$</span></li>
<li>Spin-scaling
<span class="math-container">$$
E_x[n_{\uparrow}, n_{\downarrow}] = \frac{1}{2}\left(E_x[2n_{\uparrow}] + E_x[2n_{\downarrow}]\right)
$$</span></li>
<li>Uniform density scaling
<span class="math-container">$$
E_x[n_\gamma] = \gamma E_x[n]\mathrm{, where}\ n_\gamma(\mathbf{r}) = \gamma^3 n(\gamma \mathbf{r})
$$</span></li>
<li>Fourth order gradient expansion (the expression is from <a href="http://mcc.illinois.edu/workshops/electronicstructure/2017/talks/ES_2017_Perdew.pdf" rel="noreferrer">Perdew's presentation</a>)
<span class="math-container">$$
\lim_{s\rightarrow 0, \alpha\rightarrow 1} F_x(s,\alpha) =  1 + \frac{10}{81}s^2 - \frac{1606}{18225} s^4 + \frac{511}{13500} s^2(1-\alpha) + \frac{5913}{405000}(1-\alpha)^2
$$</span></li>
<li>Non-uniform density scaling
<span class="math-container">$$
\lim_{s\rightarrow\infty}F_x(s,\alpha) \propto s^{-1/2}
$$</span></li>
<li>Tight lower bound for two electron densities
<span class="math-container">$$
F_x(s,\alpha=0) \leq 1.174
$$</span></li>
</ol>
<p><strong>Correlation constraints</strong></p>
<ol start="7">
<li>Non-positivity
<span class="math-container">$$
F_c(r_s,t,\zeta,\alpha) \geq 0
$$</span></li>
<li>Second order gradient expansion
<span class="math-container">$$
\begin{align}
\lim_{t\rightarrow 0}H(r_s, \zeta, t, \alpha) &amp;= \beta \phi^3  t^2 \\
\beta &amp;\approx 0.066725
\end{align}
$$</span></li>
<li>Rapidly varying limit (using the term from PBE's paper, instead of from SCAN's paper, is it &quot;<em>Uniform density scaling to the low density limit</em>&quot;?)
<span class="math-container">$$
\lim_{t\rightarrow\infty}H(r_s, \zeta, t, \alpha) = -\varepsilon_c^{unif}
$$</span></li>
<li>Uniform density scaling to the high density limit
<span class="math-container">$$
\begin{align}
    \lim_{r_s\rightarrow 0}H(r_s, \zeta, t, \alpha) &amp;= \gamma \phi^3\ln \left(t^2\right) \\
    \gamma &amp;= \frac{1}{\pi} (1 - \ln 2)
\end{align}
$$</span></li>
<li>Zero correlation energy for one electron densities
<span class="math-container">$$
H(r_s, \zeta=1, t, \alpha=0) = -\varepsilon_c^{unif}
$$</span></li>
<li>Finite non-uniform scaling limit (<strong>I don't know this</strong>)</li>
</ol>
<p><strong>Exchange and correlation constraints</strong></p>
<ol start="13">
<li><p>Size extensivity (<strong>I don't know this</strong>)</p>
</li>
<li><p>General Lieb-Oxford bound
<span class="math-container">$$
F_{xc}(r_s, \zeta, t, \alpha) \leq 2.215
$$</span></p>
</li>
<li><p>Weak dependence upon relative spin polarization in  the low-density limit (<strong>I don't know this</strong>)</p>
</li>
<li><p>Static linear  response of the uniform electron gas (<strong>I don't know this</strong>)</p>
</li>
<li><p>Lieb-Oxford bound for two-electron densities
<span class="math-container">$$
F_{xc}(r_s, \zeta=0, t, \alpha=0) \leq 1.67
$$</span></p>
</li>
</ol>
<hr>
<p><strong>Summary: What are the constraints for 12, 13, 15, 16? If you want, you can give one constraint in one answer.</strong></p>

Best Answer: <h2>Constraint #13: Size-Extensivity</h2>
<p>While the <a href="https://en.wikipedia.org/wiki/Size_consistency_and_size_extensivity" rel="noreferrer">Wikipedia page for size-consistency and size-extensivity</a> gives a clear formula for the definition of size-consistency, unfortunately they did not give a definition of size-extensivity, so I had to look deeper into the reference that they provided. They say that size-extensivity was introduced by Bartlett, and they cite <a href="https://www.annualreviews.org/doi/10.1146/annurev.pc.32.100181.002043" rel="noreferrer">this review paper of his from 1981</a>, but this paper itself credits the following papers, which I have now looked at for the first time and summarized below:</p>
<ul>
<li>(1955) <a href="https://journals.aps.org/pr/abstract/10.1103/PhysRev.97.1353" rel="noreferrer">Keith Brueckner first recognized</a> in his study of the uniform electron gas, that some terms in the energy obtained by Rayleigh-Schroedinger perturbation theory, incorrectly do not scale linearly with the number of electrons <span class="math-container">$N$</span> as <span class="math-container">$N\rightarrow \infty $</span>. He found a way to cancel out all of these spurious terms, up to fourth-order in the perturbation theory. These spurious terms are also the reason why CI (configuration interaction) converges slowly with respect to the number of excitations included. A year later, Brueckner published the paper with Gell-Mann that became the subject of <a href="https://materials.stackexchange.com/a/1321/5">my other answer here</a>. He was one of the all-time greats, and lived to 90.</li>
<li>(1957) <a href="https://royalsocietypublishing.org/doi/10.1098/rspa.1957.0037" rel="noreferrer">Jeffrey Golstone proved</a> the &quot;linked-diagram theorem&quot; which ensures that the spurious terms found by Brueckner, get cancelled out to <em>all</em> orders in perturbation theory. Goldstone by the way, is one of the most influential physicists that's still alive! He's currently 85 and he was even a co-author on the quite recent paper that popularized Adiabatic Quantum Computing :)</li>
<li>(1965) Bartlett's review paper says that Hans Primas was actually the one to first really emphasize this concept of having proper scaling. I don't know much about Primas, though I found that <a href="https://en.wikipedia.org/wiki/Hans_Primas" rel="noreferrer">he survived to the age of 86</a> :)</li>
<li>(1971-1973) Wilfried Meyer used this concept of size-extensivity to justify the CEPA model. At the time, Meyer had just finished making the <a href="https://materials.stackexchange.com/questions/tagged/molpro">MOLPRO software</a> in the 1960s, a software which is now more than 50 years later, maybe the most popular quantum chemistry software for fast high-accuracy calculations.</li>
<li>(1978) <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/qua.560140504" rel="noreferrer">Bartlett and Purvis used the term</a>  &quot;size-extensivity&quot; here, so this is perhaps where the term was first introduced, but he uses it to describe what the 1955 and 1957 papers achieved.</li>
</ul>
<p><strong>So what is size-extensivity?</strong></p>
<p>My reading of the above Bartlett papers tells me that <strong>for a homogenous system like an electron gas or a set of non-interacting He atoms, the energy should scale linearly with the number of particles</strong> and that the concept can also be generalized to properties other than energy.</p>


================================================================================

Question: What are good resources to learn to code for matter modeling?
Body: <p>A little bit of coding can a long way in matter modeling. Coding is not used only to write big programs but can be used for scripting, data processing, automation, and more. But sometimes many resources (books, papers, etc.) are generic and superficial.</p>
<p>Are any good resources to learn to code in the context of matter modeling? What do you recommend?</p>

Best Answer: <h2><a href="https://missing.csail.mit.edu/" rel="noreferrer">The missing semester</a>: An MIT Course</h2>
<p>For a beginner in the field, most often the difficulty would be in working with unix-like environments, which is the only way to access many of the matter modeling software and probably the most convenient way to analyse the outputs of the calculations. And this is often learned the hard way by searching the internet on how to work with different tools for different purposes, which often takes a long time to get going with and is most often frustrating for people less familiar with the working of these systems. This course introduces one to the basic components of a unix-like environment, working with shell, scripting, editing files, data analysis and many more which would help build a base understanding of these systems from which one can easily learn the specific tools required for one's work. It's named aptly as &quot;the missing semester&quot; as it is often not covered while teaching computational techniques and is left to the learner to figure out on their own.</p>


================================================================================

Question: Introduction to protein folding for mathematicians
Body: <p>My background is mostly in (applied) math with healthy doses of physics and computer science. Are there any good introductions to protein folding and its challenges for someone with that kind of quantitative background but very little knowledge of biology, chemistry, etcetera?</p>

Best Answer: <p>Great question! Protein folding has been in open question for decades. Just recently, there's been a lot of discussion regarding DeepMind's <a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology" rel="nofollow noreferrer">AlphaFold project</a>, which was <em><a href="https://mattermodeling.stackexchange.com/q/3849/5">discussed at length on our very own site here</a></em>.</p>
<p>My answer will be complementary to the one above, but the references I will provide will be closer to the <em>physics</em> side of the problem.</p>
<p>First thing's first, what you should be studying is <em>Statistical Mechanics</em> both in and out of equilibrium. How deep you want to dive into Statistical Mechanics will depend on your research interests or requirements.</p>
<p>As an introduction, a text that would be suitable for you should also be suitable for a biochemist wanting to come to the quantitative side. A &quot;meet halfway&quot; book.</p>
<p>This is it:
<em>&quot;<a href="https://rads.stackoverflow.com/amzn/click/com/0815344309" rel="nofollow noreferrer" rel="nofollow noreferrer">Molecular Driving Forces: Statistical Thermodynamics in Biology, Chemistry, Physics, and Nanoscience</a>&quot;</em>
by Ken A. Dill and Sarina Bromberg</p>
<p>If you've had <em>healthy doses of physics</em>, this book is a great introduction to basic statistical physics, thermodynamics and biophysics. I definitely recommend it. The problems are not difficult and facilitate the development of the needed physical intuition.</p>
<p>The author, Ken Dill is well-known for his contributions in this field. One of his most cited papers is: &quot;<a href="https://doi.org/10.1021/ma00200a030" rel="nofollow noreferrer">A lattice statistical mechanics model of the conformational and sequence spaces of proteins</a>&quot;</p>
<p>An alternative text on this subject is Lectures on <a href="https://www.worldscientific.com/worldscibooks/10.1142/5741" rel="nofollow noreferrer"><em>&quot;Statistical Physics and Protein Folding&quot;</em></a>, by Kerson Huang, late professor emeritus at MIT.</p>
<p>Depending on how much Statistical Mechanics you want to learn, I would also recommend David Chandler's &quot;Introduction to Modern Statistical Mechanics&quot; as a supporting text. Other classics are great too.</p>
<p>A great amount of literature is around, some focusing even on <a href="https://www.pnas.org/content/116/19/9360" rel="nofollow noreferrer">topolgy in protein folding</a>, others more focused on the computational-bio aspect, but I think these are the <em>basics</em> from the physics side.</p>
<p>Hopefully a good start!</p>


================================================================================

Question: Geometry optimization: what happens in the algorithm?
Body: <p>Geometry optimization using density functional theory (DFT) is done by moving the atoms of a molecule to get the most stable structure with the lowest possible ground state energy. My question is what are the equations involved in this procedure? Where are the electrons in this step?</p>

Best Answer: <h2>GDIIS (Geometry Direct Inversion in Iterative Subspace)</h2>
<p>This is a <em><strong>very</strong></em> popular method, implemented in almost all the major quantum chemistry software packages, which is quite different from the quasi-Newton and related approaches mentioned in <a href="https://mattermodeling.stackexchange.com/a/4533/5">Shoubhik's excellent answer</a>.</p>
<p>DIIS was originally <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.540030413" rel="noreferrer">published in 1982 by Peter Pulay</a> for accelerating SCF convergence, and has become the default method for SCF convergence (and also the convergence of many other things now, such as coupled cluster equations) in almost every electronic structure software I know, including (each link takes you to the software's DIIS page in the documentation, just search &quot;DIIS&quot;) <a href="https://www.molpro.net/info/2015.1/doc/manual/node354.html" rel="noreferrer">MOLPRO</a>, <a href="https://psicode.org/psi4manual/master/autodir_options_c/module__scf.html" rel="noreferrer">Psi4</a>, <a href="https://manual.q-chem.com/4.3/sect0080.html" rel="noreferrer">Q-Chem</a>, <a href="http://slater.chemie.uni-mainz.de/cfour/index.php?n=Main.ListOfKeywordsInAlphabeticalOrder" rel="noreferrer">CFOUR</a>, <a href="https://www.mrcc.hu/MRCC/manual/pdf/manual.pdf" rel="noreferrer">MRCC</a>, etc.</p>
<p>In 1984, Csaszar and Pulay published a paper titled <a href="https://www.sciencedirect.com/science/article/abs/pii/S0022286084871987" rel="noreferrer">&quot;Geometry optimization by direct inversion in the iterative subspace&quot;</a>, which has now known as &quot;Geometry DIIS&quot; or &quot;GDIIS&quot;. The original paper is surprisingly simple, being only 4 pages (including all references!) of easy-to-follow steps, <strong>all of which I will summarize here, in the same notation as in the original paper</strong>:</p>
<p>Geometries <span class="math-container">$\mathbf{x}_i$</span> generated in the first <span class="math-container">$m$</span> cycles (each labeled by the index <span class="math-container">$i$</span>) are used to generate an improved geometry for the <span class="math-container">$(m+1)^{\textrm{th}}$</span> cycle, in the following way:</p>
<p><span class="math-container">$$\tag{1}
\mathbf{x}_{m+1} = \sum_{i=1}^m c_i \mathbf{x}_i - \mathbf{H}^{-1}\left(\sum_{i=1}^m c_i\mathbf{g}_{i} 
  \right),
$$</span></p>
<p>where <span class="math-container">$\mathbf{g}_i$</span> is the gradient vector at iteration <span class="math-container">$i$</span> and <span class="math-container">$\mathbf{H}$</span> is the Hessian matrix or an approximation to it, and the <span class="math-container">$c_i$</span> coefficients are obtained by simple linear algebra:</p>
<p><span class="math-container">$$
\begin{pmatrix} 
  \langle \mathbf{e}_1 | \mathbf{e}_1\rangle  &amp; \cdots    &amp; \langle \mathbf{e}_1 | \mathbf{e}_m\rangle &amp; 1\\ 
  \vdots  &amp; \ddots    &amp; \vdots  &amp; \vdots\\
  \langle \mathbf{e}_m | \mathbf{e}_1\rangle  &amp; \cdots    &amp; \langle \mathbf{e}_m | \mathbf{e}_m\rangle &amp; 1\\ 
1 &amp; \cdots &amp; 1 &amp; 0\\ 
\end{pmatrix}
\begin{pmatrix}
c_1 \\ \vdots \\ c_m \\ -\lambda 
\end{pmatrix} 
= 
\begin{pmatrix}
0 \\ \vdots \\ 0 \\ -\lambda 
\end{pmatrix},\tag{2}
$$</span></p>
<p>with <span class="math-container">$\lambda$</span> being the Lagrange multiplier and <span class="math-container">$\mathbf{e}_i$</span> being the error between <span class="math-container">$\mathbf{x}_i$</span> and the &quot;true&quot; final geometry <span class="math-container">$\mathbf{x}_f$</span>, which <em>in the case of a quadratic energy function</em> can be obtained by:</p>
<p><span class="math-container">$$
\mathbf{e}_i = -\mathbf{H}^{-1}\mathbf{g}_i.\tag{3}
$$</span></p>
<p><strong>That's it!</strong></p>
<p>If you're interested in more detail I will also say:</p>
<ul>
<li><em><strong>The reason why we are solving Eq. 2</strong></em> is because we want <span class="math-container">$\sum c_i = 1$</span> and <span class="math-container">$\sum c_i \mathbf{e}_i$</span>. </li>
<li>You can get faster convergence if <span class="math-container">$\mathbf{H}$</span> is updated at each iteration, so we could replace it with <span class="math-container">$\mathbf{H}_i$</span> in all of the above.</li>
<li>You don't need to include <em>all</em> previous <span class="math-container">$\mathbf{x}_i$</span>. To save RAM and cost you can choose to use only the last few geometries, or perhaps only the geometries that are within some threshold of the most recent (ideally most accurate) geometry.</li>
<li>Although the energy landscape is not <em>exactly</em> quadratic, every differential function with a minimum has a quadratic approximation near the minimum (e.g. the quadratic Taylor polynomial), so the approximation in Eq. 3 is quite accurate if the geoemtry at the <span class="math-container">$i^{\textrm{th}}$</span>  iteration is close enough to the minimum (it doesn't even have to be very &quot;close&quot; to the minimum, just enough that energy landscape looks quadratic there).</li>
</ul>


================================================================================

Question: Are there critical mistakes to avoid when creating a workstation (32-128 cores)?
Body: <p>What are practical concerns for creating a workstation that will likely be expanded in the future to have more workstations/nodes?</p>
<p>I am interested in running atomistic and quantum mechanical calculations.</p>
<p>Most jobs would be 12 or fewer cores (nearly all would be 4-6 core jobs), but there would be times when many would be running simultaneously. It would be nice to control priority, but this isn't crucial.</p>
<p>The most used software would be psi4, Gaussian, GPAW, Quantum ESPRESSO and GROMACS. These would all be run on CPU's. In the future perhaps I would try GPU's for plane wave DFT and MD, but, Gaussian basis set electronic structure programs are not well suited for GPU's, yet.</p>
<p>My feeling is that a default Linux distribution would probably be okay, or perhaps <a href="https://mattermodeling.stackexchange.com/a/1427/181">Quantum Mobile</a> in which case Windows would also be okay, but I don't know how well Quantum Mobile is for production runs, it advertises on desktops as educational and tutorial.</p>

Best Answer: <p>One mistake you already described is using virtualization. Nothing is more reliable than running your code directly on your hardware, without intermediaries (even when the virtualization software and Windows are evolving each day). It will be better to have a dual-boot system if you really need Windows than virtualization.</p>
<p>To avoid mistakes, you need to start reading the specifications of the code(s) you pretend to use.</p>
<p>Codes that use planewaves (like QuantumEspresso, ABINIT, etc.) are known to be huge RAM consumers (and I mean, they eat LOTS of RAM). On the other hand, codes like SIESTA uses much less RAM.</p>
<p>If running Molecular Dynamics calculations, must of the main codes used today (LAMMP, GROMACS, NAMD, DESMOND) use GPUs (and believe me, they are much more efficient than using CPUs).</p>
<p>If the code uses hard disk extensively, you will need high capacity hard disk and even SSD drives (or SCSI) to avoid write/read delays.</p>
<p>Finally, if you will add your workstation to a cluster, you will need a high speed network cards to avoid delays in the node-node communication.</p>


================================================================================

Question: What are some examples of active learning methods used in atomistic machine learning?
Body: <p>Many machine learning attempts in atomistic applications (see <a href="https://materials.stackexchange.com/a/25/63">this answer</a>) seem to parameterize models on calculated data (i.e., CCSD(T), DFT, etc.). This approach suggests some automatic procedure for <a href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)" rel="noreferrer">active machine learning</a> in this context.</p>

<p>So the question is: <strong>what is the current status of active machine learning methods for atomistic applications?</strong>
Specifically, I want to know which algorithms are more appropriate for chemical problems.</p>

Best Answer: <p>Inverse designing of materials with known target properties is of great importance (to reduce time, labour, financial etc. costs) than the traditional way of materials design which is guided by human intuition, followed by trial and error loops (see figure below from <a href="https://www.nature.com/articles/s41524-019-0153-8" rel="noreferrer">Active learning in materials science with emphasis on adaptive sampling using uncertainties for targeted design</a>
). </p>

<p>Active learning optimization methods are generally used to iteratively guide experiments such that only a small number of promising experiments are carried out to discover new stable materials with target properties. During the iterative loop, the algorithm can learn from both successful and failed previous experiments to guide the next experiment. </p>

<p>To formulate a simple example, imagine you want to discover new stable quaternary compound in the quaternary system <span class="math-container">$\ce{Ba - Sr - Mn - O}$</span>. One would start the experiment from <span class="math-container">$\ce{BaSrMn2O6}$</span>, and the active learning algorithm learns from the existing data and the success/failure of this experiment to guide the next <em>point</em> to experiment in the quaternary system, such that smaller number of steps will be needed to reach convergence, which is to find a new stable compound. The same example can be reiterated to find new quaternary compound with target dielectric constant etc. It is specifically very useful when the compound search space is huge, but only very few experimental data is available on such compounds. </p>

<p><a href="https://i.sstatic.net/fTpJ3.png" rel="noreferrer"><img src="https://i.sstatic.net/fTpJ3.png" alt="The trajectory of new materials discovery"></a></p>

<p>In theory, we want to find the design <em>x</em>, that maximises a desired function <em>f(x)</em>. Bayesian global optimisation (BGO) is a powerful algorithm used to search for extrema of objective functions which are costly and complex. BGO is applied in active learning to reach convergence. Describing BGO is beyond the scope of this answer and I suggest you look into <a href="https://www.nature.com/articles/s41524-019-0153-8" rel="noreferrer">1</a> to understand the details. I also recommend the paper <a href="https://www.nature.com/articles/s41467-018-03821-9" rel="noreferrer">Experimental search for high-temperature ferroelectric perovskites guided by two-step machine learning</a> that deploys ML classification, regression and active learning techniques to discover new ferroelectric materials.</p>

<p>Finally, I have written an answer <a href="https://materials.stackexchange.com/questions/18/what-is-the-current-status-of-machine-learning-applied-to-materials-or-molecular/1286#1286">here</a> describing the current trend of ML in materials science. Check that out too.</p>


================================================================================

Question: What are the different ways of calculating dispersion constants?
Body: <p>There are many different dispersion corrections out there. The most famous is D3 <a href="https://doi.org/10.1063/1.3382344" rel="nofollow noreferrer">[1]</a> (and the new D4 <a href="https://doi.org/10.1063/1.4993215" rel="nofollow noreferrer">[2]</a>), but there's probably other approaches too. The dispersion energy can be written as:</p>
<p><span class="math-container">$$
E_{\textrm{disp}} = -\sum_{ij}\sum_{6,8,10,\cdot \cdot  }\frac{C_n^{ij}f^{(n)}_{\textrm{damp}}(r)}{r^n},
$$</span></p>
<p>where the <span class="math-container">$C^{ij}_n$</span> coefficients are calculated differently depending on whether using D3 or D4, and the damping function <span class="math-container">$f$</span> can take on various forms.</p>
<p><strong>What are the other ways of calculating <span class="math-container">$C^{ij}_n$</span>?</strong></p>
<hr>


<p>[1] <a href="https://doi.org/10.1063/1.3382344" rel="nofollow noreferrer"><em>J. Chem. Phys.</em> 132, 154104 (<strong>2010</strong>)</a><br>
[2] <a href="https://doi.org/10.1063/1.4993215" rel="nofollow noreferrer"><em>J. Chem. Phys.</em> 147, 034112 (<strong>2017</strong>)</a></p>

<p>P.S. For further discussion and overview, see <a href="https://pubs.acs.org/doi/10.1021/acs.chemrev.5b00533" rel="nofollow noreferrer">Grimme's</a> and <a href="https://pubs.acs.org/doi/pdf/10.1021/acs.chemrev.6b00446" rel="nofollow noreferrer">Tkatchenko's</a> review papers.</p>

Best Answer: <h1>2007 (Becke &amp; Johnson): XDM</h1>
<p>XDM stands for &quot;exchange-hole dipole moment&quot; which is a model <a href="https://aip.scitation.org/doi/10.1063/1.2795701" rel="noreferrer">introduced by Becke and Johnson in 2007</a> for calculating dispersion constants. The formulas are as follows:</p>
<p><span class="math-container">$$\begin{align}
\!\!\!\!\!\!\!\!C_6    &amp;= \frac{\alpha_i\alpha_j}{\mathcal{M}_i\alpha_j + \mathcal{M}_j\alpha_i} \mathcal{M}_i\mathcal{M}_j           \tag{1}\label{eq1} \\
\!\!\!\!\!\!\!\!C_8    &amp;=   \frac{\alpha_i\alpha_j}{\mathcal{M}_i\alpha_j + \mathcal{M}_j\alpha_i}\left(\frac{3}{2}\mathcal{M}_{1i}\mathcal{M}_{2j} + \frac{3}{2}\mathcal{M}_{2i}\mathcal{M}_{1j}\right)            \tag{2}\label{eq2}\\
\!\!\!\!\!\!\!\!C_{10} &amp;=   \frac{\alpha_i\alpha_j}{\mathcal{M}_i\alpha_j + \mathcal{M}_j\alpha_i}\left(\frac{10}{5}\mathcal{M}_{1j}\mathcal{M}_{3j} + \frac{10}{5}\mathcal{M}_{3i} \mathcal{M}_{1j} + \frac{21}{5}\mathcal{M}_{2i} \mathcal{M}_{2j} \right), \tag{3}\label{eq3}
\end{align}$$</span></p>

<p>where for the system <span class="math-container">$x$</span>: the dipole polarizability is <span class="math-container">$\alpha_{x}$</span> and the <span class="math-container">$l^{\textrm{th}}$</span> multi-pole moment is <span class="math-container">$\mathcal{M_{lx}}$</span>, and <span class="math-container">$l=1,2,3$</span> correspond to the dipole, quadrupole, and octupole moments respectively.</p>


================================================================================

Question: How to calculate diffraction pattern from a model of unit cell?
Body: <p>I remember that 20+ years ago we used a program called Powder Cell to calculate diffraction patterns from models of materials (for example, to compare it with experimental data from powder diffraction).
I just fired this program up under wine and it still works:</p>

<p><a href="https://i.sstatic.net/enoJ6.png" rel="noreferrer"><img src="https://i.sstatic.net/enoJ6.png" alt="Powder Cell screenshot"></a></p>

<p>What are modern alternatives?</p>

<p>Note: this program takes description of a unit cell (atoms and unit cell parameters) and produces indexed pattern.
This is different than using the Debye scattering formula to calculate diffraction pattern of any set of atoms, but without Miller indices.  </p>

Best Answer: <p>Perhaps the easiest solution is to use <a href="http://jp-minerals.org/vesta/en/" rel="noreferrer">VESTA</a>, which can read in a CIF (and many other crystalline structure formats) and produce a powder diffraction pattern ("Utilities" > "Powder Diffraction Pattern"). Behind the scenes, VESTA is using <a href="http://fujioizumi.verse.jp/download/download_Eng.html" rel="noreferrer">RIETAN-FP</a> to do the calculation, which has a standalone version to download if you wanted.
<a href="https://i.sstatic.net/qAExS.png" rel="noreferrer"><img src="https://i.sstatic.net/qAExS.png" alt="enter image description here"></a>
Another way you could do this, especially for those who have to do this for many structures and don't mind using Python, is with the <a href="https://pymatgen.org/pymatgen.analysis.diffraction.xrd.html" rel="noreferrer"><code>xrd module</code></a> in <a href="https://pymatgen.org/index.html" rel="noreferrer">Pymatgen</a>, which provides a bit more flexibility. This could be done as shown below:</p>



<pre><code>import pymatgen as pm
from pymatgen.analysis.diffraction.xrd import XRDCalculator
p = '/path/to/my.cif' #path to CIF
structure = pm.Structure.from_file(p) #read in structure
xrd = XRDCalculator() #initiate XRD calculator (can specify various options here)
pattern = xrd.get_pattern(structure)
print(pattern)
</code></pre>


================================================================================

Question: How can quantum computing accelerate materials modeling?
Body: <p>Materials modeling is very computationally intensive and first-principles simulation of a real system of reasonable size typically involves the use of classical supercomputers.</p>

<p>How can quantum computing accelerate materials modeling? Are there differences in how quantum computing can be used to accelerate wavefunction vs. density functional methods?</p>

Best Answer: <p>Quantum computers provide the possibility of simulating systems that are so mathematically complex that classical computers cannot practically be used. The difficulty comes from the fact that the electrical properties of materials, and other chemical systems, are governed by the laws of many body quantum mechanics, which contain equations that are extremely hard to solve with classical computers. The calculations are essentially intractable, taking the age of the universe, or longer, to solve. A quantum computer doesn’t have this problem - by definition the qubits already know how to follow the laws of quantum physics - see the classic paper, <a href="https://www.academia.edu/25883393/Richard_Feynman_Simulating_Physics_with_Computers" rel="noreferrer">Feynman, R., <em>Simulating Physics with Computers</em>, 1982, Int J. Phys.</a>.</p>

<p>In current quantum computers, aka noisy intermediate-scale quantum (NISQ) devices, the noise can actually be a feature when it comes to modeling chemical systems - see <a href="https://www.wired.com/story/opinion-noisy-quantum-computers-chemistry-problems/" rel="noreferrer"><em>Noisy Quantum Computers Could Be Good for Chemistry Problems</em>, Wired, April 2019</a>. Some further research in this direction is given in the following papers.</p>

<p><a href="https://www.nature.com/articles/s41534-020-0259-3" rel="noreferrer">Nam, Y. et al, <em>Ground-state energy estimation of the water molecule on a trapped-ion quantum computer</em>, Nature, npj Quantum Information, April 3, 2020, 6, 33</a></p>

<p><a href="https://www.pnas.org/content/114/29/7555" rel="noreferrer">Reiher M. et al, <em>Elucidating reaction mechanisms on quantum computers</em>, PNAS, July 18, 2017, 114(29) 7555-7560</a></p>

<p><a href="https://arxiv.org/abs/1809.00058" rel="noreferrer">Rivera, N. et al, <em>Ab initio calculation of phonon polaritons in silicon carbide and boron nitride</em>, Aug 31, 2018</a></p>

<p>From a practical standpoint, we are still in the very early stages of developing quantum computers, however, and further research is needed to solve significant challenges, including creating systems with many more qubits, improving qubit performance, and developing software languages and frameworks for quantum computers. For more information see the NSF report <a href="https://arxiv.org/pdf/1706.05413.pdf" rel="noreferrer"><em>Quantum Information and Computation for Chemistry</em>, NSF Workshop Report, June 20, 2017</a></p>

<p>A <a href="https://www.mckinsey.com/industries/chemicals/our-insights/the-next-big-thing-quantum-computings-potential-impact-on-chemicals?cid=soc-web" rel="noreferrer">recent report by McKinsey</a> gives a higher level market overview of the field.</p>


================================================================================

Question: Since MKL is not optimized for AMD hardware, should I use a math library specific to AMD, or would an open-source one be just as good?
Body: <p>The codes for quantum mechanical calculations make heavy use of linear algebra, and it seems most of them delegate this task to time-tested and highly optimized libraries, instead of trying to deploy their own. There is a plethora of options. For example, when we install Gamess-US in Ubuntu GNU/Linux, several options are listed, both proprietary and free / open source software:</p>

<p><a href="https://i.sstatic.net/pCx9x.png" rel="noreferrer"><img src="https://i.sstatic.net/pCx9x.png" alt="enter image description here"></a></p>

<p>In the past I used ATLAS, and then MKL, as we are nudged towards it by the (very fast) qualifier given in the install screen. But then I was doing some research on it, and found this information on its <a href="https://en.wikipedia.org/wiki/Math_Kernel_Library" rel="noreferrer">wikipedia page</a>:</p>

<blockquote>
  <p>Intel MKL and other programs generated by the Intel C++ Compiler improve performance with a technique called function multi-versioning: a function is compiled or written for many of the x86 instruction set extensions, and at run-time a "master function" uses the CPUID instruction to select a version most appropriate for the current CPU. However, as long as the master function detects a non-Intel CPU, it almost always chooses the most basic (and slowest) function to use, regardless of what instruction sets the CPU claims to support. This has netted the system a nickname of "cripple AMD" routine since 2009.</p>
</blockquote>

<p>This got me a bit worried, as I'm trying to do a Gamess-US install in a machine with a <a href="https://en.wikipedia.org/wiki/Zen_(microarchitecture)" rel="noreferrer">Ryzen</a> processor, and so I'm afraid I will get a crippled install if I use intel MKL. Given the number of remaining choices, without a clear idea of relative performance between them, I got some <a href="https://en.wikipedia.org/wiki/Analysis_paralysis" rel="noreferrer">analysis paralysis</a>. The obvious alternative would be one of AMD, but in absence of a substantial speed advantage, I could as well use a open source one, as I try to favor free software whenever I can.</p>

<p>If someone more experienced could give some advice on this problem, I would be grateful. How does the several linear algebra libraries compare to each other, on the workloads typical of materials modeling? Is there a big difference from one to other, or they are all optimized to such degree that doesn't matter much which one is used. Is there one with good overall performance over a variety of hardware, not biased against alternative hardware like the MKL seems to be?</p>

Best Answer: <p>OpenBLAS is a free, open-source BLAS library that has fast support for even recent processors. (It is based on the earlier, famous GotoBLAS library which became obsolete years ago.) OpenBLAS is also multi-platform: in addition to x86 and x86_64 it also supports other architectures like ARM and PowerPC. OpenBLAS also has runtime CPU detection; if you compile it in, the resulting library supports all processors and picks the best kernel at runtime.</p>
<p>IIRC OpenBLAS is as fast (sometimes has been even faster!) as MKL on some Intel processors; I don't think I've seen benchmarks on AMD hardware. But, the nice thing about OpenBLAS is that it's free, so it typically comes built-in your linux distribution. E.g. OpenBLAS has been available on Fedora and Red Hat Enterprise for a few years now (courtesy of yours truly), and to use it you just need to install the package</p>
<h3>yum install openblas-devel</h3>
<p>and then link to the flavor you want: on Fedora/Red Hat the available variants are</p>
<ul>
<li><code>libopenblas</code> sequential library with 4-byte integers</li>
<li><code>libopenblaso</code> OpenMP parallel library with 4-byte integers</li>
<li><code>libopenblasp</code> pthreads parallel library with 4-byte integers</li>
<li><code>libopenblas64</code> sequential library with 8-byte integers</li>
<li><code>libopenblaso64</code> OpenMP parallel library with 8-byte integers</li>
<li><code>libopenblasp64</code> pthreads parallel library with 8-byte integers</li>
</ul>


================================================================================

Question: Create a new DFT functional from experimental data
Body: <p>This is my first time here and I think you can help me.</p>
<p>I would like to try (just for fun) to create a new DFT functional starting from experimental data. Do you have any manual, procedure or something that can help me? I know that the recent functionals (like the ones from Truhlar's group) are parametrized to fit experimental data, then I wonder how I can do that by myself.</p>
<p>The idea behind this question is that some years ago I ran some calculation with a molecule I synthesized and crystallized, obtaining also crystallographic information. The optimization of the geometry with a lot of different functionals fails to reproduce the experimental structure. Only with MP2 I was able to obtain the correct geometry. Now I wondered if it is possible to create a DFT functional starting from an experimental structure.</p>

Best Answer: <p>An starting point can be the following review paper:</p>
<ul>
<li><a href="https://doi.org/10.1088/0965-0393/13/1/R01" rel="noreferrer">Designing meaningful density functional theory calculations in materials science—a primer. Ann E Mattsson <em>et al</em>. 2005 Modelling Simul. Mater. Sci. Eng. 13 R1 (DOI: 10.1088/0965-0393/13/1/R01)</a></li>
</ul>
<p>As the authors state:</p>
<blockquote>
<p>Our primary goal is to provide practical guidance in the design of
meaningful DFT simulations, and we discuss many of the computational
issues that need to be confronted. Adapting a set of calculations to
simulate a given physical property involves careful construction of a
model system and detailed manipulation of many options available in a
code. The need to check and verify the adequacy of any calculation
with respect to computational variables is repeatedly demonstrated. A
secondary goal is to encourage publishing salient calculational
details. For very large calculations, reporting the specifics is
especially important.</p>
</blockquote>
<p>Another important guide can be the article:</p>
<ul>
<li><a href="http://dx.doi.org/10.1063/1.1904565" rel="noreferrer">Prescription for the design and selection of density functional
approximations: More constraint satisfaction with fewer fits. John P. Perdew et a. J. Chem. Phys. 123, 062201 (2005); DOI: 10.1063/1.1904565)</a></li>
</ul>
<p>Here the authors addressed some issues like:</p>
<ul>
<li>Is density functional theory <em>ab initio</em>?</li>
<li>Why the uniform density limit is sacrosanct.</li>
<li>Is exact exchange needed?</li>
</ul>
<p>A good question they ask is: &quot;<em>Is there any secure place for empiricism in Kohn–Sham density functional theory?</em>&quot;. Later on, they answer: &quot;<em>...we hope that empiricism can be completely avoided by modeling the adiabatic connection, somewhat as in Ref. 76. (Note, however, that limited empiricism
can provide a useful tack-on long-range van der Waals correction...)</em>&quot;</p>


================================================================================

Question: References for Molecular Dynamics?
Body: <p>I am new to the world of Molecular Dynamics (zero practical experience), and I know that a starting point to really go into the back-bone of MD is Statistical Mechanics (took a semester course in undergrad, loved it, will be the focus of my PhD).</p>
<p>My ultimate goal is to become a <em>developer</em> of advanced or modern MD, such as Many-body Molecular and Quantum MD methods (e.g. Path Integral MD). Example systems I am interested in being able to model are quantum fluids and super-critical fluids.</p>
<p>What are some <em>essential</em> books I need to get and read in order to become an avid MD theoretician? Are there any <em>must read</em> papers I need to dedicate my weekend mornings to?</p>

Best Answer: <p>A good place to start is the classical Allen-Tildesley book, Computer Simulation of Liquids, which covers the basics of molecular dynamics that hasn't really changed in a long time. The book can be supplemented with literature, such as review articles for whatever it is you want to do.</p>
<p>Software manuals are also often quite useful to find out how things are done nowadays. For instance, GROMACS has had very good documentation for a long time; <a href="http://manual.gromacs.org/current/reference-manual/index.html" rel="noreferrer">here</a> is a link to their current reference manual which covers a lot of topics and has references to the original literature.</p>


================================================================================

Question: How exactly does one compute the vibrational density of states from the output of a molecular dynamics simulation?
Body: <p>There are many things that can be done with the output of a molecular dynamics simulation, but one of the more powerful things is the ability to compute the vibrational density of states (or the infrared spectrum and raman spectrum). It is well known that the vibrational density of states can be related to the fourier transform of the velocity autocorrelation function (VACF). My question is, how exactly do I compute this autocorrelation function?</p>
<p>To be more concrete, suppose I have a large file of xyz-formatted velocities, as I would get from a molecular dynamics simulation (MD). Each velocity frame is separated in time by <span class="math-container">$k\Delta t$</span>, where <span class="math-container">$k$</span> is the stride at which you save the velocities and <span class="math-container">$\Delta t$</span> is the actual simulation time step.</p>
<p>The VACF is defined as,
<span class="math-container">$$
C(t)=A\langle\vec{v}(0)\cdot\vec{v}(t)\rangle
$$</span>
where <span class="math-container">$A$</span> is some normalization constant and <span class="math-container">$\vec{v}(t)$</span> is the velocity of the system at time <span class="math-container">$t$</span>. The reason I am asking this question is not because there aren't resources explaining how to calculate this quantity, and why it is useful, but because there are way too many of them and they often contradict each other.</p>
<p>First of all, there is a fairly trivial way to calculate this which is to actually do the dot-product between all the velocity vectors for all values of time and just average them. This scales as <span class="math-container">$O(N^2)$</span> and is almost never used because there is an elegant way to calculate correlation functions using fourier transforms which is much more efficient, so the answer should describe that approach, though I don't think it's necessary to describe what a fourier transform actually is in answering this question.</p>
<p>Here are some questions I would like answered in detail, which aren't totally clear to me because of conflicting information I have seen.</p>
<ul>
<li>What value should be chosen for the normalization constant <span class="math-container">$A$</span>?</li>
<li>Is the final VACF meant to be the sum of the VACFs of the velocity components for each atom, or the mean of the VACFs? (I'm pretty sure it's the sum, but I've seen the mean written in a few places.)</li>
<li>This is the question that is least clear to me. Where does the gap between frames, <span class="math-container">$k\Delta t$</span>, enter the picture when calculating the VACF?</li>
</ul>
<hr />
<p>There is a very good outline of how to do this mathematically in this <a href="https://physics.stackexchange.com/questions/282734/phonon-density-of-states-from-velocity-autocorrelation-function">Physics SE question</a>. What I am hoping for is something more like an algorithm or an answer written in pseudo-code.</p>

Best Answer: <p>Ok, I figured out the answer, so I will go ahead and answer my own question.</p>
<p>First, I will provide an overview in bullet points:</p>
<ul>
<li>[Optional] Generate the velocities at evenly spaced frames from the geometries by the equation <span class="math-container">$\frac{x(t)-x(t-\Delta T)}{n\Delta t}$</span> where <span class="math-container">$x(t)$</span> is the geometry at time <span class="math-container">$t$</span>. <span class="math-container">$n$</span> is the lag between frames form the simulation that are saved and <span class="math-container">$\Delta t$</span> is the simulation time step.</li>
<li>Read in the velocities generated in the previous step or which were saved from the simulation. Note that it is slightly better to use the ones from the simulation since they will tend to be slightly more accurate due to the fact that after the simulation one does not usually have access to all of the data which one does during the simulation</li>
<li>Compute the average <span class="math-container">$\langle v(t)v(t+T)\rangle$</span> (more on this in a minute)</li>
<li>Renormalize the above time correlation by its value at <span class="math-container">$t=0$</span>, so that the max value is <span class="math-container">$1.0$</span></li>
<li>Perform a fourier transform of the renormalized velocity autocorrelation function (VACF)</li>
<li>Get the discrete frequencies used to build up the fourier transform for plotting purposes</li>
</ul>
<p><strong>Detailed Answer:</strong></p>
<p>First, you need to get your hands on a bunch of velocities which are evenly spaced in time. Typically this time will not be the simulation time, but you will need to know what the time actually is.</p>
<p>Now, the original source of my confusion was due to the fact that nearly any paper you read which says they computed the VACF will use the following equation:
<span class="math-container">$$\mathrm{VACF}=\langle v(0)v(t)\rangle$$</span>
This is an incredibly terse notation, normally <span class="math-container">$\langle\rangle$</span> would imply an ensemble average which means you average some quantity for one or many atoms over time. The fact a time average and the true ensemble average are equal is due to the ergodic principle, which I won't describe here.</p>
<p>What you are computing in the VACF is really not an ensemble average though. You are averaging over <em>all atoms</em> and <em>all time lags</em>.</p>
<p>So, I will rewrite this equation as:</p>
<p><span class="math-container">$$
\langle v(t)v(t+T)\rangle=\sum_{i=1}^{T}\sum_{j=1}^{N}v_j(t_i)\cdot v_j(t_i+n\Delta t)
$$</span></p>
<p>Once again, <span class="math-container">$n\Delta t$</span> is the time-spacing between frames. <span class="math-container">$T$</span> is the total length of signal you have (i.e. number of points in the time-series). Notice that since each time windows will be getting shorter as <span class="math-container">$i$</span> increases, one usually performs the first sum up to <span class="math-container">$M/2$</span> as long as this is long enough that the correlation function has sufficiently decohered within that window. This ensures that one has the same statistical accuracy in each time window by keeping the length of each window the same. One can further increase the accuracy by taking a very long time series as one might get from MD and chopping it into independent segments and averaging the result of the above signal over those independent segments.</p>
<p>Finally, one usually renormalizes <span class="math-container">$\langle v(t)v(t+T)\rangle$</span> by <span class="math-container">$\langle v(0)v(0)\rangle$</span>. That is, the first point in the TCF will equal <span class="math-container">$1$</span> and all other points should be bounded between <span class="math-container">$-1$</span> and <span class="math-container">$1$</span>. This is proper to do since this is a correlation function, and correlations are usually normalized to lie between <span class="math-container">$-1$</span> and <span class="math-container">$1$</span>.</p>
<p><strong>Computational Details:</strong></p>
<p>The pertinent equation I have written up above is quite easy to calculate numerically. It is just two nested for loops. However, this can be extremely slow to calculate as it scales quadratically, and one often has very long time-series and many atoms when doing MD. It turns out, for reasons I won't explain here, that the calculation of either autocorrelation functions or cross-correlation functions can be written in terms of a fourier transform and inverse fourier transform. In that case, rather than directly computing the product as above, one computes a fourier transform of the time series, takes the product of that series with itself, and inverse fourier transforms.</p>
<p><strong>Getting the VDOS:</strong></p>
<p>Going from a correlation function to something more physically meaningful is usually rather simple, as there are many physical observables which are directly related to some kind of TCF. So, to get the VDOS, which is what I happened to be asking about, one simple performs a fourier transform of the VACF. There is a last point which is that the fourier transform builds up a frequency-space representation of a time-domain signal from periodic basis functions (usually it's a complex basis formed of sines and cosines). So, to actually plot the VDOS, which is what you usually want, you need to get the frequencies of these basis functions and the corresponding intensities.</p>
<p><strong>Practical Details:</strong></p>
<p>If all you want are simple auto-correlations and cross-correlations, there is a small python package called <a href="https://github.com/pdebuyl-lab/tidynamics" rel="noreferrer">tidynamics</a> which can do this. It's also quite easy to implement these correlations calculations using either <a href="https://pypi.org/project/pyFFTW/" rel="noreferrer">pyfftw</a> or the <a href="https://numpy.org/doc/stable/reference/routines.fft.html" rel="noreferrer">numpy fft module</a>. Note that to get the frequencies for the VDOS, you need the function <a href="https://numpy.org/doc/stable/reference/generated/numpy.fft.fftfreq.html#numpy.fft.fftfreq" rel="noreferrer">np.fft.fftfreq</a>.</p>


================================================================================

Question: Easy ways to generate &quot;teaching&quot; band structures in Python?
Body: <p>I'd like to introduce band structure to a class of undergraduate chemists, along the lines of Roald Hoffmann's <em>Solids and Surfaces</em>.</p>
<p>That is, I'd like to start with a s-band in 1D, which is easy because you can do it with hydrogen:</p>
<p><a href="https://i.sstatic.net/P8OlZ.png" rel="noreferrer"><img src="https://i.sstatic.net/P8OlZ.png" alt="1D band structure of hydrogen 1s" /></a></p>
<p>But then I'd like to look at different 2p-bands in 1D, 3d-bands in 1D, and build up a 2D and 3D band structure.</p>
<p>For example, here's a 2p band in 1D which naturally introduces nodes (from <em>Solids and Surfaces</em> by Roald Hoffmann):</p>
<p><a href="https://i.sstatic.net/MyUu0.png" rel="noreferrer"><img src="https://i.sstatic.net/MyUu0.png" alt="1D band structure of p-orbitals" /></a></p>
<p>And here's how this builds the 2D case with both s and p orbitals (also from <em>Solids and Surfaces</em> by Hoffmann):</p>
<p><a href="https://i.sstatic.net/2c144l.png" rel="noreferrer"><img src="https://i.sstatic.net/2c144l.png" alt="2D band structure including s and p orbitals" /></a></p>
<p>Ideally all of these would make it fairly easy to change the lattice parameters to show the effects, delocalization vs. localization, etc.</p>
<p>There are plenty of codes and notes for <em>real</em> materials. Are there already some tight-binding packages / notebooks for teaching purposes?</p>

Best Answer: <p>I'd suggest having a look at <a href="http://www.physics.rutgers.edu/pythtb/" rel="noreferrer">PythTB</a> by Sinisa Coh and David Vanderbilt, which I found very useful and easy to use.</p>
<p>Here's a stab at recreating the band structure of the 2d square lattice shown above</p>
<pre class="lang-py prettyprint-override"><code>#!/usr/bin/env python
&quot;&quot;&quot;s and p orbitals on 2d square lattice.

 - nearest neighbor hopping only
 - no hopping between orbitals of different type (e.g. s &lt;&gt; pz)
&quot;&quot;&quot;

from pythtb import tb_model
import matplotlib.pyplot as plt

# 2d square lattice
lat=[[1.0, 0.0],[0.0, 1.0]]
# one s and 3 p orbitals (at the same site)
orb=[[0.,0.] for _ in range(1+3)]

# 2-dimensional k-space, 2-dimensional real space
my_model=tb_model(dim_r=2,dim_k=2,lat=lat,orb=orb)

# p-orbitals are at higher energy
my_model.set_onsite([0,3,3,3])

# set hoppings (one for each connected pair of orbitals)
# (amplitude, i, j, [lattice vector to cell containing j])

# isotropic nearest-neighbor hopping for s and pz
for neighbor in [ [1,0], [0,1]]:
    my_model.set_hop(-0.5, 0, 0, neighbor) # s
    my_model.set_hop(-0.25, 3, 3, neighbor) # pz

# directional nearest-neighbor hopping for px and py
my_model.set_hop(+0.25, 1, 1, [1,0])
my_model.set_hop(-0.25, 1, 1, [0,1])
my_model.set_hop(-0.25, 2, 2, [1,0])
my_model.set_hop(+0.25, 2, 2, [0,1])

# generate list of k-points following a segmented path in the BZ
k_label=[r&quot;<span class="math-container">$\Gamma$</span>&quot;,r&quot;<span class="math-container">$X$</span>&quot;, r&quot;<span class="math-container">$M$</span>&quot;, r&quot;<span class="math-container">$\Gamma$</span>&quot;]
path=[[0.0,0.0], [0.5,0.0], [0.5, 0.5], [0.0,0.0]]
(k_vec,k_dist,k_node)=my_model.k_path(path,100)

# solve model
evals=my_model.solve_all(k_vec)

# plot band structure
fig, ax = plt.subplots()
for band in evals:
    ax.plot(k_dist, band)
ax.set_title(&quot;s and p orbitals on 2d square lattice&quot;)
ax.set_xlabel(&quot;Path in k-space&quot;)
ax.set_ylabel(&quot;Band energy&quot;)
ax.set_xticks(k_node)
ax.set_xticklabels(k_label)
ax.set_xlim(k_node[0],k_node[-1])
for n in range(len(k_node)):
  ax.axvline(x=k_node[n], linewidth=0.5, color='k')
fig.tight_layout()
fig.savefig(&quot;s_and_p.pdf&quot;)
</code></pre>
<p><a href="https://i.sstatic.net/hQFqO.png" rel="noreferrer"><img src="https://i.sstatic.net/hQFqO.png" alt="enter image description here" /></a></p>
<p>Note that matplotlib colors the lines by band energy at each k-point, not by the orbital symmetry of the band.</p>
<p>This may be suitable as a first simple model to understand how the hopping matrix elements influence the band structure.</p>
<p>From here, there are a number of possible extensions to learn more / make it more realistic:</p>
<ul>
<li>the nearest-neighbor hopping matrix elements between different orbitals are not entirely independent parameters and can be expressed in terms of bond integrals using the <a href="https://en.wikipedia.org/wiki/Tight_binding#Table_of_interatomic_matrix_elements" rel="noreferrer">Slater-Koster tables</a>.</li>
<li>double the size of the unit cell, look at band folding and/or introduce site-dependent onsite energies (e.g. to simulate symmetry breaking when adsorbing a 2d material on a substrate)</li>
<li>go 3d</li>
<li>look at the effect of introducing 2nd- and 3rd-nearest neighbor hopping (e.g. in graphene, 2nd-nearest neighbor hopping affects the electronic structure near the Fermi level only by a rigid shift, while 3rd-nearest neighbor hopping plays a significant role)</li>
</ul>
<p>The extensions above are still within the tight-binding framework, treating electrons as non-interacting Fermions, and can be modeled with pythtb.</p>
<p>P.S. Another conceptually very interesting extension would be to introduce an on-site Coulomb repulsion term between electrons, i.e. to move from tight binding to the Hubbard model. The Hubbard model is no longer solvable exactly (except for 1d), but a solution to its mean-field approximation can be found by iteration to self-consistency (similar to what DFT codes do, but on a lattice).
This is not very difficult to do, but it goes beyond the scope of pythtb and I am not aware of a similar, easy-to-use software package that implements this functionality. If someone does, please let me know, I think it would make for a great addition to this toolbox.</p>
<p>P.P.S. There have been at least two attempts (one by me) to convince the authors to host the pythtb source code on GitHub to make it easy to contribute back (e.g. I made some minor modifications for my Ph.D. thesis). So far, they have been reluctant due to possible maintenance/supervision involved, but it may be worth trying again.</p>


================================================================================

Question: Is there a list of all universality classes for phase transitions with examples of each?
Body: <p>I've often had this problem: I have a model that has a phase transition in it, but I don't know what universality class it falls into or what the universality class is <em>called</em>. </p>

<p>Is there anywhere on the web where there is a big list of all the classes and examples of each (hopefully with critical exponents as well)?</p>

<p><strong>For example:</strong><br>
Name: 2D Ising Model<br>
Symmetry: Z<span class="math-container">$_2$</span><br>
Dimension: 2<br>
Other examples:  liquid-gas transition, etc<br>
Critical exponents: ...   </p>

Best Answer: <p>A locally interacting system displaying a continuous phase transition belongs to a universality class that is determined solely by the system symmetries and dimensionality.</p>

<p>Drawing from <a href="https://en.wikipedia.org/wiki/Universality_class#List_of_critical_exponents" rel="noreferrer">Wikipedia's list</a> (itself mostly based on <a href="https://doi.org/10.1103/RevModPhys.76.663" rel="noreferrer">Ódor's paper</a>) and <a href="https://physics.stackexchange.com/a/255632/75633">this answer</a> from Physics SE, here's a partial list of universality classes and critical exponents:</p>

<p><span class="math-container">\begin{array}{| c | c | c c c c c c | c|}
\hline
\textbf{dim.} &amp; \textbf{Symm.} &amp;\alpha &amp; \beta &amp; \gamma &amp; \delta &amp; \nu &amp; \eta &amp; \textbf{class} \\
 \hline \hline
\text{any} &amp; \text{any} &amp; 0 &amp; 1/2 &amp; 1 &amp; 3 &amp; 1/2 &amp; 0 &amp; \text{Mean field} \\\hline
2 &amp; \text{Sym}_{2} &amp; 0 &amp; 1/8 &amp; 7/4 &amp; 15 &amp; 1 &amp; 1/4 &amp;  \\
3 &amp; \text{Sym}_{2} &amp; 0.11007(7) &amp; 0.32653(10) &amp; 1.2373(2) &amp; 4.7893(8) &amp; 0.63012(16) &amp; 0.03639(15) &amp; \text{Ising} \\
4+ &amp; \text{Sym}_{2} &amp; 0 &amp; 1/2 &amp; 1 &amp; 3 &amp; 1/2 &amp; 0 &amp; \\\hline
2 &amp; \text{Sym}_{3} &amp; 1/3 &amp; 1/9 &amp; 13/9 &amp; &amp;  5/6 &amp; &amp; \text{3-state Potts} \\\hline
2 &amp; \text{Sym}_{4} &amp; 2/3 &amp; 1/12 &amp; 7/6 &amp; &amp;  2/3 &amp; &amp; \text{Ashkin-Teller (4-state Potts)} \\\hline
3 &amp; {\mathcal {O}}(2) &amp; −0.0146(8) &amp; 0.3485(2) &amp; 1.3177(5) &amp; 4.780(2) &amp; 0.67155(27) &amp; 0.0380(4) &amp; \text{XY} \\  \hline
3 &amp; {\mathcal {O}}(3) &amp; −0.12(1) &amp; 0.366(2) &amp; 1.395(5) &amp; &amp; 0.707(3) &amp; 0.035(2) &amp; \text{Heisenberg} \\\hline
1 &amp; \mathbf{1} &amp; &amp; 0 &amp; 1 &amp; &amp; 1 &amp; &amp; \\
2 &amp; \mathbf{1} &amp; −2/3 &amp; 5/36 &amp; 43/18 &amp; 91/5 &amp; 4/3 &amp; 5/24 \\
3 &amp; \mathbf{1} &amp; −0.625(3) &amp; 0.4181(8) &amp; 1.793(3) &amp; 5.29(6) &amp; 0.87619(12) &amp; 0.46(8) \text{ or } 0.59(9) &amp; \text{Ordinary percolation} \\
4 &amp; \mathbf{1} &amp; −0.756(40) &amp; 0.657(9) &amp; 1.422(16) &amp; &amp; 0.689(10) &amp; −0.0944(28) \\
5 &amp; \mathbf{1} &amp; &amp; 0.830(10) &amp; 1.185(5) &amp; &amp; 0.569(5) &amp;  \\
6+ &amp; \mathbf{1} &amp; −1 &amp; 1 &amp; 1 &amp; 2 &amp; 1/2 &amp; 0 \\
\hline
1 &amp; \mathbf{1} &amp; 0.159464(6) &amp; 0.276486(8) &amp; 2.277730(5) &amp; 0.159464(6) &amp; 1.096854(4) &amp; 0.313686(8) &amp;  \\
2 &amp; \mathbf{1} &amp; 0.451 &amp; 0.536(3) &amp; 1.60 &amp; 0.451 &amp; 0.733(8) &amp; 0.230 &amp; \text{Directed percolation}\\
3 &amp; \mathbf{1} &amp; 0.73 &amp; 0.813(9) &amp; 1.25 &amp; 0.73 &amp; 0.584(5) &amp; 0.12 \\
4+ &amp; \mathbf{1} &amp; −1 &amp; 1 &amp; 1 &amp; 2 &amp; 1/2 &amp; 0 \\
\hline
\end{array}</span></p>

<p>where <span class="math-container">$\mathbf{1}$</span> denotes the <a href="https://en.wikipedia.org/wiki/Trivial_group" rel="noreferrer">trivial group</a>, <span class="math-container">$\text{Sym}_{n}$</span> the <span class="math-container">$n$</span>-th <a href="https://en.wikipedia.org/wiki/Symmetric_group" rel="noreferrer">Symmetric group</a>, and <span class="math-container">$\mathcal{O}(n)$</span> the <a href="https://en.wikipedia.org/wiki/Orthogonal_group" rel="noreferrer">Orthogonal group</a>.</p>


================================================================================

Question: How to do periodic DFT calculations using Quantum ESPRESSO?
Body: <p>I have only done quantum mechanical computations using non-periodic wavefunctions. I'd like to try to periodic-wavefunction density functional theory. </p>

<p>Quantum ESPRESSO seems like a good package because it's open-source. </p>

<p>Can some one help me get started by providing me some tips or an input file? </p>

<p>For example, I'd like to reproduce these calculation details:</p>

<blockquote>
  <p>The crystal structures of HC1 and HC2 were optimized using PBE-D2 as implemented in
  Quantum ESPRESSO. The plane- wave cutoff was 30 Ry, and the k-point
  meshes were 2 × 3 × 2 and 2 × 2 × 1 respectively, in accordance with
  the shapes of the unit cells</p>
</blockquote>

Best Answer: <p>Unlike non-periodical DFT codes, Quantum ESPRESSO uses planewave basis sets and pseudopotentials. There are many ways of writing your input file, and it all depends on <em>what</em> information you have or <em>what</em> you want to learn. </p>

<p>Here is the skeleton of your input file: </p>

<pre><code>===============================================================================

&amp;CONTROL
  ...
/

&amp;SYSTEM
  ...
/

&amp;ELECTRONS
  ...
/

[ &amp;IONS
  ...
 / ]

[ &amp;CELL
  ...
 / ]

ATOMIC_SPECIES
 X  Mass_X  PseudoPot_X
 Y  Mass_Y  PseudoPot_Y
 Z  Mass_Z  PseudoPot_Z

ATOMIC_POSITIONS { alat | bohr | crystal | angstrom | crystal_sg }
  X 0.0  0.0  0.0  {if_pos(1) if_pos(2) if_pos(3)}
  Y 0.5  0.0  0.0
  Z O.0  0.2  0.2

K_POINTS { tpiba | automatic | crystal | gamma | tpiba_b | crystal_b | tpiba_c | crystal_c }
if (gamma)
   nothing to read
if (automatic)
   nk1, nk2, nk3, k1, k2, k3
if (not automatic)
   nks
   xk_x, xk_y, xk_z,  wk
if (tpipa_b or crystal_b in a 'bands' calculation) see Doc/brillouin_zones.pdf

[ CELL_PARAMETERS { alat | bohr | angstrom }
   v1(1) v1(2) v1(3)
   v2(1) v2(2) v2(3)
   v3(1) v3(2) v3(3) ]

[ OCCUPATIONS
   f_inp1(1)  f_inp1(2)  f_inp1(3) ... f_inp1(10)
   f_inp1(11) f_inp1(12) ... f_inp1(nbnd)
 [ f_inp2(1)  f_inp2(2)  f_inp2(3) ... f_inp2(10)
   f_inp2(11) f_inp2(12) ... f_inp2(nbnd) ] ]

[ CONSTRAINTS
   nconstr  { constr_tol }
   constr_type(.)   constr(1,.)   constr(2,.) [ constr(3,.)   constr(4,.) ] { constr_target(.) } ]

[ ATOMIC_FORCES
   label_1 Fx(1) Fy(1) Fz(1)
   .....
   label_n Fx(n) Fy(n) Fz(n) ]
</code></pre>

<p>To start off, you will be "filling in the blanks" with the information found <a href="https://www.quantum-espresso.org/Doc/INPUT_PW.html" rel="noreferrer">here</a>.</p>

<p>For example, select "relax" to relax atomic positions within the unit cell. You will be able to define your structure by space group no., lattice constants <span class="math-container">$a,b,c$</span>, or by the primitive vectors in <code>&amp;CELL_PARAMETERS</code>.</p>

<p>The <code>&amp;SYSTEM</code> card is the most important. This is where you will define your cell dimensions, your energy cutoff, any corrections such as <code>vdw_corr</code> (e.g. Grimme-D2,D3) and you can call hybrid functionals <code>via input_dft</code>. </p>

<p>Next, you will need to get your pseudopotentials. The Quantum ESPRESSO website has a library of pseudopotentials developed by Andrea Dal Corso. These are the default, plug-and-play pseudopotentials you can use instead of generating your own. </p>

<p>Here are three places where you can get good pseudopotentials:
The PSLIBRARY by A. Dal Corso is <a href="https://dalcorso.github.io/pslibrary/" rel="noreferrer">here</a>. There you will find Ultrasoft and PAW pseudopotentials.</p>

<p>The Standard Solid State Pseudopotential library in MaterialsCloud is <a href="https://www.materialscloud.org/discover/sssp/table/efficiency" rel="noreferrer">here</a>.</p>

<p>The <a href="http://www.pseudo-dojo.org/" rel="noreferrer">Pseudo Dojo</a> is also gaining popularity. There, you might be limited for now, to Norm-conserving or PAW pseudopotentials depending on the species.</p>

<p>Everything else is relatively straight forward and easy to work with. Once you define your system. </p>

<p>In the example you provide, they used</p>

<p><code>K_POINTS automatic 
2 3 2 0 0 0</code> </p>

<p>to generate that first mesh. This is very loose. You will need a much denser grid to study properties, but this should be good enough to get you started! Hope this helps. To visualize your system directly from your input, many use <a href="http://www.xcrysden.org/" rel="noreferrer">XCrysDen</a>.</p>


================================================================================

Question: When do relativistic effects need to be explicitly included?
Body: <p>For many applications with heavy metals, pseudopotentials can be used to include some amount of relativistic effects. But for what sort of systems does it become necessary to actually use a relativistic method (e.g Dirac Hartree Fock)? Is this there active research into material modeling with a relativistic formalism?</p>

Best Answer: <p>When to include relativistic corrections or modeling of any kind in computational methods is a rather complex one. Full Dirac methods as you asked about (DHF) recapture two important factors, so called scalar relativistic effects, and spin effects. I'll elaborate on each and when including them is important.</p>

<p><strong>Scalar Relativistic Effects</strong> - This largely describes the so called "mass/velocity" relationship. You may be familiar that as a massive object moves with great velocity it gains mass (so that its velocity can never truly reach or surpass the speed of light). In the case of the electron, its effective velocity is a function of <span class="math-container">$Z_\text{eff}$</span> (in atomic units <span class="math-container">$Z_\text{eff}/c \approx v$</span> or for say hydrogen <span class="math-container">$1/137 = 0.007c$</span>). This is why psuedo-potentials can easily approximate  this effect, generally the largest <span class="math-container">$Z_\text{eff}$</span> is for electrons near the core (1s 2s 2p etc.) but valence electrons experience only a small percentage of the <span class="math-container">$Z_\text{eff}$</span> due to screening. The caveat is you may realize is that d and f orbitals experience much less screening, but are valence orbitals for the transition metals, lanthanides, and actinides. For lighter transition metals the effect is still minor but by Actinides especially, valence electrons can be moving <span class="math-container">$0.3-0.5c$</span>. However, once your valence becomes a p or s again, you no longer have such a significant valence effect. Thus, lead can be fine with a pseudo-potential, where uranium would not be.</p>

<p><strong>TL;DR : If you're worried about scalar relativistic effects you're looking at lanthanides and actinides</strong></p>

<p><strong>Spin effects</strong> - (this relates to the Pauli principle and electrons having spin<span class="math-container">$= \pm 1/2$</span>). This is a much more complicated issue. Since spin effects are proportional to total atomic spin (or molecular) it depends on the charge and occupancy of your species. For example, molecular oxygen (ground state triplet) has very large spin effects (on the order of 0.5-1.0 kcal/mol), yet molecular nitrogen has almost none. Conversely, Ni(II) (ground state triplet) has abnormally small spin effects, even though it has a similar total spin, the considerations of the orbitals themselves also plays a role. Here is where computational chemists generally start to use (the often reviled) heuristic considerations. In the case of worrisome spin effects, you should either rely on experimental evidence that they are important or otherwise do extensive research to determine if they might play a role. </p>

<p><strong>TL;DR: Spin effects are a guessing game, but if this is your area of research, endeavor to be an expert on which things have large spin effects in general.</strong></p>

<p>Finally, materials modeling with relativity! The main consideration here is that DHF and other relativistic methods are QM methods with worse (roughly squared of the base scaling of a non-relativistic method, e.g. DHF is <span class="math-container">$O(N^{4-6})$</span> since HF is <span class="math-container">$O(N^{2-3})$</span> on most computers, though a book might tell you HF is formally <span class="math-container">$O(N^4)$</span>, with computational tricks it's cheaper than that) scaling. Since materials modeling with QM methods is (to my knowledge) still in its adolescence, this makes full or even partial relativistic materials modeling more or less cutting edge science.</p>

<p><strong>TL;DR : Send me a copy of your publication if you manage to figure out relativistic materials modeling on a non-super computer.</strong>     </p>


================================================================================

Question: What is the largest system that has been studied by the coupled cluster method?
Body: <p>This is parallel to the analogous question about the largest DFT calculation: <a href="https://materials.stackexchange.com/questions/166/what-is-the-largest-material-that-has-been-studied-using-density-functional-theo">What is the largest material that has been studied using density functional theory?</a></p>

<p>I assume we allow high performance computing (e.g local correlation, sparsity, parallelization, etc).</p>

Best Answer: <h1>ORCA</h1>
<p>The local pair-natural orbital (DLPNO) based coupled cluster method have been managed to investigate large organic molecule and small proteins (linear C150H302 (452 atoms, &gt;8800 basis functions) , Crambin with 644 atoms, and more than 6400 basis functions ,C350H902 (&gt;1000 atoms, &gt; 20000 basis functions)).</p>
<p>Riplinger, C., Sandhoefer, B., Hansen, A., &amp; Neese, F. (2013). Natural triple excitations in local coupled cluster calculations with pair natural orbitals. The Journal of chemical physics, 139(13), 134101.</p>
<p>Riplinger, C., Pinski, P., Becker, U., Valeev, E. F., &amp; Neese, F. (2016). Sparse maps—A systematic infrastructure for reduced-scaling electronic structure methods. II. Linear scaling domain based pair natural orbital coupled cluster theory. The Journal of chemical physics, 144(2), 024109.</p>
<p>In recent years, considerable amount of effort has been made on developing linear scaling coupled cluster methods. These methods have been implemented in the electronic structure package ORCA which is developed by Max Planck Institute, Germany.</p>


================================================================================

Question: Enforce a space group on a given crystal structure
Body: <p>If I have a CIF that is P1, is there a (preferably free) code that can map the structure to some user-specified space group? I know that there are ways to do similar things with molecular systems. For instance, <a href="http://www.chemcraftprog.com/" rel="noreferrer">ChemCraft</a> has an option to detect the nearest point group and then apply it to the molecule. I'd like to do the same for a crystalline structure given a space group.</p>

Best Answer: <p>This can be done using <a href="https://pymol.org/2/" rel="noreferrer">PyMOL</a> (which, while not free for general use, does have a free student license) via the <code>set_symmetry</code> argument, as documented <a href="https://pymolwiki.org/index.php/Set_Symmetry" rel="noreferrer">here</a>. This allows for a user-specified spacegroup and will update the CIF to match.</p>


================================================================================

Question: In DFT, while studying a system using different exchange-correlation functional approximations, are we expected to check convergence for each method?
Body: <p>As pointed out in a <a href="https://materials.stackexchange.com/questions/316/in-dft-if-i-complete-a-convergence-test-once-then-want-to-study-a-different-sy">previous question</a> each time you start a new DFT calculation, it is recommended to do a convergence test. If I am studying a system using different exchange correlation functional approximations like GGA, meta-GGA, hybrid, DFT+U, should I explicitly check for convergence for each method?</p>

<p>How do people approach this in real life calculations? If I am doing a calculation, am I expected to check convergence for each method? Alternatively, if I find a similar work in literature, is it understood that the authors have checked for convergence explicitly for every method?</p>

Best Answer: <p>The short answer is "yes" you should check, however this is not as arduous as it might appear at first. Your initial convergence calculations will be fairly exhaustive, sweeping over a large range of basis set size, Brillouin zone sampling etc. but when you change exchange-correlation functional you only need to check that your previously determined parameters were sufficient.</p>

<p>As an example, if I were to use a plane-wave pseudopotential DFT program I might start with LDA and converge the plane-wave cut-off by looking at cut-off energies of 300 eV, 400 eV, 500 eV... 1200 eV, and perhaps decide to use 600 eV. If I then change to PBE, I would start at 600 eV and check that this is reasonable by either comparing dE_total/dlog(E_cutoff) with the LDA one, or looking at 700 eV and perhaps 800 eV (comparing the trend to the corresponding trend with LDA). This is much quicker and easier than the original convergence check, since we know roughly what we expect the answer to be and how everything should vary.</p>

<p>The natural follow-on question is <em>why</em> the convergence might depend on the choice of exchange-correlation functional. The principal answer to this is the self-interaction error: the Hartree term in the Kohn-Sham equations is the Coulomb repulsion between the  electron density at a point and the electron density at another point, but at each of these points some of this electron density was due to the same particle, which means that a particle repels itself! This is called self-interaction, and is completely incorrect. The "true" exchange-correlation functional would remove this spurious self-interaction but, since we don't have this wondrous functional at present, we must make do with approximations to it. These approximations vary considerably as to how much they mitigate the effect of self-interaction.</p>

<p>If we focus on the two main considerations of basis set size and Brillouin zone/k-point sampling (for periodic systems) then we can see what difference we expect from the self-interaction error.</p>

<p>The appropriate basis set size usually depends primarily on the states near the nuclei. A large self-interaction leads to spurious delocalisation of states with high electron density, particularly "shallow core" states such as atomic d- and f-states. This can change the size of basis needed to describe the states accurately. If you are using pseudopotentials, then there is an additional effect: when you change the functional you also change the pseudopotential, and this may require more basis sets. This is particularly true in my experience if you're using functionals with some Fock exchange, as the pseudopotential generation in this case is rather difficult and the resultant potentials tend to be rather hard.</p>

<p>The Brillouin zone sampling depends on the bonding in your system so you may not expect this to be strongly dependent on the exchange-correlation functional, but this is not always true. One of the clearest examples might be transition metal oxides, e.g. NiO, which is strongly affected by the self-interaction error in the Ni d-states. LDA predicts that NiO is a non-magnetic metal (due to self-interaction error), so an exhaustive convergence check would conclude you need a high k-point sampling, but do not need to worry about spin (collinear or non-collinear); however, changing to PBE removes just enough of the self-interaction to open up a small band gap and make NiO an antiferromagnetic insulator, meaning that you do not need as high a k-point sampling but you do need to consider the spin density (at least at the collinear level). The antiferromagnetic insulating state would also be obtained with a modest Hubbard U, even with LDA, on the Ni d-states, and/or a proportion of Fock or screened-Fock exchange (which removes this self-interaction directly).</p>


================================================================================

Question: Difficult cases for converging Kohn-Sham SCF calculations
Body: <p>In a recent article by Woods <em>et. al.</em><sup>[1]</sup> a couple of methods for converging ground-state Kohn-Sham DFT calculations are reviewed and compared. In their test suite they give plenty of examples for badly convergent cases for self-consistent field calculations. Taking a look at their examples it seems that mainly isolated atoms, large cells, slabs and unusual spin systems that cause trouble with convergence.</p>

<p>I am wondering what the experience of the community are with respect to cases where standard approaches (e.g. Kerker mixing, some temperature) fail. Did you encounter systems where you had a tough time? What type / class of system were they? What are common approaches to circumvent the problem or improve convergence?</p>

<p>My interest mainly concerns with <em>numerical</em> convergence problems in the self-consistent field procedure itself. So when one would expect Kohn-Sham DFT to be a sensible model for a particular system, making the SCF problem well-posed, but still the SCF fails to converge (like e.g. the slab of Gold they mention in the cited paper).</p>

<h3>References:</h3>

<ol>
<li>Woods, N. D.; Payne, M. C.; Hasnip, P. J. Computing the self-consistent field in Kohn–Sham density functional theory. <em>J. Phys.: Condens. Matter</em> <strong>2019,</strong> <em>31</em> (45), 453001. <a href="https://doi.org/10.1088/1361-648X/ab31c0" rel="noreferrer">DOI: 10.1088/1361-648X/ab31c0</a>.</li>
</ol>

Best Answer: <p>A few materials/simulation boxes I've had some proper trouble with:</p>

<ol>
<li><p>HSE06 + noncollinear magnetism + antiferromagnetism, Vasp noncollinear:</p>

<ul>
<li>This was a strongly antiferro material (4 Fe atoms, in an up-down-up-down configuration).</li>
<li>HSE06 is apparently difficult to converge anyway.</li>
<li>Noncollinear magnetism/antiferromagnetism apparently creates problems for any charge density/spin density mixer (or so I've been told/seen). But this was quite bad.</li>
<li>Solved with: AMIX = 0.01, BMIX=1e-5, AMIX_MAG=0.01, BMIX_MAG=1e-5, Methfessel-Paxton order 1 smearing of 0.2 eV. Davidson solver (ALGO=Fast). It took ~160 SCF steps, but it <em>did</em> converge.</li>
</ul></li>
<li><p>Cell with really different a, b, c, GPAW, PW mode:</p>

<ul>
<li>This was a much simpler, spin-paired, metallic system. However, the
cell was 5.8 x 5.0 x ~70 angstroms.</li>
<li>This happens in general when a cell is really elongated along a particular axis/in general has a very 'non-cubic' shape.</li>
<li><a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.64.121101" rel="noreferrer">This paper</a>, which I believe is the reference for Quantum Espresso's 'local-TF' charge-density mixing explains (I think) why this happens. Turns out that really large lattice vector (or in general a very 'non-cubic' cell) ill-conditions the charge-mixing problem. This is the precise problem that is addressed in the paper.</li>
<li>Since GPAW does not implement the 'local-TF' mixing yet, I solved this by using <code>mixer=Mixer(beta=0.01)</code>. The convergence is very slow, but again, it <em>did</em> converge.</li>
</ul></li>
<li><p>A colleague and I discussed the convergence of a (possibly antiferromagnetic) nickel compound (that's about as much I know), Vasp. I don't have many details on this other than it was a similar pain as case #1 above. It took a lot of work and mostly just turning down AMIX and AMIX_MAG.</p></li>
<li><p>I recently saw another person attempting to converge a single Ni atom in a box (GPAW, LCAO mode), with magnetic moment set by Hund's rule, and I think the thing actually converged density to a log10-error of &lt; -2.4 (you want to aim for &lt; -4.0). To converge further, they proceeded with a Fermi-Dirac smearing of... 0.5 eV. This was one of the more extreme cases I've seen.</p></li>
</ol>

<p>I will keep updating this answer as I see/find more pathological cases, but these are what I have so far.</p>


================================================================================

Question: What functions other than Gaussians are used for orbital basis sets?
Body: <p>The Gaussian function <span class="math-container">$\propto\exp((x-a)^2/b)$</span> with <span class="math-container">$b&gt;0$</span> is one of the most common functions used in molecular modelling (e.g. Gaussian type orbitals).</p>

<p>What are some examples of applications of functions (in the literature) used other than the Gaussian in molecular modelling, and are there explanations as to why those are preferred over the Gaussian?</p>

Best Answer: <p>There are two considerations guiding the choice a basis for orbital expansion:
1. Compactness;
2. Efficiency of computations.</p>

<p>There are two common choices for basis functions (A) <strong>Gaussians</strong> and (B) <strong>Plane waves</strong>.
Both of those allow for the most efficient way to evaluate the integrals needed to construct the Fock matrix (i.e. second derivative for kinetic energy, nuclear-electron attraction and electron-electron repulsion)  - <strong>analytically</strong>.</p>

<p>(A) Gaussians is by far the most popular set for <strong>localized</strong> systems (e.g. molecules). The reason is because Gaussian functions are themselves localized and thus they can be used to compactly represent electron density localized around nuclei (i.e. you don't need too many Gaussian functions to do this). Plane waves resolve all of the simulation volume with equal accuracy, so a large part of the computational effort is wasted on (nearly) empty space.
(B) Plane waves are much more popular for computations on <strong>periodic</strong> (condensed matter) systems (e.g. crystal structures). They enable even more efficient computations, since (unlike Gaussians) they are <strong>orthonormal</strong> and so there is no need to compute the overlap matrix S.</p>

<p>In practice, the sharp features (like the cusp near the nucleus) are still tough for the plane waves. This problem is circumvented by using the pseudopotentials, which smooth out the sharp features.</p>

<p>Another good reason to use either Gaussian or plane wave basis sets is that a lot of work has been done for you to develop really good Gaussian contractions and pseudopotentials. For a typical system there is no need to reinvent the wheel. That said, better basis sets are sought after and situationally (e.g. for a strange system) a different basis set could be much better.</p>


================================================================================

Question: Simulating breaking bonds in molecular dynamics
Body: <p>How does one introduce the possibility of breaking a bond in a molecule during MD simulation?</p>

<p>I only found the cases when we just introduce the harmonic potential <span class="math-container">$U_{ij} = \frac{1}{2}k(r_i - r_j)^2$</span> for atoms in a molecule or introduce an additional equation restricting the length of a bond during simulaton.</p>

<p>Nevertheless, it is still not clear for me how we can take into account the breaking bond result?</p>

Best Answer: <p>The quadratic potential is the simplest possible model for a bond. You can derive it by considering the Taylor expansion of the potential around the natural bond length</p>

<p><span class="math-container">$V(r - r_0) = V(r_0) + \frac{d V(r_0)}{d r} (r - r_0) + \frac{1}{2} \frac{d^2 V}{dr^2} (r - r_0)^2$</span></p>

<p>The constant term can be set to 0 since it does not contribute to the force and just sets the 0 point of your energy scale. The linear term is 0, because you are at a stationary point [1].</p>

<p>That leaves you with
<span class="math-container">$V(r - r_0) = \frac{1}{2} \frac{d^2 V}{dr^2} (r - r_0)^2$</span></p>

<p>If we require the second derivative to be constant, we have recovered the harmonic potential.
As @Charlie Crown illustrated, this force resulting from this potential does not go to 0 at infinity, while the Morse potential does. You can of course take a polynomial of higher order than two, but not every order is suitable. A third order polynomial results in a potential that (typically) goes to negative infinity at large <span class="math-container">$r$</span>, so instead a quartic potential is some times used. It has the advantage of being slightly "wider" than the quadratic one. That said, a completely unrelated reason why none of these can simulate bond breaking/formation is that the implementation requires explicit declaration of which atoms should interact via the stretching potential.</p>

<p>Still, at large <span class="math-container">$r$</span> both differ significantly from the Morse potential. <em>Why then is the Morse potential not used?</em>
The restoring force for large <span class="math-container">$r$</span> is very low in case of the Morse potential, hence it takes longer for the bond length to return to the equilibrium position. The quadratic potential describes the potential well for displacements close to equilibrium and for moderate temperatures, this is the part of the potential you care about.</p>

<p>Obviously that still leaves the question of how to simulate bond breaking in a force field. ReaxFF assumes that the bond order of a pair of atoms can be determined from the interatomic distance alone. </p>

<p><a href="https://i.sstatic.net/oJQ60.png" rel="noreferrer"><img src="https://i.sstatic.net/oJQ60.png" alt="Bond order C-C distance dependence"></a></p>

<p>(qualitative recreation from [2])</p>

<p>The sigma, pi and double pi bonds contribute increasingly to the overall bond order (max individual bond order is 1) as the atoms get closer together. For simplicity I am leaving out the corrections made to the overall bond order necessitated by overcoordination.
The bond stretching potential takes the form of a modified Morse potential</p>

<p><span class="math-container">$E_{Bond} = -D_e \cdot BO_{ij} \cdot \exp(p \cdot (1 - BO_{ij}^p))$</span></p>

<p>where <span class="math-container">$p$</span> is a bond specific parameter[2].</p>

<p>References:</p>

<p>[1]: Frank Jensen, <em>Introduction to Computational Chemistry</em> Chap. 2</p>

<p>[2]: J. Phys. Chem. A 2001, 105, 9396-9409</p>


================================================================================

Question: Which basis set families were optimized to give the lowest possible variational energy for a given number of orbitals?
Body: <p>Dunning's basis set families (such as cc-pV<strong>X</strong>Z and aug-cc-pCV<strong>X</strong>Z) were not optimized to give the lowest variational energy for a given number of orbitals. For example the S-type orbitals are optimized, then held fixed while P-type orbitals are optimized, then the S-type and P-type orbitals are both held fixed while the D-type orbitals are optimized, and so on. Likewise when the "tight" functions of the CV correction are optimized to create an cc-pCV<strong>X</strong>Z basis set, the exponents of the cc-pV<strong>X</strong>Z basis set are held fixed. <strong>When holding some parameters fixed during the optimization, one will not get the absolute optimal outcome possible for a given number of parameters.</strong> </p>

<p>Furthermore, the Dunning basis set sequence is constructed in such a way that when increasing the <strong>X</strong> in cc-pV<strong>X</strong>Z by one, a fixed number of new orbitals are added: for example when going from <strong>X</strong>=3 to <strong>X</strong>=4 (TZ to QZ) for some second row atom, there will be one new G-type exponent, one new F-type exponent, one new D-type exponent, and so on; <em>but</em> it's possible that a lower variational energy would be obtained with the same number of orbitals if we were to avoid adding the G-type function and instead invest in more P-type functions. <strong>This is because the goal of Dunning's basis set sequences, is to construct them in a <em>systematic</em> manner so that there will be a smooth extrapolation to the CBS (complete basis set) limit for some actual property (e.g. energy difference),</strong> whether or not a lower total energy for a particular X value could be obtained by adding more exponents for one type of orbital than another.</p>

<p><strong><em>Which basis set families are constructed simply to get the lowest variational energy for a given number of orbitals, regardless of the ability to extrapolate well or the ability to optimize the exponents in a systematic way?</em></strong> I appreciate that this: </p>

<ul>
<li>will be <em>much</em> more costly than holding some exponents fixed while optimizing others, and that</li>
<li>total energies might be improved at the expense of actual properties (like ionization energies or atomization energies) being worsened, and that</li>
<li>extrapolations to the CBS limit will not be as smooth,</li>
</ul>

<p>but I'm curious to know what exists!</p>

<p><em>For the purposes of this question, I am interested in one-particle basis set families, so not ECGs (explicitly correlated Gaussians) for example. You may choose to answer by giving just one basis set family in one answer, or by giving all examples you know in one answer.</em></p>

Best Answer: <p>I think there are none. If you look at e.g. the work by Dunning, the problem is that the functions appear in a semi-random order: e.g. the second D function may be energetically more important than the first F function, so you go from 1D -&gt; 2D -&gt; 2D1F -&gt; etc, but it can also be less important, and then you would go 1D -&gt; 1D1F -&gt; 2D1F, etc. This is something I've seen first hand when trying to form property-optimized basis sets automatically; you can get pretty weird-looking results from such a procedure. (By 1D -&gt; 2D -&gt; 2D1F, I mean having one d-orbital, then two d-orbitals, then two d-orbitals and one f-orbital, in the basis set sequence).</p>
<p>The solution of picking the functions in groups like 1D, 2D1F, 3D2F1G, etc is elegant in that you get a more or less consistent basis in any case, since you collect all contributions of the same order. Petersson's nZaP basis sets that are designed for constant error in the energy per electron across the periodic table also use a similar composition, since they're likewise designed for post-HF calculations. In contrast, Jensen's sets have a different composure since the polarization effects behave differently to the correlation effects; for the pc sets the composition is more like 1D -&gt; 2D1F -&gt; 4D2F1G etc.</p>
<p>As to the part that deals with the exponents: Dunning-style sets are designed to be additive, so the exponents are optimized in steps - with the exception of the cc-pwCVXZ sets for transition metals where the exponents would overlap too much and so the exponents are relaxed. In contrast, Petersson's nZaP sets and the polarization-consistent Jensen basis sets use optimized primitives, although uses Hartree-Fock contracted orbitals and adds fresh primitives to describe the polarization and correlation effects, so it's not fully optimized at post-HF level of theory.</p>


================================================================================

Question: Calculating HSE06 band structures on Quantum ESPRESSO
Body: <p>I am currently trying to figure out how to compute band structures for my system, using the hybrid functional HSE06. I'm doing this on Quantum Espresso. As I understand, there are a handful of ways to do this:</p>

<p>1) Generate maximally localized wannier functions (MLWFs) with the HSE functional turned on</p>

<p>2) The 'fake scf' procedure: This is similar to what is listed in Vaspwiki(<a href="https://www.vasp.at/wiki/index.php/Si_HSE_bandstructure" rel="noreferrer">here)</a>.</p>

<p>3) Generate scf data for a coarse 'q' mesh (of the Fock operator) and then interpolate these bands using softwares like BoltztraP2.</p>

<p>I am not familiar with the Wannier module on Quantum ESPRESSO, hence option (1) is sort of last resort for me, since Wannier takes some time to learn.</p>

<p>The 'fake scf' procedure doesn't seem to work. The k-point mesh for my original calculation (my system is a bilayer TMD - Transition metal dichalcogenide) is 12x9x1. I tried using a commensurate 'q' grid and tried the 'fake scf' procedure, but I couldn't get it to work.</p>

<p><strong>I would be grateful if anyone could help me figure out how to compute these bandstructures - either through one of the methods I mentioned, or any easy method as you see fit.</strong> I would like to add that I have access to considerable computational resources, so doing expensive calculations should not be much of an issue. </p>

<p>Also, I would like to say that I am open to methods that use other softwares - including VASP or CASTEP. I have both licenses (CASTEP through Materials Studio). But things can get a little tricky here because VASP for example, uses a different implementation of the Hubbard 'U' and it would corrupt my results if I port my QE input to VASP to calculate the band-structures.</p>

Best Answer: <p>I have seen all the methods you mentioned but have only done one myself; I'll explain here how to use Wannier90 in conjunction with Quantum Espresso to get band structures for hybrid functional calculations. It does take a bit of time to learn, but not very long! You can learn the basics and do some first calculations in an afternoon. There are subtleties of course that I won't get into, and if you have specific issues you should probably ask another question or send a message to the Wannier90 mailing list.</p>

<h2>Initial Checklist</h2>

<ol>
<li><p>Compile and install <a href="http://www.wannier.org/" rel="noreferrer">Wannier90</a></p></li>
<li><p>If you are using QE &lt;= 6.0, you will need to replace the qe/PP/src/pw2wannier90.f90 file with one provided in the Wannier90 tarball, and recompile QE. Otherwise, make sure you have a working compiled version of QE. Newer versions will typically be better for hybrid functional calculations (pairwise band parallelization, ACE algorithm, support for ultrasoft &amp; PAW...)</p></li>
<li><p>This method requires commensurate k-point grids between QE and Wannier90. Certain  k-point meshes may cause issues in Wannier90 when identifiying nearest neighbors, etc.  It's recommended that you prepare a manual k-point grid with the utility located in wannier90/utility/kmesh.pl and use it in your QE calculation. Wannier90 does not recognize k-point symmetry, so it is a good idea to disable symmetry in your hybrid calculation with <code>nosym = .true</code> and <code>noinv = .true.</code> in the <code>&amp;SYSTEM</code> block, along with the aforementioned manual k-points at the highest density you can afford, with corresponding q-point grid (as an aside, this type of grid is always needed for Wannier90, but normally you use this full grid with an nscf calculation after your automatic grid scf calculation when not using hybrid functionals, to reduce computational cost). I will leave it to you to determine what you need in terms of k- and q-point densities for your specific calculation.</p></li>
<li><p>The input files you need at the beginning are: DFT input for QE, Wannier90 input file (.win), and a pw2wan input file. Here are some I used for a very quick (probably unconverged) calculation I did on my desktop, just using bulk silicon. In the interest of time I used automatic projections with the SCDM method and default values, you should read up on this and decide how you should proceed in your own calculation (manual projections require the <code>begin projections</code> block in Wannier90 input--read the user guide for more info). It can affect the band structure significantly.</p></li>
</ol>

<p>QE input file: silicon.in</p>

<pre><code> &amp;control
    calculation = 'scf'
    restart_mode='from_scratch',
    prefix='Si-HSE',
    pseudo_dir = './',
    outdir='./TMP_DIR/'
 /
 &amp;system    
    ibrav=  2, celldm(1) =10.20, nat=  2, ntyp= 1,
    ecutwfc =30.0,  nbnd = 8,
    input_dft='hse', nqx1 = 1, nqx2 = 1, nqx3 = 1, 
    x_gamma_extrapolation = .true.,
    exxdiv_treatment = 'gygi-baldereschi',
    nosym = .true., noinv = .true
 /
 &amp;electrons
    mixing_beta = 0.7
 /
ATOMIC_SPECIES
 Si  28.086  Si_ONCV_PBE-1.1.upf
ATOMIC_POSITIONS alat
 Si 0.00 0.00 0.00 
 Si 0.25 0.25 0.25 
K_POINTS crystal
8
  0.00000000  0.00000000  0.00000000  1.250000e-01
  0.00000000  0.00000000  0.50000000  1.250000e-01
  0.00000000  0.50000000  0.00000000  1.250000e-01
  0.00000000  0.50000000  0.50000000  1.250000e-01
  0.50000000  0.00000000  0.00000000  1.250000e-01
  0.50000000  0.00000000  0.50000000  1.250000e-01
  0.50000000  0.50000000  0.00000000  1.250000e-01
  0.50000000  0.50000000  0.50000000  1.250000e-01
</code></pre>

<p>Wannier90 input file: silicon.win</p>

<pre><code>! Silicon HSE 1
 num_wann    = 8
 num_bands   = 8
 num_iter    = 20
 kmesh_tol   = 0.0000001
 auto_projections = .true.

! Use as much precision as you can (at least 6 decimals) to prevent issues with matching to QE output
Begin Unit_Cell_Cart
-2.698804 0.0000 2.698804
 0.0000 2.698804 2.698804
-2.698804 2.698804 0.0000
End Unit_Cell_Cart

begin atoms_frac
Si 0.00  0.00  0.00
Si 0.25  0.25  0.25
end atoms_frac

!begin projections
!Si:sp3
!end projections

! To plot the WF interpolated bandstructure
bands_plot       = .true.
bands_num_points = 200

begin kpoint_path
L 0.50000  0.50000 0.5000 G 0.00000  0.00000 0.0000
G 0.00000  0.00000 0.0000 X 0.50000  0.00000 0.5000
X 0.50000 -0.50000 0.0000 K 0.37500 -0.37500 0.0000
K 0.37500 -0.37500 0.0000 G 0.00000  0.00000 0.0000
end kpoint_path

! KPOINTS

mp_grid : 2 2 2

begin kpoints
0.0 0.0 0.0
0.0 0.0 0.5
0.0 0.5 0.0
0.0 0.5 0.5
0.5 0.0 0.0
0.5 0.0 0.5
0.5 0.5 0.0
0.5 0.5 0.5
end kpoints
</code></pre>

<p>pw2wannier90 input file: silicon.pw2wan</p>

<pre><code>&amp;INPUTPP
 prefix = 'Si-HSE'
 outdir = './TMP_DIR'
 seedname = 'silicon'
 scdm_proj = .true.
 scdm_entanglement = 'isolated'
 scdm_mu = 0.0
 scdm_sigma = 1.0
/
</code></pre>

<h2>Calculation Procedure</h2>

<ol>
<li>Run Wannier90 to generate some needed files, with the postprocessing option: <code>$ wannier90 -pp silicon</code></li>
<li>Run your QE hybrid functional calculation (obviously use the correct mpi commands for your system when running in parallel): <code>$ pw.x -inp silicon.in &gt; silicon.out</code></li>
<li>Run pw2wannier: <code>$ pathtoqe/PP/src/pw2wannier.x -inp silicon.pw2wan</code></li>
<li>Run Wannier90 again to do the calculation and get band structure: <code>$ wannier90 silicon</code></li>
</ol>

<p>Doing the above I got this band structure. It obviously has some problems (some bands jumping up above the VBM) but the gap is larger than PBE and I did this with a very sparse grid, and no optimization of any options.</p>

<p><a href="https://i.sstatic.net/AlgQ1.jpg" rel="noreferrer"><img src="https://i.sstatic.net/AlgQ1.jpg" alt="silicon band structure"></a></p>


================================================================================

Question: Derivation of Slater-Koster equations
Body: <p>I am trying to derive the Slater-Koster equations (Table. 1 of Ref. 1) for the two-centre approximation of hopping integrals between atomic orbitals. I understand that Slater-Koster approximates the two centre hopping integral as:</p>
<p><span class="math-container">\begin{equation}
E_{n,m} = \int{\psi^*_n (\textbf{r}-\textbf{R})H\psi_m (\textbf{r}) dV},\tag{1}
\end{equation}</span></p>
<p>assuming that potential, <span class="math-container">$V$</span> is spherically symmetric.</p>
<p>I tried substituting spherical harmonics, <span class="math-container">$Y_{lm}$</span> as the atomic orbitals <span class="math-container">$\psi (\textbf{r})$</span> but I am struggling to get to the result. First, I am not sure how to write the spherical harmonics <span class="math-container">$Y_{lm}$</span> not centred at the origin. Then, I can't see how the integral would separate into two parts representing pi and sigma bonds.</p>
<p>Does anyone know how to derive one of the Slater-Koster equations? For example:</p>
<p><span class="math-container">\begin{equation}
E_{p_z, d_{x^2-y^2}} = \frac{1}{2}\sqrt{3}n(l^2-m^2) V_{pd\sigma} -n(l^2-m^2) V_{pd\pi},\tag{2}
\end{equation}</span></p>
<p>where <span class="math-container">$l,m$</span> and <span class="math-container">$n$</span> are directional cosines of connecting vector <span class="math-container">$\textbf{R}$</span> (i.e. <span class="math-container">$l=\sin{\beta}\cos{\alpha}=\frac{R_x}{R}$</span>).</p>
<p>References</p>
<ol>
<li>Slater, JC &amp; Koster, GF, &quot;Simplified LCAO Method for the Periodic Potential Problem&quot; Physical Review (1954); link: <a href="https://journals.aps.org/pr/abstract/10.1103/PhysRev.94.1498" rel="noreferrer">https://journals.aps.org/pr/abstract/10.1103/PhysRev.94.1498</a></li>
</ol>

Best Answer: <p>Spherical harmonics are not themselves full atomic orbitals. Consider the <a href="https://en.wikipedia.org/wiki/Hydrogen_atom#Wavefunction" rel="noreferrer">Hydrogen wave function</a>, which separates into a radial part and an angular part. The latter is a spherical harmonic, but the former is some other function (in the case of Hydrogen it's a Laguerre polynomial). In general, we can approximate the angular part for other atoms with the same spherical harmonics, but we usually don't  know the radial part analytically. Hence, if you want to directly evaluate Eq. (1) by integration you also need to find the radial part somewhere. The strength of Slater's and Koster's work, however, is that we can avoid this problem entirely, by hiding all radial dependence in the Slater-Koster parameters (in your example they are <span class="math-container">$V_{pd\sigma}$</span> and <span class="math-container">$V_{pd\pi}$</span>).</p>
<p>For simplicity, I'll focus on the case of an <span class="math-container">$s$</span>-<span class="math-container">$p$</span> overlap. This simplifies the geometry, and will let me borrow pictures and notation from <a href="http://www.menet.umn.edu/%7Edtraian/tony-thesis.pdf" rel="noreferrer">this thesis</a>. Let's say we have an <span class="math-container">$s$</span> orbital at site <span class="math-container">$i$</span> with wave function <span class="math-container">$\psi_{is}$</span>, and a <span class="math-container">$p_\alpha$</span> orbital at site <span class="math-container">$j$</span> with wave function <span class="math-container">$\psi_{jp_\alpha}$</span>, where <span class="math-container">$\alpha \in \{x,y,z\}$</span>. In Dirac's bra-ket notation, the overlap between them can be written
<span class="math-container">$$
E_{s,p_\alpha}=\langle \psi_{is}|H_{2c}|\psi_{jp_\alpha}\rangle = \langle S|H_{2c}|P_\alpha\rangle, \tag{3}
$$</span>
where <span class="math-container">$H_{2c}$</span> is the two-center Hamiltonian, and we've introduced short-hand notation for the the two wave functions. Of course, behind the bra-ket notation you have exactly the kind of overlap integral shown in Eq. (1).</p>
<p>Next step is to work out the geometry. Let <span class="math-container">$\vec{r}$</span> be the vector connecting sites <span class="math-container">$i$</span> and <span class="math-container">$j$</span>, and let <span class="math-container">$\vec{d}$</span> be a unit vector along the same direction. We decompose the <span class="math-container">$p$</span> orbital at site <span class="math-container">$j$</span> into components parallel to (<span class="math-container">$\sigma$</span>) and perpendicular to (<span class="math-container">$\pi$</span>) the vector <span class="math-container">$\vec{d}$</span>, as shown in this figure:</p>
<p><a href="https://i.sstatic.net/nJ93C.png" rel="noreferrer"><img src="https://i.sstatic.net/nJ93C.png" alt="S-p atomic orbital overlap decomposition" /></a>
Figure from Anthony Carlson's 2006 MSc thesis at University of Minnesota.</p>
<p>In this notation, <span class="math-container">$\sigma$</span> and <span class="math-container">$\pi$</span> (and also <span class="math-container">$\delta$</span>) is used to denote the component of angular momentum about the axis <span class="math-container">$\vec{d}$</span>. <span class="math-container">$\sigma$</span> means zero, <span class="math-container">$\pi$</span> means <span class="math-container">$1$</span>, etc. To proceed, we also define <span class="math-container">$\vec{a}$</span> as pointing along the <span class="math-container">$p$</span> orbital, and <span class="math-container">$\vec{n}$</span> as a vector perpendicular to <span class="math-container">$\vec{d}$</span> in the plane spanned by <span class="math-container">$\vec{a}$</span> and <span class="math-container">$\vec{d}$</span>. Then, the <span class="math-container">$p$</span> orbital (at site <span class="math-container">$j$</span>) can be decomposed
<span class="math-container">$$
|P_\alpha\rangle = \vec{a}\cdot\vec{d}|P_\sigma\rangle + \vec{a}\cdot\vec{n}|P_\pi\rangle. \tag{4}
$$</span>
Then, the overlap is simply
<span class="math-container">$$
\langle S|H_{2c}|P_\alpha\rangle = \left( \vec{a}\cdot \vec{d} \right) \langle S | H_{2c} |P_\sigma\rangle + \left( \vec{a}\cdot\vec{n} \right) \langle S | H_{2c} | P_\pi\rangle, \tag{5}
$$</span>
where the second term is zero by symmetry. Further, we can easily express this in terms of directional cosines since we can without loss of generality choose <span class="math-container">$\vec{a}$</span> to be parallel to one of the coordinate axes. With
<span class="math-container">$$
d_x=\frac{\vec{r}\cdot\hat{x}}{|\vec{r}|},\quad d_y=\frac{\vec{r}\cdot\hat{y}}{|\vec{r}|},\quad d_z=\frac{\vec{r}\cdot\hat{z}}{|\vec{r}|}, \tag{6}
$$</span>
we get
<span class="math-container">$$
\langle S|H_{2c}|P_\alpha\rangle = d_\alpha \langle S|H_{2c}|P_\alpha\rangle = d_\alpha V_{sp\sigma}, \tag{7}
$$</span>
where all radial overlap is hidden in the parameter <span class="math-container">$V_{sp\sigma}$</span>. In tight-binding models it's common to denote this overlap integral <span class="math-container">$t_{sp\sigma}$</span> if it occurs in a hopping term.</p>
<p>The same approach works for other orbital combinations. You just need to set up the geometry and coordinate systems properly, and know where the atomic orbitals point. (Admittedly this can become quite complicated in some systems, e.g. transition metal oxides.) Then the Slater-Koster parameters can be treated as tuning parameters - either tuned to explore possible phenomena in some system, or fit to reproduce some experiment or calculated band structure.</p>


================================================================================

Question: Is ARM64 the next big thing?
Body: <p>Considering how GPUs in matter modeling was first recognized in gaming devices like PlayStation, it is also interesting to consider whether ARM64 chips, which are the type of chips in modern smart phones, will play a similarly revolutionary role in matter modeling.</p>
<p>Now it is no secret that ARM64 is something that should be considered seriously. For example, <a href="https://www.neowin.net/news/apple-announces-that-its-macs-will-start-using-its-own-custom-arm-processors" rel="noreferrer">Apple's recent announcement</a> that all MACs will come with the ARM64 processor, and the Microsoft Surface X Pro, which runs on ARM64 with a full Windows 10 build. Furthermore, <a href="https://wccftech.com/microsoft-backports-wsl2-to-windows-10-1909-and-1903/#:%7E:text=Microsoft%20said%20that%20if%20you%20are%20using%20an,distributions%20interact%20with%20Windows%20and%20bringing%20several%20improvements." rel="noreferrer">Windows Subsystem for Linux2 (WSL2)</a> is also compatible with ARM64.</p>
<p>The benefits of ARM64 are the &quot;small form factors&quot; which allow it to be more power efficient, smaller, thinner and lighter. Furthermore, they have instant on capability, and long standby times. These are particularly promising from a consumer standpoint but could also be beneficial for high-performance computing as well.</p>
<p><strong>However, the serious drawbacks of ARM64 is the software compatibility.</strong></p>
<p>Notably I found <a href="https://www.olcf.ornl.gov/wp-content/uploads/2019/11/ARM_NVIDIA_APP_EVALUATION-1.pdf" rel="noreferrer">this link</a> which lists ARM + NVIDIA HPC Software that are currently compatible:</p>
<ul>
<li>CoMet</li>
<li>LAMMPS</li>
<li>NAMD</li>
<li>VMD</li>
<li>DCA++</li>
<li>Gromacs</li>
<li>LSMS</li>
</ul>
<p><strong>What other popular matter modeling software are compatible with ARM64 or what software are notoriously not currently compatible?</strong> (e.g. Adobe creative cloud is apparently not yet completely supported by ARM64)</p>


Best Answer: <p><strong>It depends on what chips datacenters are using.</strong> If large data centers switch over to ARM-based processors, scientific computing will follow. Since most of our software is open source (or not far from that), converting to ARM compatibility will not be too much of an issue once the demand is there.</p>
<p>Scientific computing is currently dominated by enterprise-grade Intel-compatible x86_64 processors. The reason for this is not because those are necessarily the best possible processors for scientific computing, but because they are available. Semiconductor manufacturing has massive economies of scale, and scientific computing is a relatively tiny market, so we end up buying whatever high performance chips are used by commercial data centers. <strong>So the real question is this: are commercial data centers going to switch over to ARM chips? Or are they going to stick to x86?</strong></p>
<p>As far as that question goes, I have no idea. For now, I suspect that x86 maintains a performance edge in power-hungry data center processors, but that could change.</p>
<p><strong>Update from Nov 2020:</strong> The initial reviews of the performance of Apple's new ARM-based Macs are very promising, even when they are translating binaries compiled for x86. I think this makes it much more likely that other firms were make big investments in desktop and possibly server-class ARM-based processors.</p>


================================================================================

Question: Examples for results obtained from modeling materials having had a direct impact in discovery of new materials
Body: <p>Materials Modeling helps give theoretical insight into the working of various materials and in turn helps enhance the working of these materials. But are there examples of discovery of new materials which was a direct consequence of modeling of materials? The example(s) may include materials with applications in any field.</p>

Best Answer: <p>A few years ago, I was fascinated by the news of the discovery of a new Carbon allotrope based on first-principle simulations by using USPEX software described <a href="https://uspex-team.org/en/research/materials" rel="noreferrer">here</a>. Or particularly this article:</p>

<p><strong>Zhu Q., Oganov A.R., Salvado M., Pertierra P., Lyakhov A.O. (2011). Denser than diamond:ab initio search for superdense carbon allotropes. Phys. Rev. B83, 193410</strong>, <a href="https://uspex-team.org/static/file/Zhu-carbon-2011.pdf" rel="noreferrer">download here</a>.</p>

<p><strong>Update</strong>:</p>

<p>I promised to elaborate a bit more on my answer here. As I said in my comments after a few years later, this new Carbon allotrope is synthesized in the lab, which is truly fascinating how a theoretically predicted material structure, is synthesized by experimentalists (see here: <a href="https://elementy.ru/novosti_nauki/431105" rel="noreferrer">https://elementy.ru/novosti_nauki/431105</a> and sorry for the Russian language only).</p>

<p>There are several other examples of discovering new materials for specific applications of batteries or fuel cells are provided <a href="https://uspex-team.org/en/research/materials" rel="noreferrer">here</a>, that shows how quantum mechanical simulations combined with machine-learning methods could be used to tailor materials structure for a new application and developing new materials properties based on that.</p>


================================================================================

Question: Are there differences in accuracy and reliability between the frozen phonon method and Density Functional Perturbation Theory?
Body: <p>When proposing or predicting new materials, whether in the bulk or in two dimensions, it is important to discuss the stability of the system. This is usually done through thermodynamics and(or) lattice dynamics. A feasible way to determine the dynamical stability of a material is by modeling its phonon band structure. </p>

<p>I see that two popular methods of modeling the phonon band structure are:</p>

<p>(a) the frozen phonon / finite displacement method (as implemented in <code>phonopy</code>)</p>

<p>(b) Linear Response Theory / Density Functional Perturbation Theory (as implemented in <code>Quantum ESPRESSO</code> (<code>ph.x</code>)</p>

<p>Are the two methods equivalent or is one more reliable/accurate than the other, and if so, why?</p>

Best Answer: <p>Short answer: Modern implementations of these two methods lead to similar accuracies.</p>

<p>Longer answer: The calculation of phonons requires the calculation of the Hessian of the potential energy surface <span class="math-container">$V(\mathbf{R})$</span>, also known as the matrix of force constants:</p>

<p><span class="math-container">$$
\frac{\partial^2 V(\mathbf{R})}{\partial \mathbf{R}_i\partial\mathbf{R}_j}=-\frac{\partial \mathbf{F}_j}{\partial\mathbf{R}_i},
$$</span></p>

<p>where <span class="math-container">$\mathbf{R}$</span> is a collective coordinate of all atomic positions, <span class="math-container">$i$</span> and <span class="math-container">$j$</span> label atoms <span class="math-container">$i$</span> and <span class="math-container">$j$</span> in your system, and <span class="math-container">$\mathbf{F}_j=-\partial V(\mathbf{R})/\partial\mathbf{R}_j$</span> is the force felt when displacing atom <span class="math-container">$j$</span>.</p>

<p><strong>Finite displacement.</strong> This is what you call "frozen phonon", and in this method you calculate the forces in DFT, and then calculate the derivative of the forces by finite difference methods. Therefore, the numerical approximation is that of approximating a derivative with a finite difference formula. In principle, you can make this calculation as accurate as you want by using increasingly accurate approximations to the numerical derivative, but in practice even low-order approximations lead to very accurate answers. The advantages of this method are that it is very simple to implement, and therefore it is in fact available using any underlying electronic structure method that can calculate forces, which includes semilocal DFT, hybrid DFT, or other non-DFT methods, like force fields or dynamical mean-field theory. The disadvantage of this method is that it requires the construction of supercells to capture long-wavelength phonons, which can make the calculations expensive. Most finite displacement codes use "diagonal" supercells, which lead to poor scaling, but the recently introduced "nondiagonal" supercells <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.92.184301" rel="noreferrer">here</a> [disclaimer: I am a co-author of this work], significantly reduce the computational cost.</p>

<p><strong>DFPT.</strong> In density functional perturbation theory, the calculation of the Hessian is specialized to DFT, and the second derivative of the energy is calculated as</p>

<p><span class="math-container">$$
\frac{\partial^2 E}{\partial\lambda_i\lambda_j}=\int\frac{\partial^2V(\mathbf{r})}{\partial\lambda_i\lambda_j}n(\mathbf{r})d\mathbf{r}+\int\frac{\partial n(\mathbf{r})}{\partial\lambda_i}\frac{V(\mathbf{r})}{\partial\lambda_j}d\mathbf{r}.
$$</span></p>

<p>This expression is general for parameters <span class="math-container">$\lambda$</span>, and in the case of phonons they simply become the atomic coordinate <span class="math-container">$\lambda_i=\mathbf{R}_i$</span>. This expression requires the calculation of the derivative of the density, which in turn requires the derivative of the Kohn-Sham states. These terms can all be calculated within DFPT with the usual numerical approximations of finite basis sets, etc. Again, in practice, modern implementations are relatively easy to converge. The advantage of DFPT is that it does not require the construction of supercells, one can build a finite wave vector response within the primitive cell, so the computational cost is smaller than in the finite displacement method. The disadvantage of this method is that it is restricted to DFT (so no DMFT for example), and furthermore, the algorithmic implementation is not trivial, so it is only widely available for semilocal DFT (so no hybrid DFT either).</p>

<p>In summary, these two methods lead to comparable accuracies. If DFPT is available, then the calculations will be cheaper and DFPT should be the method of choice. However, DFPT is only widely available with semilocal DFT, which means that if you want phonons at the hybrid functional level, or using beyond-DFT methods, then you have to use the finite displacement method.</p>


================================================================================

Question: Validity of adiabatic approximation in TDDFT
Body: <p>In the time-dependent Kohn-Sham formalism the effective potential on electrons is given by
<span class="math-container">$$ v_s[\rho(\mathbf{r},t)]=v(\mathbf{r},t)+v_H(\mathbf{r},t)+v_{xc}[\rho(\mathbf{r},t)] $$</span>
where, <span class="math-container">$v(\mathbf{r},t)$</span> is the external TD potential, <span class="math-container">$v_H(\mathbf{r},t)$</span> is the TD Hartree potential and <span class="math-container">$v_{xc}[\rho(\mathbf{r},t)]$</span> is the TD exchange-correlation potential. To get accurate results, we need to find a good approximation to <span class="math-container">$v_{xc}$</span> and a common starting point is the adiabatic approximation:
<span class="math-container">$$ v_{xc}[\rho(\mathbf{r},t)]= \left.v_{xc}[\rho_0(\mathbf{r})]\right|_{\rho_0(\mathbf{r})=\rho(\mathbf{r},t)} $$</span>
where <span class="math-container">$v_{xc}[\rho_0(\mathbf{r})]$</span> is the static XC potential. As said in Ullrich's TDDFT book,</p>

<blockquote>
  <p>this approximation means that <span class="math-container">$v_{xc}[\rho(\mathbf{r},t)]$</span> becomes exact in the limit where the adiabatic theorem of quantum mechanics applies, i.e., a physical system remains in its instantaneous eigenstate if a perturbation that is acting on it is slow enough.</p>
</blockquote>

<p><strong>What is the time scale on which the adiabatic approximation works?</strong> In other words, <strong>when is this approximation valid?</strong></p>

Best Answer: <p>The time scale is related to time derivative of the kinetic energy of electrons defined as:</p>

<p><span class="math-container">$$T(t) = \sum_{i} \int |\nabla \phi_{i}(\mathbf{r},t)|^{2} d^{3} \mathbf{r}$$</span></p>

<p>You have this for time-derivative of the kinetic energy:</p>

<p><span class="math-container">$$\frac{d T(t)}{d t} \simeq \frac{T(t=0)}{\tau}$$</span></p>

<p>Where <span class="math-container">$\tau$</span> is the relaxation time for kinetic energy in the order of period associated with the lowest excitation energy of the system.</p>


================================================================================

Question: Database or repository with values for the Hubbard potential U?
Body: <p>The <a href="https://materialsproject.org/" rel="noreferrer">Materials Project</a> and other materials databases have a nice periodic table on their home page that allows you to search for compounds. For some materials, they give an empirical <span class="math-container">$U$</span> value used to model the bandstructure, and a reference to the paper. However, this information is available for a limited amount of materials.</p>

<p>Is there a databse or repository where one can find <span class="math-container">$U$</span> values obtained by experiment or determined <em>ab-initio</em> for metals across the periodic table? </p>

Best Answer: <p>Unfortunately, I don't believe such a resource exists. One should keep in mind that it's not just a matter of a single U value for every metal, of course. The "best" empirical U value will depend on the property of interest. Perhaps more problematic, not every metal environment is made equal. Different oxidation states, coordination environments, and overall electronic structures will inevitably influence the empirically ideal U value as well as any <em>ab initio</em> U value, making it hard to have a one-value-fits-all approach for a given metal.</p>

<p>This variety and application-specific nature is probably the major reason why such a database doesn't exist right now, to the best of my knowledge. That being said, I think it would be an amazing resource if such a database did exist, even if there were a variety of U values categorized by material and property.</p>

<p>Beyond the band structures, it's worth mentioning that a commonly used set of U values are those taken from <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.73.195107" rel="noreferrer">this work</a> by Ceder and coworkers to reproduce the oxidation energies of transition metal oxides. This is what is used throughout the <a href="http://oqmd.org/documentation/vasp" rel="noreferrer">Open Quantum Materials Database</a> to try and achieve more accurate formation energies.</p>


================================================================================

Question: What are the pre/post processing tools available for VASP?
Body: <p>I am aware of <a href="https://romerogroup.github.io/pyprocar/" rel="noreferrer">pyPROCAR</a> and <a href="http://vaspkit.sourceforge.net/" rel="noreferrer">VASPKIT</a>. Are there any other tools supporting VASP</p>

Best Answer: <p>The <a href="https://wiki.fysik.dtu.dk/ase/" rel="noreferrer">Atomic Simulation Environment</a> (ASE) is a pretty big name missing from your list. It can read in a variety of VASP-generated output files and do several post-processing analyses. More importantly, it can be used to set up calculations and even run them using its <a href="https://wiki.fysik.dtu.dk/ase/ase/calculators/vasp.html" rel="noreferrer"><code>Vasp()</code> calculator</a>, provided you link it to a VASP executable. You can read more about ASE in <a href="https://iopscience.iop.org/article/10.1088/1361-648X/aa680e/meta" rel="noreferrer">this paper</a>.</p>

<p><a href="https://pymatgen.org/" rel="noreferrer">Pymatgen</a> also has several pre- and post-processing tools that work with VASP, although you cannot as easily launch calculations from the tool like you can with ASE. Pymatgen is a more general platform meant for materials analysis, but there are indeed several <a href="https://pymatgen.org/pymatgen.io.vasp.html" rel="noreferrer">VASP utilities</a>. You can read more about Pymatgen in <a href="https://www.sciencedirect.com/science/article/pii/S0927025612006295" rel="noreferrer">this paper</a>.</p>

<p>There are also the highly popular <a href="https://theory.cm.utexas.edu/vtsttools/scripts.html" rel="noreferrer">VTST Tools scripts</a>, which can be used to parse output files for various properties and set up transition state calculations.</p>

<p>And then there are countless short scripts scattered around the Internet for various use-cases, such as those of <a href="https://www.nsc.liu.se/~pla/vasptools/" rel="noreferrer">Peter Larsson</a>.</p>

<p>Of course, this excludes the many dozens of GUI-based programs that can parse VASP output files, such as those listed <a href="https://en.wikipedia.org/wiki/List_of_molecular_graphics_systems" rel="noreferrer">here</a>.</p>


================================================================================

Question: Benchmarking data for Ewald summation algorithms
Body: <p>Are there good benchmarks available for testing homemade Ewald summation code?</p>

<p>Optimally it would be very nice if there was several types of systems.</p>

<p>NIST has a very nice benchmark for energies of SPC/E water configurations <a href="https://www.nist.gov/mml/csd/chemical-informatics-research-group/spce-water-reference-calculations-10a-cutoff" rel="noreferrer">NIST SPCE</a> made up of 100, 200, 300, 750 water molecules and breaks the energy down into LJ, LJ long range correction, Ewald real, Ewald recipricol, Ewald self, Ewald intra.</p>

<p>However, it would be nice to also test Forces and pressure as well. It would also be nice to do this for flexible molecules.</p>

<p>While it is possible to generate a configuration in an open source package like Gromacs for instance, I am hesitant because there are so many unseen variables. The NIST data is very clear on what their parameters were and it takes no digging to ensure you use the same protocol. It doesn't help that it is well known that different packages, when given the same input parameters return different results, even if slightly different. A simple, purpose made testing code/database would be nice.</p>

Best Answer: <p>I have looked for a similar set of benchmarks and haven't found one, unfortunately. Hopefully the data referred to in the comments comes through!</p>
<p>In the meantime, I have an MIT-licensed implementation of the Wolf-like alternative to Ewald that @PhilHasnip <a href="https://arxiv.org/abs/1801.01070" rel="noreferrer">linked to</a>. Here is <a href="https://github.com/wcwitt/real-space-electrostatic-sum" rel="noreferrer">the repository</a>. Forces and stresses are available. You could try benchmarking against that, and I'd be happy to participate in generating a database.</p>


================================================================================

Question: How are charge transfer processes modeled in materials?
Body: <p>I'm interested in looking at current flow across a nanoscale junction, specifically a pair of electrodes linked by a molecular bridge. How is this sort of problem typically approached?</p>

<p>I'm vaguely familiar with the idea of using Green's functions for this purpose, but don't know much in detail beyond that. In terms of the device, is it more common to treat the electrodes as a large cluster or employ periodic boundary conditions for the whole system?</p>

Best Answer: <p>Here is a paper reviewing charge transport in molecular junctions: J. Chem. Phys. 148, 030901 (2018), <a href="http://dx.doi.org/10.1063/1.5003306" rel="noreferrer">http://dx.doi.org/10.1063/1.5003306</a></p>

<p>A typical way to treat such a problem is to couple the Non-equilibrium Green's function (NEGF) formalism with density-functional-theory (DFT), short NEGF-DFT. You need to solve
<span class="math-container">$$
(E-H-\Sigma^{R,B}(E))\cdot G^R(E) = I,
$$</span>
<span class="math-container">$$
G^\lessgtr(E) = G^R(E)\cdot\Sigma^{\lessgtr,B}(E)\cdot G^A(E),
$$</span>
where <span class="math-container">$E$</span> is the electron energy, <span class="math-container">$H$</span> the Hamiltonian obtained from DFT, <span class="math-container">$G$</span> are Green's functions, <span class="math-container">$\Sigma$</span> are self-energies and <span class="math-container">$I$</span> is the identity matrix. The superscripts denote the retarded, advances, lesser and greater <span class="math-container">$G$</span> and <span class="math-container">$\Sigma$</span>. The self-energies come from the open boundary conditions (OBC) and couple the device region perturbatively to the leads. These semi-infinite leads are essentially a periodic continuation of the electronic structure of the electrodes. The OBC enable particles to enter and leave the simulation domain. Once the equations are solved for all energies of interest, obeservable quantities such as the current and charge density are computed from the <span class="math-container">$G^\lessgtr(E)$</span>. Injecting charge into the device will affect the potential and therefore also the Hamiltonian. This creates a dependence of DFT on NEGF and vice-versa, which needs to be resolved self-consistently.</p>

<p>A detailed review of NEGF-DFT can be found here:
Proceedings of the IEEE, vol. 101, no. 2, pp. 518-530, Feb. 2013, <a href="http://dx.doi.org/10.1109/JPROC.2012.2197810" rel="noreferrer">http://dx.doi.org/10.1109/JPROC.2012.2197810</a></p>

<p>NEGF-DFT can also treat thermal transport, and coupled electro-thermal transport though the latter comes at considerable computational cost. In the presence of strong electron-electron or electron-phonon coupling NEGF can break down and more sophisticated but also far more costly simulations are needed such as the hierarchical quantum master equation (HQME) framework [C. Schinabeck, R. Härtle, and M. Thoss, Phys. Rev. B 94, 201407(R) (2016)].</p>

<p><strong>EDIT 2</strong>: The Hamiltonian, <span class="math-container">$G$</span> and <span class="math-container">$\Sigma$</span> depend on the electron momentum <span class="math-container">$k$</span>. If there is periodicity in the transverse directions, these equations have to be solved for multiple <span class="math-container">$k$</span>-vectors.</p>

<p><strong>EDIT 3</strong>: more on OBC</p>

<p>The Hamiltonian has the following form (requires a localized basis and correct ordering of the atoms):
<span class="math-container">$$
H = \begin{pmatrix}
%       \ddots &amp; &amp; &amp;  \\[0.2em]
       H_{11} &amp; H_{12} &amp; &amp; \\[0.2em]
       H_{21} &amp; H_{22} &amp; H_{23} &amp; \\[0.2em]
       &amp; H_{32} &amp; H_{33} &amp; \ddots  \\[0.2em]
       &amp; &amp; \ddots &amp; \ddots \\[0.2em]
\end{pmatrix}
$$</span>
The <span class="math-container">$H_{nn}$</span> are matrices corresponding to slabs sorted along the transport axis. Plug this into the stationary Schrödinger equation <span class="math-container">$ (IE-H)\Psi = 0$</span>
to find
<span class="math-container">$$(IE-H_{nn})\Psi_n - H_{nn+1}\Psi_{n+1} - H_{nn-1}\Psi_{n-1}=0,$$</span>
where <span class="math-container">$\Psi_n$</span> is the wave function in the <span class="math-container">$n$</span>-th slab. Assuming that the electrode material is periodically continued, we know that <span class="math-container">$H_{11}$</span>, <span class="math-container">$H_{12}$</span>, and <span class="math-container">$H_{21}$</span> should be repeated, i.e. <span class="math-container">$H_{00}=H_{11}$</span> etc. If the potential is homogeneous a plane-wave ansatz can be used for <span class="math-container">$\Psi$</span> to compute <span class="math-container">$\Sigma_{11}$</span>, which contains the "impact" the lead <span class="math-container">$H_{00}$</span> has on <span class="math-container">$H_{11}$</span>. This <span class="math-container">$\Sigma$</span> is the self-energy in the NEGF equations. A thorough derivation can be found in [Phys. Rev. B 74, 205323 (2006)] <a href="https://doi.org/10.1103/PhysRevB.74.205323" rel="noreferrer">https://doi.org/10.1103/PhysRevB.74.205323</a>.</p>

<p>The content of <span class="math-container">$H_{nn}$</span> depends on the electronic structure in the transverse directions. It will be different in the case of a nanowire without periodicity than in a 2D material or in bulk. But the tri-diagonal structure of the Hamiltonain remains the same in all cases. Therefore, the procedure to obtain the boundary self-energy is the same.</p>

<p>Edit: typo</p>


================================================================================

Question: What software is available to do molecular dynamics on Windows?
Body: <p>Is there a Windows-based molecular dynamics simulation software which is easy to use?</p>

Best Answer: <h2>LAMMPS</h2>
<p>LAMMPS provides pre-compiled Windows binaries. You can download them <a href="http://packages.lammps.org/windows.html" rel="noreferrer">here</a>.</p>


================================================================================

Question: How to find Young&#39;s Modulus using stress strain curve using Python?
Body: <p>I am trying to find the</p>
<ul>
<li>The Young's Modulus of an aluminum alloy</li>
<li>The yield stress</li>
</ul>
<p>Below is the data for the stress-strain curve. I tried finding the region where the slope is constant and to my surprise, I found that only 4 data points, in the beginning, give me a straight line. After that, there is another straight line that gives me a different slope. I am confused as to which line to use for the calculation of Young's Modulus. If I use the second line I get an answer that does not conform with the reality of the alloy.</p>
<p>So, I tried to find a package for the same : <a href="https://pythonhosted.org/MatPy/Material%20Analytics.html" rel="noreferrer">Here</a> is a link for the package which seems very useful. I wanted to download it but could not find it on conda( I am using Jupyter). I was hoping if someone would provide me with the link too.</p>
<p>PS: If there is no such package I would be grateful to receive help with the question without the package, too using Python.</p>
<pre><code>Eng. Strain Eng. Stress
0.000000    4.174826667
0.00000444  4.188426667
0.0000103   4.164346667
-3.28E-06   4.202586667
4.294E-05   7.26848
0.0001964   16.04202667
0.0003372   22.67858667
0.00048938  27.47632
0.00065652  30.70698667
0.00083308  32.89658667
0.00101768  34.41408
0.0012028   35.57272
0.001385    36.52789333
0.0015676   37.36733333
0.00173426  38.04624
0.00193392  38.65781333
0.00210862  39.24997333
0.00228426  39.80757333
0.00245992  40.32866667
0.00263822  40.91192
0.00280184  41.40042667
0.00298324  41.87698667
0.00315386  42.37184
0.00333258  42.77624
0.00347468  43.24722667
0.00365992  43.64136
0.003817    44.06544
0.00399314  44.49149333
0.0041544   44.90893333
0.00432732  45.32496
0.0044855   45.73408
0.00464472  46.13429333
0.00482466  46.54008
0.00499114  46.85842667
0.00514588  47.30917333
0.00529986  47.64224
0.00546718  48.00805333
0.0056278   48.38162667
0.00578708  48.74576
0.005939    49.07576
0.00609224  49.42216
0.00626128  49.75216
0.00640792  50.1016
0.00658574  50.41605333
0.0067529   50.76744
0.00688528  51.10133333
0.00705596  51.44245333
0.0072123   51.75136
0.00736482  52.06888
0.00752996  52.38224
0.00769128  52.68725333
0.00782884  53.04141333
0.00800278  53.30536
0.00814314  53.62370667
0.00831612  53.94037333
0.00844992  54.23848
0.0086224   54.51992
0.00876314  54.81496
0.00893086  55.08557333
0.00907164  55.37144
0.00924458  55.67368
0.00939518  55.95234667
0.00955464  56.2632
0.00971818  56.518
0.00986798  56.79248
0.01000268  57.03533333
0.01016506  57.32344
0.01033396  57.64594667
0.0104826   57.85688
0.01062482  58.13832
0.01079106  58.40506667
0.01093044  58.66234667
0.01109734  58.91434667
0.01124544  59.14917333
0.01140076  59.39784
0.0115488   59.66208
0.01170736  59.91490667
0.01186224  60.14085333
0.01202728  60.42061333
0.01216586  60.62210667
0.01231978  60.91189333
0.01248044  61.14253333
0.01262062  61.37677333
0.01277486  61.60464
0.01293878  61.84056
0.01308824  62.07674667
0.01324216  62.28269333
0.01339498  62.50834667
0.0135425   62.72845333
0.01370544  62.95021333
0.01386166  63.14504
0.0140343   63.40565333
0.01417988  63.62189333
0.01433782  63.81394667
0.014485    64.04514667
0.01463944  64.2572
0.01477906  64.45786667
0.01495412  64.68738667
0.01508752  64.85781333
0.01524908  65.06986667
0.01539768  65.29994667
0.01555352  65.47256
0.01569838  65.70794667
0.0158504   65.90362667
0.0160023   66.07208
0.0161674   66.28496
0.01631316  66.43429333
0.01647162  66.63189333
0.0166194   66.86005333
0.0167683   67.02157333
0.0169192   67.21392
0.01708108  67.40792
0.01723832  67.59722667
0.01737604  67.76485333
0.0175355   67.97274667
0.01769274  68.17978667
0.01784858  68.32938667
0.01799818  68.49093333
0.01815954  68.68466667
0.01831758  68.83314667
0.01847322  69.04992
0.01862252  69.21117333
0.0187715   69.39434667
0.01891798  69.56754667
0.0190876   69.7524
0.01923398  69.89256
0.01939434  70.08712
0.0195366   70.22893333
0.01971648  70.34189333
0.01985822  70.55896
0.02002302  70.69912
0.02017412  70.87232
0.0203226   71.05325333
0.02047664  71.22368
0.02063518  71.378
0.02079262  71.49984
0.02095872  71.6772
0.02110058  71.83706667
0.0212731   71.98554667
0.02141456  72.14514667
0.02157642  72.31472
0.0217224   72.45544
0.0218881   72.58008
0.02204202  72.73882667
0.02220198  72.90618667
0.02234996  73.05717333
0.02250912  73.16042667
0.0226554   73.34888
0.02281184  73.45768
0.0229732   73.63141333
0.02311676  73.80461333
0.02328296  73.88509333
0.02343206  74.03997333
0.02359212  74.17874667
0.02374814  74.35861333
0.02391816  74.46602667
0.02405478  74.59314667
0.02421896  74.71914667
0.02437068  74.9076
0.02453578  74.99642667
0.02467912  75.10328
0.02484332  75.27866667
0.02498948  75.3972
0.02517178  75.54706667
0.02531152  75.65808
0.02547098  75.78770667
0.02563648  75.88650667
0.02577992  76.01586667
0.02594008  76.15573333
0.02609704  76.29701333
0.02625378  76.43245333
0.02643204  76.52432
0.02656072  76.63229333
0.0267267   76.73442667
0.02689018  76.89376
0.0270433   77.02032
0.02720226  77.13714667
0.02736182  77.24706667
0.02752298  77.35976
0.02767006  77.468
0.0278426   77.61010667
0.02798816  77.73112
0.02815316  77.84464
0.0283104   77.90069333
0.0284782   78.07165333
0.02864822  78.19074667
0.0288112   78.26538667
0.02897176  78.45469333
0.02913716  78.51962667
0.02929088  78.6176
0.0294621   78.70698667
0.02960114  78.84074667
0.02978062  78.92290667
0.0299273   79.07472
0.03009852  79.14413333
0.03024762  79.23293333
0.03041582  79.36672
0.03057076  79.41834667
0.03072468  79.57042667
0.03088362  79.64592
0.03105124  79.73778667
0.03120838  79.88128
0.03136452  79.96650667
0.03151964  80.06365333
0.03168926  80.17216
0.03184218  80.28456
0.03200576  80.37784
0.03216934  80.47802667
0.03232044  80.52965333
0.03247638  80.68146667
0.03264298  80.69978667
0.03278724  80.82829333
0.03294822  80.95792
0.03311742  81.05642667
0.03327538  81.10861333
0.0334452   81.22048
0.03359166  81.38226667
0.03376652  81.49912
0.03391038  81.58461333
0.03407878  81.68341333
0.03423976  81.74005333
0.03440254  81.85717333
0.0345623   81.95264
0.03472748  82.05312
0.03487094  82.10474667
0.035047    82.18466667
0.03520916  82.2624
0.0353812   82.36562667
0.03552828  82.43002667
0.03569588  82.55048
0.03585182  82.63096
0.03601058  82.74032
0.03616732  82.79362667
0.03634498  82.85578667
0.03649106  82.95016
0.0366649   83.07117333
0.03682908  83.16330667
0.03698662  83.22381333
0.03714678  83.31541333
0.03732324  83.39701333
0.03747414  83.4936
0.03763572  83.56464
0.03782082  83.61794667
0.03797092  83.68733333
0.03813632  83.77336
0.03830854  83.83554667
0.0384661   83.93712
0.03862746  84.03925333
0.03879688  84.11253333
0.03894456  84.15082667
0.03911498  84.26296
0.03929084  84.29237333
0.03944556  84.41450667
0.03961096  84.48
0.03978278  84.56384
0.03993792  84.60490667
0.04009688  84.69261333
0.0402832   84.74978667
0.0404331   84.8336
0.04059326  84.90021333
0.04076308  84.96738667
0.04092102  85.06786667
0.04109364  85.1256
0.04126306  85.19165333
0.04143128  85.25104
0.04157474  85.33709333
0.04174676  85.41978667
0.04192444  85.47864
0.04209686  85.54525333
0.0422381   85.61685333
0.0424186   85.6496
0.04257452  85.77506667
0.04274174  85.84333333
0.04291396  85.87885333
0.04307834  85.95546667
0.04324334  86.05317333
0.04341436  86.11256
0.0435689   86.18194667
0.04373328  86.22024
0.04391194  86.28853333
0.04407714  86.36568
0.04423408  86.37122667
0.04439564  86.47725333
0.0445703   86.56941333
0.04472804  86.58216
0.0449045   86.65210667
0.0450564   86.73594667
0.04523388  86.82309333
0.0454063   86.89637333
0.0455721   86.91968
0.04572482  87.00570667
0.04589806  87.05901333
0.0460562   87.10341333
0.04622744  87.1728
0.04639262  87.24050667
0.04655822  87.28048
0.04673426  87.33712
0.04690388  87.42813333
0.04706606  87.47866667
0.04723628  87.55802667
0.04740208  87.58856
0.04757974  87.65794667
0.0477387   87.68293333
0.0479073   87.76730667
0.04807128  87.85224
0.04822944  87.85778667
0.04841394  87.92608
0.04858176  88.00877333
0.04874936  88.08205333
0.04891334  88.12701333
0.04908194  88.1764
0.04924654  88.22304
0.04940992  88.24690667
0.04958014  88.34626667
0.04975518  88.39290667
0.0499083   88.43232
0.05008898  88.50114667
0.05025638  88.54277333
0.05041856  88.61162667
0.05058616  88.64936
0.05076444  88.70541333
0.05092158  88.74928
0.05108858  88.8292
0.05125376  88.86141333
0.05143364  88.91413333
0.05158676  88.98074667
0.05177086  89.00962667
0.05193686  89.07458667
0.05210448  89.12064
0.05226222  89.19890667
0.05243526  89.24053333
0.05259824  89.2572
0.05276966  89.33269333
0.05293786  89.38765333
0.05310588  89.42816
0.05327832  89.48034667
0.0534455   89.53808
0.05361472  89.60578667
0.05378032  89.63301333
0.0539429   89.70794667
0.05411492  89.72125333
0.05428514  89.754
0.05443022  89.8628
0.05462358  89.88778667
0.05479318  89.95608
0.0549608   89.99602667
0.05512678  90.00824
0.05528696  90.04432
0.05545114  90.07485333
0.05562438  90.14368
0.05578996  90.25304
0.05597306  90.26192
0.05612618  90.30688
0.05629198  90.3352
0.05647266  90.3896
0.05663422  90.42901333
0.05680888  90.48784
0.05697748  90.51448
0.0571465   90.52949333
0.0573137   90.63938667
0.05748734  90.66602667
0.05766038  90.74597333
0.05782256  90.74042667
0.05800444  90.78706667
0.05819176  90.85976
0.05835998  90.88586667
0.0585306   90.92858667
0.05872134  90.98909333
0.05887224  91.04794667
0.05905656  91.05293333
0.05922758  91.12066667
0.05938532  91.1412
0.05956842  91.20448
0.05972698  91.23389333
0.05989358  91.26610667
0.06006944  91.34602667
0.06024608  91.36546667
0.06039578  91.41874667
0.06058612  91.46426667
0.06075594  91.51090667
0.06092696  91.56917333
0.06109196  91.56973333
0.06126418  91.61914667
0.06142636  91.65744
0.0615978   91.67133333
0.06177204  91.74237333
0.06193984  91.79565333
0.0621177   91.82674667
0.0622674   91.85506667
0.06247424  91.92944
0.06261388  91.9472
0.06278894  91.97330667
0.06297766  92.03656
0.06314546  92.06933333
0.06332012  92.09152
0.06347464  92.14981333
0.0636493   92.17757333
0.06382514  92.22477333
0.0639857   92.27082667
0.06415874  92.28637333
0.06433218  92.3108
0.06450924  92.34909333
0.06467384  92.43957333
0.06483722  92.46624
0.06501428  92.49842667
0.06518248  92.50453333
0.0653672   92.59834667
0.0655169   92.58501333
0.0657004   92.62498667
0.06587222  92.67992
0.06604122  92.70269333
0.06620704  92.73378667
0.06637322  92.76488
0.06654586  92.81816
0.06671446  92.87810667
0.06688428  92.88810667
0.06707222  92.94138667
0.06724526  92.97746667
0.06742272  92.98413333
0.0675877   93.02242667
0.06775914  93.05130667
0.0679394   93.13568
0.06809958  93.17453333
0.06828066  93.19285333
0.06844444  93.20008
0.06861788  93.24669333
0.06879374  93.28
0.06896476  93.31053333
0.06913216  93.3616
0.06930642  93.3716
0.06948266  93.41989333
0.06965208  93.42933333
0.06984 93.45597333
0.06999212  93.53533333
0.070172    93.56197333
0.070339    93.58029333
0.07051648  93.60418667
0.07068106  93.61194667
0.07085248  93.70077333
0.07102632  93.71741333
0.0712042   93.72021333
0.07137844  93.77405333
0.07155308  93.8312
0.0717249   93.81957333
0.0718891   93.86784
0.07206496  93.88781333
0.0722384   93.97221333
0.0724231   93.98442667
0.07257562  94.01773333
0.07275468  94.02882667
0.07291726  94.07712
0.07311162  94.10821333
0.0732742   94.11986667
0.07343476  94.16317333
0.07360942  94.19370667
0.073774    94.22698667
0.07396594  94.24144
0.07413496  94.30250667
0.07431524  94.32858667
0.0744589   94.32248
0.07464282  94.33746667
0.07481706  94.41296
0.07498968  94.42128
0.07515508  94.45293333
0.07533778  94.49954667
0.07550518  94.53173333
0.07567742  94.55562667
0.07584844  94.58338667
0.07601826  94.62890667
0.07620296  94.63666667
0.07637078  94.68274667
0.07655588  94.68328
0.07671886  94.73768
0.07689068  94.78544
0.07706372  94.79208
0.07724562  94.84984
0.07741704  94.86981333
0.07759692  94.88146667
0.07775748  94.91256
0.077943    94.97194667
0.07811562  94.95696
0.07829028  95.00912
0.0784621   95.04912
0.07864038  95.03245333
0.0788134   95.10904
0.07898486  95.07797333
0.07914662  95.13402667
0.07932488  95.18842667
0.0795116   95.20674667
0.07968102  95.21674667
0.07985042  95.2384
0.08003192  95.29722667
0.08020778  95.30112
0.08038564  95.35717333
0.0805615   95.36885333
0.08072448  95.40714667
0.08090314  95.436
0.08107054  95.44322667
0.08125646  95.47930667
0.08142468  95.50373333
0.08160214  95.54869333
0.0817647   95.54536
0.08194458  95.55981333
0.08211682  95.61418667
0.0822979   95.65192
0.08244882  95.67690667
0.08264078  95.67802667
0.08281662  95.71578667
0.08298764  95.73853333
0.083147    95.71909333
0.08331964  95.78184
0.08350796  95.83624
0.08369188  95.85954667
0.08384358  95.85178667
0.08404076  95.89229333
0.08420092  95.92226667
0.08437918  95.92338667
0.08454136  95.94778667
0.08473008  96.01552
0.08490274  96.02552
0.0850665   96.05493333
0.08524558  96.09656
0.08542264  96.08269333
0.08559528  96.13098667
0.0857655   96.15208
0.08594096  96.17538667
0.08610754  96.20426667
0.08629266  96.20202667
0.08645766  96.25589333
0.08664678  96.27976
0.0868146   96.28864
0.08699084  96.31416
0.087149    96.31861333
0.08734578  96.38357333
0.08750996  96.39410667
0.08769428  96.44962667
0.08787414  96.48293333
0.08804516  96.47461333
0.08822222  96.52845333
0.08839688  96.53453333
0.08857756  96.56674667
0.08874738  96.58952
0.08892926  96.58061333
0.08910592  96.58061333
0.08927656  96.60392
0.08944316  96.66776
0.0896335   96.67888
0.08980572  96.70717333
0.0899868   96.72770667
0.09016186  96.74216
0.09032524  96.77101333
0.09050794  96.78989333
0.090687    96.83373333
0.09086126  96.82597333
0.09103428  96.87704
0.09120572  96.89704
0.09137956  96.92034667
0.09156348  96.95365333
0.09174214  96.96032
0.09192202  96.95864
0.09209062  97.02968
0.09226768  97.03357333
0.0924391   97.05189333
0.09261738  97.05522667
0.09278318  97.11461333
0.0929687   97.10573333
0.09314414  97.11517333
0.0933216   97.12573333
0.09349344  97.13629333
0.09366568  97.17234667
0.09385964  97.23621333
0.09402864  97.22896
0.0941884   97.24784
0.09438278  97.27282667
0.09455178  97.30168
0.09471918  97.32666667
0.09490028  97.32666667
0.09506486  97.36109333
0.09525842  97.41770667
0.0954222   97.38218667
0.0955892   97.4388
0.0957731   97.44936
0.09594936  97.43768
0.0961385   97.47432
0.0963051   97.49930667
0.09648456  97.53650667
0.0966584   97.52429333
0.09684432  97.57536
0.09700812  97.60810667
0.09719322  97.622
0.0973566   97.64141333
0.09754212  97.67637333
0.09771596  97.65306667
0.09789542  97.68581333
0.09806002  97.71136
0.0982407   97.72690667
0.09842058  97.78682667
0.09860328  97.748
0.0987731   97.76853333
0.09894974  97.82570667
0.09913808  97.82237333
0.09931956  97.85346667
0.09948576  97.86954667
0.09966966  97.90010667
0.09983828  97.90730667
0.10003104  97.93117333
0.10021092  97.95170667
0.10038394  97.97834667
0.10057308  97.95282667
0.10074008  98.02776
0.10091312  98.00501333
0.10110146  98.00944
0.1012757   98.07384
0.10145274  98.11048
0.1016439   98.12048
0.10181412  98.13045333
0.10200366  98.09605333
0.10216746  98.18597333
0.10234934  98.15986667
0.10251914  98.2004
0.10268576  98.2276
0.10287852  98.23314667
0.10305074  98.21706667
0.103227    98.2476
0.10340044  98.30421333
0.10357066  98.30253333
0.10375294  98.34472
0.10394248  98.33752
0.10412116  98.39304
0.10430666  98.35082667
0.10447288  98.43464
0.10464994  98.44906667
0.1048451   98.45464
0.1050121   98.46352
0.1051948   98.47016
0.1053598   98.49570667
0.10555254  98.49736
0.10573364  98.50349333
0.10591028  98.53178667
0.10607648  98.57565333
0.10625434  98.58952
0.10643784  98.60061333
0.10662256  98.65392
0.10678392  98.64002667
0.10697508  98.66392
0.10715214  98.67778667
0.10732396  98.69666667
0.10751552  98.66557333
0.10767648  98.77269333
0.107864    98.75661333
0.1080471   98.76381333
0.10822618  98.77658667
0.10840364  98.77605333
0.10858834  98.79602667
0.10876098  98.83765333
0.1089497   98.80824
0.10912154  98.9148
0.1093139   98.90648
0.10948572  98.89037333
0.10966076  98.91149333
0.10984708  98.93368
0.11002294  98.9892
0.11020926  98.9348
0.11039156  98.95589333
0.11056982  98.98032
0.11075534  99.02693333
0.1109344   99.03581333
0.11111308  99.10133333
0.11129296  99.05637333
0.11146076  99.08965333
0.11165192  99.12408
0.11183582  99.08634667
0.11201328  99.12074667
0.11217988  99.12130667
0.11236216  99.13962667
0.1125533   99.19957333
0.11272516  99.1796
0.11290864  99.20624
0.11308934  99.21344
0.11327162  99.23010667
0.11344184  99.26008
0.11362736  99.28896
0.11381206  99.30170667
0.11398872  99.32837333
0.11417262  99.32392
0.11435814  99.37165333
0.1145364   99.36168
0.11471508  99.36442667
0.11489416  99.40274667
0.11507886  99.42549333
0.11524586  99.40386667
0.11543218  99.43994667
0.11561246  99.46490667
0.11578066  99.47325333
0.11596498  99.46378667
0.1161525   99.48768
0.11633036  99.46658667
0.11652392  99.51541333
0.11669898  99.54485333
0.11687322  99.58037333
0.11704142  99.58093333
0.11723458  99.56482667
0.11742734  99.58482667
0.11759796  99.59925333
0.11778146  99.61757333
0.1179698   99.65698667
0.11812916  99.66698667
0.11832472  99.65754667
0.11850662  99.63365333
0.11869334  99.74245333
0.11887924  99.72746667
0.11906354  99.70914667
0.1192398   99.73968
0.11942492  99.74525333
0.11960198  99.75245333
0.11977502  99.77522667
0.11996496  99.80464
0.12014766  99.84349333
0.12032712  99.83128
0.12051626  99.83794667
0.12069414  99.87512
0.12088166  99.87792
0.12106274  99.86957333
0.12124302  99.89901333
0.12142892  99.90677333
0.12161162  99.94896
0.12178788  99.93232
0.12197218  99.97448
0.12214764  99.99725333
0.12233838  99.98781333
0.12251384  99.98893333
0.12269976  99.99837333
0.12289694  100.0350133
0.12306836  100.0549867
0.12324946  100.0633067
0.12342812  100.11048
0.12361324  100.0916267
0.12379592  100.0699733
0.12397138  100.09272
0.12415406  100.13272
0.12434644  100.1449067
0.1245082   100.0872
0.12469814  100.1554667
0.12490256  100.1698933
0.12507078  100.1893333
0.12524944  100.18488
0.1254249   100.22872
0.12559794  100.24648
0.12578786  100.23872
0.12596092  100.25816
0.12614844  100.2709067
0.1263424   100.26424
0.12651382  100.28592
0.12671664  100.2698133
0.12690176  100.2886933
0.12707076  100.3192
0.12725426  100.3591733
0.12743776  100.3525067
0.12762124  100.3441867
0.1277935   100.39416
0.12798584  100.3680533
0.1281581   100.4163467
0.12833836  100.4224533
0.12853554  100.4230133
0.1287118   100.42024
0.12890738  100.4624267
0.12907316  100.4685333
0.1292438   100.4696533
0.12945466  100.4718667
0.12962528  100.51904
0.12981118  100.50408
0.1299947   100.4974133
0.13017498  100.5262667
0.13034318  100.5523733
0.1305291   100.574
0.13070858  100.60232
0.13091622  100.60176
0.13110778  100.60344
0.13128724  100.6117333
0.13146592  100.6250933
0.1316478   100.6589333
0.13183614  100.6328533
0.13200676  100.7055733
0.13219994  100.66448
0.13239146  100.6628267
0.13255886  100.6983467
0.1327472   100.76384
0.13293714  100.7394133
0.13312226  100.7444267
0.13329528  100.7127733
0.13348602  100.7405333
0.13368322  100.7949333
0.1338643   100.8054933
0.13404298  100.7682933
0.13421762  100.80216
0.1343971   100.8077067
0.13459426  100.81768
0.1347826   100.82824
0.13497254  100.84656
0.135148    100.85544
0.13533552  100.8709867
0.13550854  100.9342667
0.13569928  100.8787467
0.13587476  100.89152
0.1360663   100.90096
0.136249    100.9437067
0.13642928  100.9781067
0.13661358  100.96256
0.13679548  100.9620267
0.13700794  100.9936533
0.13716088  100.9936533
0.13734194  101.0225333
0.13754636  101.0397333
0.13772024  100.9975467
0.13791096  101.03864
0.13808804  101.0247467
0.13826508  101.0136533
0.13844778  101.0852533
0.13864496  101.0674933
0.13882446  101.08192
0.13902564  101.0874667
0.13920352  101.0769333
0.1393886   101.1130133
0.13957534  101.1346667
0.13974354  101.1524267
0.1399351   101.18072
0.14012342  101.1402133
0.1403021   101.1846133
0.14048882  101.1868267
0.14067394  101.1940533
0.14084132  101.2323467
0.14103852  101.1762933
0.14123168  101.2240267
0.14141196  101.2406667
0.1415882   101.26456
0.14177736  101.2812
0.14195682  101.28952
0.1421733   101.2678667
0.14233346  101.2839733
0.14252584  101.2678667
0.14271014  101.3206133
0.1429065   101.3528
0.143086    101.33728
0.14329604  101.3345067
0.14347472  101.3783467
0.14366626  101.3728
0.14385378  101.3583467
0.14403004  101.4049867
0.14421274  101.4294133
0.14439624  101.3822133
0.1445878   101.4210933
0.1447753   101.4038667
0.1449556   101.4010933
0.14516164  101.4677067
0.14534112  101.4682667
0.1455222   101.4554933
0.14571374  101.4660533
0.14589726  101.4921333
0.14607752  101.49992
0.14626908  101.50768
0.14644292  101.51768
0.14663528  101.51656
0.14683004  101.5298933
0.14701838  101.5581867
0.14718818  101.5482133
0.14738616  101.5587467
0.14756646  101.5804
0.14774916  101.58984
0.1479415   101.6137067
0.1481427   101.5742933
0.14831332  101.6003733
0.14850248  101.6386933
0.14870046  101.65144
0.1488751   101.6209333
0.14906504  101.6042667
0.1492421   101.68144
0.1494401   101.6669867
0.14963808  101.682
0.14982238  101.69088
0.1500091   101.6869867
0.15019986  101.6953067
0.15038416  101.69752
0.15056684  101.7025067
0.15075598  101.72528
0.15094108  101.72696
0.1511254   101.7469333
0.1513105   101.7552533
0.15149964  101.7497067
0.15168716  101.79024
0.15189802  101.77248
0.15206866  101.81576
0.15226904  101.7846933
0.1524421   101.8096533
0.15262718  101.8374133
0.15281794  101.8640533
0.15300948  101.86016
0.1532131   101.8646133
0.15340144  101.80688
0.15359942  101.86016
0.1537789   101.90568
0.1539785   101.90792
0.15414912  101.90624
0.15433906  101.9384267
0.15454348  101.9101333
0.15472056  101.9195733
0.154896    101.9317867
0.15510526  101.9389867
0.15529438  101.9412
0.1554779   101.98616
0.15568716  101.9728533
0.15587144  101.95008
0.15605496  101.934
0.15624568  102.0006133
0.1564284   102.0028267
0.1566143   102.0183733
0.15680584  102.0156
0.15701432  102.0256
0.1571978   102.0222667
0.15738614  102.0305867
0.15758008  102.0550133
0.15775474  102.05944
0.15794226  102.0677867
0.15814428  102.0528
0.15832456  102.0933067
0.15851288  102.0816533
0.15869718  102.0644533
0.15889196  102.1382667
0.15907546  102.1188533
0.15927346  102.1127467
0.15945534  102.1382667
0.15966942  102.1277333
0.15984568  102.14328
0.16003966  102.1665867
0.16022394  102.1549333
0.1604155   102.1671467
0.16060302  102.18656
0.16079536  102.1665867
0.16098452  102.2132
0.16117442  102.1726933
0.16136198  102.20488
0.16154708  102.2104267
0.16175392  102.21432
0.161931    102.2609333
0.16212092  102.2504
0.16231732  102.2548533
0.16249998  102.2398667
0.16269718  102.2432
0.16287906  102.2398667
0.16307142  102.2876
0.16326218  102.3192533
0.16345048  102.2948267
0.16363962  102.2925867
0.16383922  102.29592
0.16403158  102.2814933
0.16422232  102.32424
0.16440824  102.3453333
0.16461106  102.34976
0.16479858  102.3519733
0.16497966  102.3447733
0.16518892  102.3192533
0.16538126  102.3547733
0.16556638  102.3886133
0.16575874  102.3481067
0.16594868  102.3492267
0.16614424  102.4180533
0.16633176  102.41416
0.16653378  102.4335733
0.16671568  102.4097067
0.16689916  102.41304
0.16710842  102.4446933
0.16729192  102.39528
0.16750118  102.4557867
0.16768388  102.4458133
0.16789554  102.468
0.16807986  102.4297067
0.16826014  102.4807733
0.16846534  102.46856
0.1686513   102.4990933
0.16885168  102.5007467
0.16904404  102.4990933
0.16922432  102.4962933
0.16942632  102.5174133
0.16960984  102.5096267
0.16980782  102.5068533
0.16999692  102.5018667
0.17019252  102.55736
0.17038164  102.55904
0.17058044  102.5523733
0.17076072  102.5385067
0.17096032  102.5695733
0.17114382  102.54848
0.17134262  102.6056533
0.17154302  102.5751467
0.17173296  102.5618133
0.17191806  102.5729333
0.17211362  102.6006667
0.17231082  102.57568
0.17250236  102.63952
0.17270598  102.6073333
0.17289514  102.6756
0.1730939   102.6345333
0.1732742   102.6361867
0.17347378  102.6450933
0.1737048   102.6978133
0.17392852  102.6789333
0.17412006  102.7105867
0.17429874  102.6611733
0.17449028  102.6756
0.17469148  102.7066933
0.17489028  102.6972533
0.17508504  102.71336
0.17526856  102.68616
0.17545526  102.7028
0.1756613   102.69448
0.17584562  102.7244533
0.17604278  102.7128
0.176244    102.7333333
0.17643878  102.75832
0.17663596  102.73944
0.1768307   102.7816267
0.1770432   102.7727467
0.17722832  102.78496
0.17742308  102.8016267
0.177609    102.7805333
0.17781342  102.7572
0.1779945   102.7999467
0.17822548  102.8365867
0.17840578  102.7888533
0.17860054  102.8388267
0.1787961   102.8099467
0.17900052  102.8182667
0.17919208  102.7921867
0.17938524  102.8110667
0.1795808   102.85824
0.17977558  102.8382667
0.17996392  102.8632267
0.18017156  102.8893333
0.18037196  102.8621333
0.18056028  102.8416
0.18076634  102.87544
0.1809635   102.8965333
0.18115264  102.886
0.18134336  102.8654667
0.18153976  102.8826667
0.18173372  102.9026667
0.18193896  102.9087467
0.18212086  102.9242933
0.18233814  102.87712
0.18251358  102.9353867
0.182714    102.92208
0.18291844  102.94984
0.1831132   102.9509333
0.18331762  102.9115467
0.1834971   102.9426133
0.18370152  102.97536
0.18390032  102.9703467
0.18410714  102.94928
0.18428986  103.0047733
0.18449668  102.9414933
0.1846818   102.95648
0.18490876  102.97704
0.1851011   102.9637067
0.18529024  103.0053333
0.18548262  102.9903467
0.1856516   102.9809067
0.1858657   103.02144
0.18605642  103.00368
0.1862665   102.98592
0.18647252  103.0281067
0.18664558  103.0242133
0.18685964  103.0436267
0.1870496   103.0292
0.18724598  103.0186667
0.18744958  103.0219733
0.18764516  103.0564
0.18784234  103.0958133
0.18804434  103.1058133
0.18824394  103.07472
0.18844114  103.0564
0.18865682  103.0819467
0.18885402  103.0708267
0.18905118  103.1185867
0.18924596  103.1157867
0.18944958  103.1102667
0.18964434  103.11304
0.18984074  103.0891467
0.19005802  103.10248
0.19025762  103.13968
0.19044274  103.1330133
0.19064474  103.08584
0.19085724  103.1429867
0.1910552   103.1074667
0.1912371   103.1546667
0.19144556  103.1590933
0.19164754  103.1774133
0.19185922  103.1252267
0.19204676  103.15576
0.19224314  103.1618667
0.1924548   103.1541067
0.19266404  103.16632
0.1928572   103.19352
0.19305036  103.21128
0.1932556   103.2007467
0.1934528   103.1635467
0.19365158  103.1979467
0.19384232  103.2179467
0.19405238  103.2406933
0.1942359   103.2101867
0.19444998  103.21072
0.19466324  103.2134933
0.19484594  103.2046133
0.19504794  103.2629067
0.19523948  103.2673333
0.19546564  103.22848
0.1956387   103.24848
0.19585438  103.2534667
0.19605638  103.23792
0.19626324  103.2912
0.19645238  103.2318133
0.19666002  103.28344
0.19686604  103.2518133
0.1970592   103.2306933
0.19725882  103.27568
0.19745276  103.2445867
0.19765558  103.2712267
0.19786002  103.30232
0.19806604  103.2673333
0.19826804  103.2806667
0.1984757   103.2723467
0.19867448  103.2901067
0.1988596   103.3234133
0.19906484  103.2901067
0.19927328  103.3317333
0.19948576  103.3256267
0.19968054  103.3173067
0.1998769   103.3345067
0.20007006  103.3161867
0.20028094  103.3212
0.20049422  103.3245333
0.20068414  103.3006667
0.20090788  103.3594933
0.20112678  103.3889067
0.20132238  103.3805867
0.20152278  103.3234133
0.20173122  103.3778133
0.2019276   103.36504
0.20214328  103.38336
0.20234772  103.37392
0.20253928  103.35672
0.20273644  103.3722667
0.20296178  103.3711467
0.20315336  103.3450667
0.20335294  103.36672
0.20356862  103.42112
0.20376664  103.36392
0.20397668  103.3817067
0.20418272  103.4155467
0.20437828  103.40944
0.20458916  103.40168
0.20479842  103.4227733
0.20501652  103.4355467
0.2052137   103.3889067
0.20540202  103.42888
0.20562334  103.3772533
0.20581812  103.45664
0.2060314   103.4089067
0.20622054  103.46608
0.20642336  103.4238933
0.20664066  103.43608
0.20683944  103.44608
0.20707204  103.4527467
0.2072507   103.4627467
0.20746158  103.4805067
0.20765634  103.4699467
0.2078624   103.4766133
0.20806118  103.4627467
0.20827364  103.46552
0.2084821   103.4710667
0.20867204  103.4799467
0.20888772  103.4605333
0.2091187   103.4227733
0.20931108  103.48384
0.20950342  103.4633067
0.20971752  103.4677333
0.2099147   103.4699467
0.21013844  103.4694133
0.21032434  103.51048
0.2105481   103.4721867
0.2107332   103.49104
0.21096178  103.5321333
0.211167    103.51216
0.21137706  103.5043733
0.21157584  103.5082667
0.21177464  103.5249333
0.21199838  103.5226933
0.21218912  103.54936
0.21240724  103.53656
0.21261486  103.5077067
0.21282334  103.5332267
0.21304304  103.5587733
0.21325954  103.5526667
0.21345512  103.5870933
0.21365956  103.54488
0.21386558  103.51992
0.21408448  103.5593333
0.21429294  103.5509867
0.21449334  103.52824
0.2147026   103.5793067
0.21492152  103.5854133
0.21512996  103.57264
0.21534 103.5432267
0.21554282  103.5809867
0.21574404  103.5759733
0.21596858  103.54656
0.21617142  103.5848533
0.21636296  103.5632
0.21658588  103.5515467
0.21679434  103.5881867
0.21700602  103.54824
0.21721044  103.60816
0.21743018  103.61872
0.2176394   103.5721067
0.21785268  103.5682133
0.21805792  103.6065067
0.21825348  103.58208
0.2184732   103.60152
0.21867604  103.60152
0.21890378  103.61984
0.21910582  103.6059467
0.21929816  103.6104
0.2195074   103.61816
0.21971826  103.6131733

</code></pre>
<p>Also please tell me how reliable the new python package is.</p>

Best Answer: <p>I was able to run the code on your data and got a Young's Modulus of <span class="math-container">$2.08236\times10^{-5}$</span>. This clearly doesn't seem right for an aluminum system, though I don't know the units you are using.</p>
<p>To get this, I had to make a few changes to both the code and your data.</p>
<ol>
<li><p>As suggested, by Enusi, I commented out <code>from model import stress_strain</code> from the __init__.py file.</p>
</li>
<li><p>Line 159 of material_analytics.py (<code>return data[0,1] + young_modulus*(x-offset)</code>) starts with a tab rather than spaces. This needs to be changed for the code to run.</p>
</li>
<li><p>If you are using Python3.x, one of the functions uses <code>xrange</code>, which no longer exists after Python2.x. This needs to be replaced with <code>range</code> in order to run.</p>
</li>
<li><p>I removed your first and fourth data points. The <code>youngs_modulus</code> function uses a log at some point, so it will give warnings if you pass it numbers that are zero or negative.</p>
</li>
</ol>
<p>I can only guess that something is wrong with your choice of data points. The curve below is an example stress-strain curve for steel and then your aluminum alloy curve. Your curve does not really have a linear section and looks more like the example curve after the yield strength.</p>
<p><a href="https://i.sstatic.net/8znsi.png" rel="noreferrer"><img src="https://i.sstatic.net/8znsi.png" alt="Stress-strain curve" /></a>
<a href="https://i.sstatic.net/SK3uC.png" rel="noreferrer"><img src="https://i.sstatic.net/SK3uC.png" alt="User stress-strain curve" /></a></p>


================================================================================

Question: What are the types of ab initio Molecular Dynamics?
Body: <p>I am new to the world of Molecular Dynamics, and am curious to know <em>exactly</em> what is considered to be an <em>ab initio</em> Molecular Dynamics (AIMD) method, and how do they work?</p>
<p>The seminal work by Roberto Car and Michele Parrinello, titled &quot;<a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.55.2471" rel="noreferrer">Unified Approach for Molecular Dynamics and Density-Functional Theory</a>&quot; was published 35 years ago!</p>

<p>I have recently come across the following &quot;types&quot; of MD methods while reading some research papers:</p>
<ul>
<li>Born-Oppenheimer Molecular Dynamics</li>
<li>Car-Parinello Molecular Dynamics <a href="https://mattermodeling.stackexchange.com/a/1525/5">[link to answer]</a></li>
<li>2nd generation CPMD</li>
<li>Approximate Car-Parrinello-like Langevin Born-Oppenheimer Molecular Dynamics</li>
<li>Many-body Molecular Dynamics</li>
<li><a href="https://aip.scitation.org/doi/abs/10.1063/1.2008258?journalCode=jcp" rel="noreferrer"><em>ab initio</em> Ehrenfest Dynamics</a></li>
<li><a href="https://wiki.fysik.dtu.dk/gpaw/dev/documentation/ehrenfest/ehrenfest_theory.html" rel="noreferrer">TDDFT/MD</a></li>
</ul>
<p>If I am missing methods, which I am sure I am, please feel free to add them through an answer!
Also, it would be appreciated if <em><strong>one</strong></em> method is explained per answer, and is summarized in <strong>2-3 paragraphs</strong>.</p>


Best Answer: <h2>CPMD: Car-Parrinello Molecular Dynamics</h2>
<p>An approximation of BOMD (Born-Oppenheimer MD) where <em>fictitious dynamics</em> is used on the electrons to keep them close to their ground state, so that we do not have to keep solving for their ground state at every single step. We start with Newton's 2nd law (as does classical MD), but instead of the force being calculated by a full-fledged <em>ab initio</em> calculation at every step, the force itself has an EOM (equation of motion) which below is given by Eq. \eqref{eq:fictitious}. For one nucleus with position <span class="math-container">$\vec{r}$</span> and several electrons with orbitals <span class="math-container">$\{\psi_i\}$</span> we get:</p>
<p><span class="math-container">\begin{align}
\tag{1}
\vec{F} &amp;= m\vec{\ddot{r}} \\
- \nabla \, E\left[\{ \psi_i \} , \vec{r} \right] &amp;= m\vec{\ddot{r}}\tag{2}\\
\mu \ddot{\psi}_i(\vec{r},t) &amp;= - \frac{\delta E}{\delta \psi_i^*(\vec{r},t)} + \sum_j \Lambda_{ij} \psi_j(\vec{r},t),\tag{3}\label{eq:fictitious}
\end{align}</span></p>
<p>where <span class="math-container">$\Lambda_{ij}$</span> is a matrix of Lagrange multipliers to allow satisfaction of the constraint that the wavefunctions <span class="math-container">$\psi_i$</span> must be orthogonal; and <span class="math-container">$E[\{\psi_i\},\vec{r}]$</span> is an energy functional (usually a Kohn–Sham energy one). For several nucleii, just make a new subscript for <span class="math-container">$\vec{r}$</span> and change the functional to <span class="math-container">$E[\{\psi_i\},\{\vec{r_I}\}]$</span>, then the equations are exactly the same.</p>
<p><strong>Implemented in:</strong></p>
<ul>
<li><a href="https://github.com/CPMD-code/CPMD" rel="nofollow noreferrer">CPMD</a>: Literally named after the method! (open-source)</li>
<li><a href="https://www.cp2k.org/about" rel="nofollow noreferrer">CP2K</a>: Might also be named after the method! (open-source)</li>
<li><a href="http://www.nwchem-sw.org/" rel="nofollow noreferrer">NWChem</a> (open-source)</li>
</ul>


================================================================================

Question: What are the great unsolved questions in Matter Modeling?
Body: <p>This is inspired from an amazingly successful question on Operations Research Stack Exchange: <a href="https://or.stackexchange.com/questions/928/great-unsolved-problems-in-o-r">What are the great unsolved problems in operations research?</a></p>
<hr>
<h3>Wikipedia has some huge lists of:</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_physics" rel="nofollow noreferrer"><strong>Unsolved problems in physics</strong></a></li>
<li><a href="https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_chemistry" rel="nofollow noreferrer"><strong>Unsolved problems in chemistry</strong></a></li>
</ul>
<p><span class="math-container">$\star$</span> But neither of them even mention the fact that the universal functonal in DFT is unknown! <span class="math-container">$\star$</span></p>
<hr>
<h3>Some great problems (not in the above lists!) were discussed in these answers:</h3>
<ul>
<li>One of the first questions ever asked on this site was: &quot;<strong><a href="https://mattermodeling.stackexchange.com/q/2/5">What is the closest thing we have to &quot;the&quot; universal density functional?</a></strong>&quot; </li>
<li><strong><a href="https://mattermodeling.stackexchange.com/a/555/5">We can't yet theoretically predict the ground state hyperfine splitting of the H atom as accurately as we can measure it</a>.</strong></li>
<li><strong><a href="https://quantumcomputing.stackexchange.com/a/2019/2293">What is the &quot;engineering complexity&quot; for building a universal quantum computer?</a></strong></li>
<li><strong><a href="https://quantumcomputing.stackexchange.com/q/2054/2293">Have anyons been confirmed to exist?</a></strong> (this one's for you <a href="https://mattermodeling.stackexchange.com/users/671/anyon">Anyon</a>!)</li>
</ul>
<hr>
<h3>Some great problems (not in either of the above lists, as far as I know!) are here:</h3>
<ul>
<li><strong>Finding a multi-electron relativistic and quantum mechanical method</strong>: 
<ul>
<li>the <strong>Schrödinger equation</strong> is <em>non-relativistic</em>,</li>
<li>the <strong>Klein-Gordon equation</strong> is relativistic but only works for <em>spinless particles</em>,</li>
<li>the <strong>Dirac equation</strong> is a <em>1-electron equation</em> and only <em>approximates QM</em> to 1st order in <span class="math-container">$\alpha$</span>,</li>
<li>the <strong>Dirac-Coulomb-Breit equation</strong> involves interacting electrons but is not invariant with respect to Lorentz transformations (it is <em>no longer properly relativistic</em>) and like the Dirac equation <em>it is not properly quantum mechanical either</em> since it is derive from first-order perturbation theory in the fine structure constant <span class="math-container">$\alpha$</span>!</li>
<li><span class="math-container">$\therefore$</span> <strong>There is no multi-electron, relativistic, QM equation</strong> like the above four for single e-.</li>
</ul>
</li>
<li><strong><a href="https://mattermodeling.stackexchange.com/a/1593/5">High temperature superconductivity</a></strong>: For low-temperatures we have BCS theory, but for high-temperature superconductors we cannot even predict <span class="math-container">$T_c$</span> (the critical temperature).</li>
<li>How to get <strong>multi-reference coupled cluster</strong> working well like CCSD(T) for single-reference?</li>
<li>Can we come up with a black-box multi-reference method like CCSD(T) for single-reference?</li>
<li>Is there a robust way to <strong>automatically select active spaces</strong>?</li>
<li>How best to reach the <strong>CBS limit</strong> for post-SCF methods? How to solve the cusp problem?</li>
<li>How to go <strong>beyond Gaussian orbitals</strong>, and remain efficient?</li>
<li>Can a quantum computer demonstrate beating a classical computer in the modeling of matter?</li>
</ul>
<p>Can you explain any of these, or perhaps discuss the most recent progress, in up to 3 paragraphs?</p>
<hr>
<p><strong>What are some other unsolved problems in the computational / theoretical study of matter, and can you explain them in up to 3 paragraphs?</strong></p>
<hr>

Best Answer: <h2>High-temperature superconductivity</h2>
<p><a href="https://i.sstatic.net/2Iu7g.jpg" rel="noreferrer"><img src="https://i.sstatic.net/2Iu7g.jpg" alt="High-Tc superconductor levitating above a magnet" /></a></p>
<p><a href="https://en.wikipedia.org/wiki/Superconductivity" rel="noreferrer">Superconductivity</a> is a fascinating macroscopic quantum phenomenon in which, as some material is cooled below a critical temperature, its electrical resistance abruptly vanishes. A superconductor can also expel magnetic flux, which allows levitation effects as shown in the picture above. The conventional form of superconductivity was first discovered in Mercury in 1911 by Heike Kamerlingh Onnes, but it took until 1957 for the microscopic Bardeen-Cooper-Schrieffer (BCS) theory to explain its origin. In short, electrons form bound states called Cooper pairs, due to an effective <em>attractive</em> interaction mediated by phonons. However, there is a less conventional, less understood cousin known as high-temperature superconductivity, or high-<span class="math-container">$T_c$</span> superconductivity.</p>
<p><a href="https://en.wikipedia.org/wiki/High-temperature_superconductivity" rel="noreferrer">It</a> is mentioned both on Wikipedia's <a href="https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_physics" rel="noreferrer">unsolved problems in physics page</a> and on the <a href="https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_chemistry" rel="noreferrer">unsolved problems in chemistry page</a>, but it equally applies to the study of matter. Since the <a href="https://link.springer.com/article/10.1007%2FBF01303701" rel="noreferrer">1986 discovery by Bednorz and Müller of superconductivity in a copper oxide</a>, with a transition temperature of <span class="math-container">$35$</span> K (high for superconductors!), there's been an immense amount of experimental, computational and theoretical activity in the field. The goals are manifold, including finding a room temperature superconductor, and to understand the mechanism. Often these systems are very complex, formed from multi-layered crystals, and involve some degree of doping and electron-electron interactions, making their modeling a complex task indeed.</p>
<p>Promising computational avenues include accurate simulations of model Hamiltonians (e.g. Hubbard Hamiltonians) in an effort to find the mechanism, and the ongoing development of suitable <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.99.245155" rel="noreferrer"><em>ab initio</em> methods</a> to model these systems. At this point, I personally think that such approaches represent the most likely path to understanding these materials, barring some breakthrough. However, that doesn't mean progress has stopped elsewhere. For example, additional clues keep coming in from experiments establishing <a href="https://www.nature.com/articles/s41563-019-0585-z" rel="noreferrer">new classes of superconducting materials</a>, and surprising <a href="https://www.nature.com/articles/s41586-019-1375-0" rel="noreferrer">transport properties</a>.</p>


================================================================================

Question: Tools for high-throughput DFT studies?
Body: <p><a href="https://en.wikipedia.org/wiki/High-throughput_screening" rel="noreferrer">High-throughput</a> density functional theory (DFT) calculations are used to screen for new materials and conduct fundamental research in materials science and materials innovation. It involves computations on tens of thousands of compounds, and such a scale demands unique calculation and data management methodologies<span class="math-container">$^1$</span>.</p>
<p><strong>What are the different tools available for conducting high throughput density functional theory calculations?</strong></p>
<p><strong>References</strong></p>
<ol>
<li>Jain, Anubhav, et al. &quot;A high-throughput infrastructure for density functional theory calculations.&quot; Computational Materials Science 50.8 (2011): 2295-2310.</li>
</ol>

Best Answer: <p>As <a href="https://mattermodeling.stackexchange.com/users/979/user979">previously stated</a>, arguably the most mature and widely used set of tools is currently a combination of <a href="https://github.com/materialsproject/pymatgen" rel="noreferrer">Pymatgen</a>, <a href="https://github.com/materialsproject/fireworks" rel="noreferrer">FireWorks</a>, <a href="https://github.com/materialsproject/custodian" rel="noreferrer">Custodian</a>, and <a href="https://github.com/hackingmaterials/atomate" rel="noreferrer">Atomate</a> (which is built upon the prior three Python packages). These tools were constructed as part of the <a href="https://materialsproject.org/" rel="noreferrer">Materials Project</a> but have seen uses in other high-throughput DFT studies.</p>
<p>Another general workflow package for automating high-throughput DFT calculations is <a href="https://www.sciencedirect.com/science/article/pii/S0927025612000717" rel="noreferrer">AFLOW</a>, which has been used in constructing the <a href="http://aflowlib.org/" rel="noreferrer">AFLOWlib</a> repository.</p>
<p>A similar package is <a href="https://github.com/wolverton-research-group/qmpy" rel="noreferrer">qmpy</a>, which has been used in constructing the <a href="http://oqmd.org/" rel="noreferrer">Open Quantum Materials Database</a>. As you can see, with each new database of DFT-computed properties, there are often specific workflow packages associated with them (typically because everyone's preferences and use-cases are different).</p>
<p>One of the benefits of the <a href="http://www.aiida.net/" rel="noreferrer">AiiDA</a> package mentioned in the prior answer here is that it retains the calculation history of the entire workflow. Seeing as most robust workflows are somewhat dynamic in their settings (e.g. if an error appears, settings are changed), this can be useful for ensuring full transparency and reproducibility. AiiDA is what powers the data on the <a href="https://www.materialscloud.org/" rel="noreferrer">Materials Cloud</a>.</p>
<p>There are also many field-specific packages that attempt to automate workflows specific to that field, oftentimes using one or more of the aforementioned packages. For instance, the <a href="https://github.com/ulissigroup/GASpy" rel="noreferrer">Generalized Adsorption Simulation for Python (GASpy)</a> code by the Ulissi group is well-suited for automating DFT calculations of inorganic surfaces, as outlined <a href="https://pubs.acs.org/doi/10.1021/acs.jcim.8b00386" rel="noreferrer">here</a>. Rosen et al. have also developed a <a href="https://github.com/arosen93/mof_screen" rel="noreferrer">workflow</a> for automating DFT calculations of metal–organic frameworks, as described <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.25787" rel="noreferrer">here</a>. Yet another is the <a href="https://mast.readthedocs.io/en/dev/" rel="noreferrer">MAterials Simulation Toolkit (MAST)</a>, which allows users to build up &quot;recipes&quot; for automated workflows in a similar manner as Atomate and was originally developed with a focus on simulating defects and diffusion in solids, as described <a href="https://www.sciencedirect.com/science/article/pii/S0927025616304591" rel="noreferrer">here</a>.</p>
<p>Edit #1: A new package, named [AMP<span class="math-container">$^2$</span>], was <a href="https://www.sciencedirect.com/science/article/pii/S0010465520302113" rel="noreferrer">just published</a> for automating DFT calculations of crystals. It looks like it has several ease-of-use features, such as robust default settings for commonly computed properties, automatically testing if a hybrid functional should be used, and an algorithm to identify complex magnetic orderings in an automated fashion. The code is available to download <a href="https://data.mendeley.com/datasets/5rdw9jv5vp/1" rel="noreferrer">here</a>.</p>
<p>Edit #2: I somehow forgot to mention that the <a href="https://wiki.fysik.dtu.dk/ase/" rel="noreferrer">Atomic Simulation Environment (ASE)</a>, which some of the prior toolkits use, is an extremely useful and flexible resource that can be used to construct your own workflows with some Python scripting. It's not specifically meant for high-throughput but can be used that way.</p>


================================================================================

Question: k-points and ENCUT convergence tests before or after relaxation?
Body: <p>Let's say I want to relax a structure using VASP. K-points and ENCUT convergence tests must be conducted <strong><code>before or after relaxation</code></strong>? and which tags should be included inside the <code>INCAR</code> file during convergence tests?</p>

Best Answer: <p>I do generally follow the follwoing:</p>
<ol>
<li><p>Keep basic tags EDIFF, EDIFF to 1E-07 and 1E-06 (or -0.005) generally. i do use defaults if i want to do a faster run just for checking.  and i dont know how can we preempt or know the potential surface is flat or not, as mentioned by Rosen. As in his answer said&quot;...However, if the potential energy surface is flat, this may lead to issues converging the structure to the local minimum because of numerically inaccurate forces.....&quot;</p>
</li>
<li><p>for KPOINTS i use 30 to 40 times the reciprocal lattice parameter. (Eg.  if a=3, b=4, c=6, then KPOINTS be 40/a, 40/b, 40/c). Not known the origin of such thumb rule !!! :). If its slab, surface or 2D material, then the Z-direction KPOINT will be 1.</p>
</li>
<li><p>i avoid using any other tags, except ISMEAR, ISYM and run static calculations for ENCUT and KPOINTS convergence.
Still i face convergence issues.</p>
</li>
</ol>
<p>but I did not face such issues so far in QE (just a basic learner now)
Regards</p>


================================================================================

Question: Solving the Schr&#246;dinger equation as a service?
Body: <p>Does anyone know of a company with a cloud-based service which takes some representation of a molecule as an input, and then somehow solves the Schrödinger equation to some level of accuracy and reasonable time frame, then spits out the energy levels of orbitals as outputs?</p>
<p>My guess is that no company does this today, but I might be wrong.</p>

Best Answer: <h1>SciCalQ</h1>
<p>The company does more than solving the Schroedinger equation. From its <a href="https://www.scicalq.com/computational-chemistry-service/" rel="noreferrer">site</a>:</p>
<blockquote>
<p>SciCalQ’s solid background in computational chemistry and quantum physics own extensive experiences in solving practical and academic problems with scientific computing.</p>
</blockquote>
<p>They do:</p>
<h3>Density Functional Theory (DFT) Computation</h3>
<ul>
<li>Geometric Configuration: Bond length, bond angle, lattice parameters, stable configuration, surface reconstruction, defects, vacancies, active sites, etc.</li>
<li>Electronic Properties: HOMO/LUMO , energy band structure, conduction band, valence band position, DOS, electron cloud density, carrier mobility, Fermi level, etc.</li>
</ul>
<h3>Molecular dynamics (MD)</h3>
<ul>
<li>Force Field for MD: Force field describes the interactions in/between moleculars in molecular dynamics simulations. We can obtain accurate force field for users.</li>
<li>Binding Free Energy: Predicting the binding free energy of ligands to macromolecules can have great practical values in drugs discovery. We rovide one-stop service to finish this prediction.</li>
<li>Bio-related Simulation: Polysaccharide molecular docking, molecular surface calculation, biological enzyme catalysis calculation, protein interaction analysis, transmembrane transport mechanism calculation, etc.</li>
</ul>
<p>For ordering, you need to fill a web form where you:</p>
<ul>
<li>Discuss one by one according to the system and properties you need to calculate.</li>
<li>Describe the system and intermediate process you need to calculate in detail (may be conjecture), and provide relevant literature.</li>
</ul>


================================================================================

Question: How to master Fortran with minimal effort?
Body: <p>Fortran language is used in many first-principles matter modeling codes, such as <code>VASP</code> and <code>QE</code>. These codes usually include many files and thousand of lines and are not readable for a newbie. Are there good strategies and resources to master the Fortran language with minimal effort if I have some experience with the <code>PYTHON</code> language? The final purpose is to read and modify the existing Fortran codes such as <code>QE</code>.</p>

Best Answer: <blockquote>
<p>&quot;How to master Fortran programing with minimal effort?&quot;</p>
</blockquote>
<p>The concept of &quot;mastering&quot; a programming language is a bit subjective, and people typically don't &quot;master&quot; a language with &quot;minimal effort&quot;.</p>
<p>However as a Fortran programmer myself, there's two strategies I can recommend:</p>
<p><strong>(1) Tutorials:</strong> As with any programming language, there's plenty of books and tutorials for Fortran, and some of these come with exercises you can do to help you learn the language. Here's some of them:</p>
<ul>
<li><a href="https://ourcodingclub.github.io/tutorials/fortran-intro/" rel="noreferrer">Our coding club Fortran intro</a></li>
<li><a href="https://fortran-lang.org/learn/" rel="noreferrer">Fortran-lang.org</a> (there's a multitude of links to other tutorials and books there!)</li>
<li><a href="https://www.fortrantutorial.com/" rel="noreferrer">FORTRANtutorial.com</a> (a 7 lesson course)</li>
<li><a href="https://www.youtube.com/watch?v=__2UgFNYgf8" rel="noreferrer">Fortran tutorial youtube video</a> (for those that like to listen rather than read)</li>
</ul>
<p><strong>(2) Practicing:</strong> As the <a href="https://mattermodeling.stackexchange.com/questions/6252/how-to-master-fortran-programing-with-minimal-effort#comment12552_6252">comment by user tmph says</a> the best way is often just to practice. When I began Fortran as an undergrad summer student, I was told ahead of time that I'd be coding in Fortran, and I dreaded it.  I went to the library and picked up a bunch of books (online tutorials and youtube videos were unheard of at the time), thinking that I'd learn based on strategy #1 above. It turns out though that I learned much faster &quot;by doing&quot; rather than by reading.</p>
<p>I recommend playing around with existing Fortran code, and adding simple features with gradually increasing complexity. Changing the number of digits for a numerical output in QE is an easy thing that I think you can try doing right now! Next you can add more output (more words, more numbers, etc.) You could also try to add new input flags for the feature you want to implement, and at every step of the way, see if it compiles, and <em>follow the error messages at compile time: I learned so much Fortran that way!</em> <strong>Very soon</strong> you'll find yourself feeling quite comfortable and you'll gain your confidence this way!</p>
<p>I find Fortran to actually be in many respects, simpler and easier to master than Python, especially since <em>most</em> Fortran code is not object-oriented, and you'll see that it's really <em>designed</em> for scientific computing (the way you can output numerical values for example, will probably convince you of this). Python was not originally designed for number crunching, and neither was C or C++ or Java: these languages were designed to be more general, and are used for writing everything from operating systems, to GUIs, to web applications, to computer games, etc., whereas Fortran was literally designed for number crunching (a bit like MATLAB and Julia, the latter which was discussed <a href="https://mattermodeling.stackexchange.com/q/501/5">here at MMSE</a>) so once you spend some time familiarizing yourself with it, I'm confident that you'll find it much more natural for scientific computing than other languages you've used in the past.</p>


================================================================================

Question: Why were no correlation consistent basis sets for potassium contracted for a non-relativistic Hamiltonian?
Body: <p>On <a href="https://www.basissetexchange.org/" rel="noreferrer">basis set exchange</a>, the only correlation consistent (cc)  basis sets for potassium, are relativistic (labelled by X2C). Very recently (I consider 2017 quite recent, because most Dunning basis sets date back to the late 1980s) the <a href="http://www.grant-hill.group.shef.ac.uk/ccrepo/potassium/kbasis.php" rel="noreferrer">ccRepo</a> has provided a basis set for potassium that "was constructed by re-contracting the DK (relativistic Douglas-Kroll) set for a non-relativistic Hamiltonian". </p>

<p><strong>I am curious why there was never any basis set for potassium that was originally contracted for a non-relativistic Hamiltonian?</strong> </p>

<p>I understand that potassium is not a light element, however non-relativistic cc basis sets have existed all along for the elements all the way up to krypton, which is much heavier than potassium. Non-relativistic cc basis sets have even been made recently by the group of Angela Wilson, up to xenon, which is almost triple as heavy (so in some sense, triple as relativistic) as potassium.</p>

Best Answer: <p>The answer in part is that not all the elements up through <span class="math-container">$\ce{Xe}$</span> have (or had until fairly recently) a nonrelativistic correlation consistent basis set. Specifically, the alkali and alkaline earth elements (columns 1 and 2 of the periodic table) have been much slower to get nonrelativistic basis sets. The cc-pVnZ basis sets for <span class="math-container">$\ce{Li}, \text{ }\ce{Be}, \text{ }\ce{Na}, \text{ and } \ce{Mg}$</span> were only published in 2011.<sup>[1]</sup> As you mentioned, <span class="math-container">$\ce{K}$</span> was very recent, but <span class="math-container">$\ce{Ca}$</span> only got cc-pVnZ basis sets in 2002.<sup>[2]</sup> Neither <span class="math-container">$\ce{Rb}$</span> nor <span class="math-container">$\ce{Sr}$</span> have a nonrelativistic cc-basis set. This discrepancy was mentioned in a recent article<sup>[3]</sup> that defined relativistic cc-basis for the alkali and alkaline earth metals, it was not further explained.</p>
<p>As to why these took so long, it may just have been a lack of interest in these atoms. Both the p-block and transition metal elements are widely studied in chemistry, but the alkali and alkaline earth metals have generally not been as studied (at least, not in contexts requiring insight from electronic structure theory). The previously discussed <span class="math-container">$\ce{Ca}$</span> basis was made specifically so they could better characterize the potential energy surface of a compound that had seen limited experimental or theoretical interest. So it doesn't seem to be due to any inherent difficulty in making basis sets for these atoms. Rather they seemed to just hold off until there enough potential applications to warrant it.</p>
<h3>References:</h3>
<p>[1] Prascher, B.P., Woon, D.E., Peterson, K.A. et al. Gaussian basis sets for use in correlated molecular calculations. VII. Valence, core-valence, and scalar relativistic basis sets for Li, Be, Na, and Mg. <em>Theor Chem Acc</em> 128, 69–82 (<strong>2011</strong>). <a href="https://doi.org/10.1007/s00214-010-0764-0" rel="noreferrer">DOI: 10.1007/s00214-010-0764-0</a></p>
<p>[2] Koput, J. &amp; Peterson K.A. <em>J. Phys. Chem. A</em> <strong>2002</strong>, <em>106</em>, 41, 9595-9599 <a href="https://doi.org/10.1021/jp026283u" rel="noreferrer">DOI:10.1021/jp026283u</a></p>
<p>[3] Hill, J.G. &amp;  Peterson K.A. Gaussian basis sets for use in correlated molecular calculations. XI. Pseudopotential-based and all-electron relativistic basis sets for alkali metal (K–Fr) and alkaline earth (Ca–Ra) elements
<em>J. Chem. Phys.</em> 147, 244106 (<strong>2017</strong>); <a href="https://doi.org/10.1063/1.5010587" rel="noreferrer">DOI: 10.1063/1.5010587</a></p>


================================================================================

Question: Are plane-wave basis sets reliable for modeling adsorption processes?
Body: <p>Plane-wave basis sets are useful in the calculation of periodic systems and can be used in combination with pseudopotentials. However, when dealing with surfaces the system is no longer periodic in, at least, one dimension. Moreover, in modeling adsorption processes, it is necessary to include a surface and the molecule attached with a suitable basis set.</p>

<p>As far as I know, a plane-wave basis set should be expanded over the whole simulation box and the periodicity in three dimensions is required. However, some studies on adsorption were made using this type of basis (for instance, <a href="https://pubs.acs.org/doi/10.1021/jp811381d" rel="noreferrer"><em>J. Phys. Chem. <strong>2009</strong>, 113, 9256-9274</em></a>), although the periodicity is broken. At first glance, it seems like a localized basis set is better for surfaces with molecules attached.</p>

<ol>
<li>Are plane-waves basis sets really useful for surfaces or adsorption processes? </li>
<li>What advantages (if there are) have plane-waves over localized basis sets in modeling surfaces?</li>
</ol>

Best Answer: <p>In the example you highlighted and indeed in most plane-wave DFT codes, there is periodicity in all three dimensions including for surface slab calculations. In the case of a surface slab, vacuum space is commonly added in the <span class="math-container">$z$</span> dimension. The vacuum space is there so that an adsorbate can bind of course, but it's also there because of the boundary conditions. A vacuum space ensures that the adsorbate-slab complex does not interact with itself over the periodic boundary, provided the vacuum space is large enough. In this way, you're modeling what is effectively a 2D system while still having 3D periodic boundary conditions as part of the DFT calculation. If an interactive example would be helpful, I recommend John Kitchin's <a href="http://kitchingroup.cheme.cmu.edu/dft-book/dft.html" rel="noreferrer">DFT ebook</a>, specifically Section 5.</p>

<p>The answer to both your questions is actually, for the most part, one in the same. The reason plane-wave basis sets are so useful is how well they lend themselves to periodic DFT calculations. In this case, you can represent a crystalline or bulk system with much fewer atoms than you would be able to do if you relied on a Gaussian basis set. A given metal might have a primitive unit cell of only a few atoms, whereas modeling the same metal with the same degree of accuracy without finite boundary conditions might require several hundred atoms to prevent edge effects and ensure the system is large enough. A localized, Gaussian basis set is not inherently better than a plane-wave basis set. The latter, however, is naturally better suited for adsorption problems in general. </p>

<p>As a side-note, because it can be helpful for to visualize orbitals in a Gaussian basis set, there are several algorithms that project plane-wave bands to Gaussian type orbitals, such as the <a href="https://github.com/jrschmidt2/periodic-NBO" rel="noreferrer">periodic NBO code</a> of Dunnington and Schmidt. This is mostly for gaining insight into the electronic structure of the chemical process, rather than a question of accuracy.</p>


================================================================================

Question: Generating special quasirandom structures (SQS) for surface calculations?
Body: <p>Given that most of the available codes for performing DFT calculations work with periodic boundary conditions, there are tools like <code>mcsqs</code> as a part of the Alloy Theoretic Automated Toolkit (<a href="https://www.brown.edu/Departments/Engineering/Labs/avdw/atat/" rel="noreferrer">ATAT</a>) that enable generation of SQS for modelling materials with configurational disorder. Inherently, these methods of generating SQS rely on the periodicity of the supercell in all directions while generating the SQS. </p>

<p>In simulations where we explicitly reduce the effect of the periodic images in one or more dimensions (like when modelling surfaces), what are the points to keep in mind to ensure that the SQS are generated and used correctly?  </p>

Best Answer: <p>First of all, it's better to define the terms here because probably not all the people here are familiar with SQS right away. SQS is an abbreviation for special quasirandom structures. The illuminating reference here is the work of <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.65.353" rel="noreferrer">Zunger et. al.</a> that basically show you that the most relevant parameter to consider when you want if SQS truly resembles a random alloy or not is <strong>radial distribution function</strong> that basically tells you how the atoms are distributed in the space.</p>

<p><span class="math-container">$$$$</span></p>


================================================================================

Question: QMC calculation of the equation of state of metals
Body: <p>Is diffusion/variational quantum Monte Carlo applicable to the calculation of the athermal (zero temperature, no zero point energy) equation of state of metals including electronic correlation? For instance, would it work to compute the total free energy (again, at <span class="math-container">$\pu{0}{K}$</span>) of gold versus its unit cell volume?</p>

Best Answer: <p>Given that variational quantum Monte Carlo (VQMC) is specifically for calculating ground state properties, it shouldn't have any problem with finding the <span class="math-container">$T=0$</span> energy. </p>

<p>It may be difficult to do a material as complicated as gold, but I'm not an expert in this field by any means. This article discusses structural optimization with VQMC: 
<a href="https://doi.org/10.1063/1.466885" rel="noreferrer">S. Tanaka J. Chem. Phys. <strong>100</strong>, 7416 (1994)</a></p>


================================================================================

Question: What are some materials discovered from first-principles calculations that ended up being incorporated in a commercial product?
Body: <p>High-throughput computational screening, inverse materials design, and advances in machine learning have really accelerated the pace in which we can identify novel materials for a wide range of applications, at least in an academic setting. What are some examples where the proposed material, or a clear variant of it, has been developed and used in the industry?</p>

Best Answer: <p>When you go to the industrial environment, it becomes really difficult to obtain published results due to the fact that if a company finds breakthrough new materials, why they should share it with the public, while they could commercialize it?</p>
<p>I found an example of a company, called <a href="https://exabyte.io/#case-study" rel="nofollow noreferrer">ExaByte</a> that works on a material discovery platform and collaborate with universities and other companies to help them find novel materials for specific applications. I came across this <a href="https://exabyte.docsend.com/view/syitiek" rel="nofollow noreferrer">example</a> that is related to the discovery of new metallic alloys for automotive and aerospace applications. You won't find many details, but overall they studied the stability of new metallic alloys based on enthalpy of formation values extracted from simulations that helped them to develop these new metallic alloys. I hope it helps here.</p>


================================================================================

Question: What tools are used for the preparation of grain boundary models?
Body: <p>What methods and tools are used for building models of grain boundaries?<br>
I mean initial models that are then optimized using molecular dynamics potentials or a different methods.</p>

<p>When I was involved in such things 10-15 years ago we were writing own codes to generate starting structures. Both big systems that emulated nano-sized policrystals</p>

<p><a href="https://i.sstatic.net/16s8t.png" rel="noreferrer"><img src="https://i.sstatic.net/16s8t.png" alt="policrystal"></a></p>

<p>and small bicrystals to analyze particular boundary types</p>

<p><a href="https://i.sstatic.net/wNvSW.png" rel="noreferrer"><img src="https://i.sstatic.net/wNvSW.png" alt="bicrystal"></a></p>

<p>(I added pictures to clarify what type of things I'm writing about)</p>

<p>I wonder what tools (if any) are used nowadays?</p>

Best Answer: <p>If you are looking for generating synthesized microstructures for using it in Phase-Field simulation, the best choice is <a href="http://dream3d.bluequartz.net/" rel="noreferrer">Dream 3D</a>, where you can control the distribution of grains and their crystallographic orientations:</p>

<p><a href="https://i.sstatic.net/JImvs.jpg" rel="noreferrer"><img src="https://i.sstatic.net/JImvs.jpg" alt="enter image description here"></a></p>

<p>If you are looking for a framework to generate synthesized polycrystalline structures for molecular dynamics simulation, the best tool is <a href="https://atomsk.univ-lille.fr/" rel="noreferrer">Atomsk</a>, that is compatible with LAMMPS, DL_POLY, VASP, QuantumEspresso, SIESTA, etc.:</p>

<p><a href="https://i.sstatic.net/WIXtc.jpg" rel="noreferrer"><img src="https://i.sstatic.net/WIXtc.jpg" alt="enter image description here"></a></p>


================================================================================

Question: What are the differences between crystal structure file formats like CIF, XYZ, PDB etc.?
Body: <p>We come across many file formats like CIF, XYZ, PDB, INS, cc1, p1, POSCAR, FDF, struct etc. Why do we have so many file formats to store crystal structure information? Are they redundant? Can someone give a historical perspective and advantages/disadvantages of these file formats?</p>

Best Answer: <p>The <a href="https://en.wikipedia.org/wiki/XYZ_file_format" rel="noreferrer">XYZ</a> format provides a list of atoms with minimal content: element and coordinates. This is often not sufficient, so people need other formats. In particular, XYZ cannot describe a crystal -- it doesn't include unit cell parameters.</p>

<p>The PDB format was designed with proteins and nucleic acids is mind.
The format is old (from 1970's) and popular, so sometimes it's used even for inorganic materials, but it's well fitted only for systems with the model>chain>residue>atom hierarchy.
Additionally, the fixed column format imposes limitations: residue name
can have up to 3 characters, chain name only a single character,
atom number - 5 digits.</p>

<p>The CIF format is a narrowed down derivative of a format called STAR.
Both are used only in structural sciences,
although they provide a general syntax for storing any data,
similarly to XML, JSON, YAML, TOML, etc (and CIF is older than XML!).
Usually, CIF is used together with a dictionary (think XML schema).
It was first used in the small molecule world.
About 20 years ago it was adapted to be the successor of the PDB format, called mmCIF. mmCIF uses CIF 1.1 syntax with a different dictionary.
One visible difference is that mmCIF has dots in tags (<code>_cell.length_a</code> instead of <code>_cell_length_a</code>).</p>

<p>Some software developers designed own file formats for their programs.
So we also have a number of software-specific formats. This is a natural thing.</p>


================================================================================

Question: What are the types of bond orders?
Body: <p><strong>I am studying the interaction of a metal with a nanostructure</strong>. As we can not trust graphical software about the bond formation, I decided to analyze them using the software <a href="http://sobereva.com/multiwfn/" rel="noreferrer">Multiwfn</a>.</p>
<p>It happens that there is a Zoo of several theories about <strong>how to calculate the bond orders</strong>:</p>
<ul>
<li>Mayer bond order analysis (Chem. Phys. Lett, 97, 270 (1983))</li>
<li>Standard multi-center bond order (Struct. Chem., 1, 423 (1990))</li>
<li>Multi-center bond order in natural atomic orbital (NAO) basis</li>
<li><a href="https://mattermodeling.stackexchange.com/a/1469/5">Wiberg bond order</a> analysis in Löwdin orthogonalized basis (Tetrahedron, 24, 1083 (1968), J. Mol. Struct. (THEOCHEM), 870, 1 (2008))</li>
<li>Mulliken bond order analysis</li>
<li>Orbital occupancy-perturbed Mayer bond order (J. Chem. Theory
Comput., 8, 908 (2012))</li>
<li>Fuzzy bond order (Chem. Phys. Lett., 383, 368 (2004))</li>
<li><a href="https://mattermodeling.stackexchange.com/a/1508/5">Laplacian bond order</a> (J. Phys. Chem. A, 117, 3100 (2013))</li>
<li>Intrinsic bond strength index (J. Phys. Chem. A, 124, 1850 (2020))</li>
<li>AV1245 index (approximate multi-center bond order for large
rings) (Phys. Chem. Chem. Phys., 18, 11839 (2016))</li>
<li>DDEC6 atomic population analysis (RSC Adv. 7, 45552 (2017))</li>
</ul>


<p><strong>What method/theory do you suggest to be used?</strong></p>

Best Answer: <h2>Wiberg (1968)</h2>
<p>Let's start with the &quot;classic&quot; bond order paper by Ken Wiberg (born in 1927 and still alive!).</p>
<p>The <strong>Wiberg Bond Index (WBI)</strong> between fragments A and B of AB is calculated as follows:</p>
<p><span class="math-container">\begin{equation}
\tag{1}
W_{AB} \equiv \sum_{\mu \in A}\sum_{\nu \in B}D_{\mu \nu}^2~ ,
\end{equation}</span></p>
<p>where <span class="math-container">$D$</span> is the following density matrix:</p>
<p><span class="math-container">\begin{equation}
\tag{2}
D_{\mu \nu} \equiv \sum_{i\in \alpha }C_{\mu i}C_{\nu i} +  \sum_{i\in \beta }C_{\mu i}C_{\nu i}~,
\end{equation}</span></p>
<p>where <span class="math-container">$\mu$</span> and <span class="math-container">$\nu$</span> are atomic orbitals for fragments A and B respectively, and <span class="math-container">$\alpha$</span> and <span class="math-container">$\beta$</span> denote spins, and <span class="math-container">$C$</span> is the matrix of atomic orbital coefficients in the LCAO formalism.</p>
<p><strong>Cons:</strong></p>
<ul>
<li>It is old and primitive. Easily you could take into account the overlap matrix  <span class="math-container">$S$</span> and the spin density matrix <span class="math-container">$Q$</span> to get the Mayer Bond Index (MBI) or an analogous WBI.</li>
<li>It relies on wavefunctions, which <a href="https://mattermodeling.stackexchange.com/q/1394/5">Walter Kohn has said is not legitimate for large systems</a>.</li>
<li>While this bond order can easily be decomposed into <span class="math-container">$\sigma$</span>, <span class="math-container">$\pi$</span> and <span class="math-container">$\delta$</span> bonding components for linear molecules, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0166128008005058" rel="nofollow noreferrer">some more work may be necessary for more general molecules</a>.</li>
</ul>
<p><strong>Pros:</strong></p>
<ul>
<li>Despite the MBI being objectively more sophisticated, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0166128008005058" rel="nofollow noreferrer">a detailed 2008 study</a> of both MBI and WBI concluded with a recommendation to use WBI and cited two reasons:
<ul>
<li>It was quite <strong>stable with respect to basis set size</strong> (see Fig. 1) and this concluding quote &quot;For the description of bond orders the preference is given to WBI      calculated with NAO, because these indices are stable enough with respect to basis set variation.&quot;</li>
<li>Their WBI results were close to what we might expect: &quot;The valences calculated using the WBI values taking into account both the covalent and ionic components of the chemical bonds are close to the formal atomic valences of main-group elements.&quot;</li>
</ul>
</li>
<li>It's very simple, easy to calculate, and therefore likely available in most software.</li>
</ul>


================================================================================

Question: Materials Modeling with Raspberry Pi?
Body: <p>Does anyone have any knowledge or direction for creating a homemade <a href="https://en.wikipedia.org/wiki/Beowulf_cluster" rel="noreferrer">Beowolf cluster</a> for materials modeling?I would like to be able to run my own &quot;hobby&quot; simulations at home, so a low energy, low capital investment is what I seek.</p>
<p><a href="https://en.wikipedia.org/wiki/Raspberry_Pi" rel="noreferrer">Raspberry Pi</a> and <a href="https://www.hardkernel.com/shop/odroid-mc1-my-cluster-one-with-32-cpu-cores-and-8gb-dram/" rel="noreferrer">ODroid</a> use small, cheap, low energy single board computers (SBC) that can be stacked into a cluster. A demonstration cluster was made in 2013 in the UK <a href="http://www.southampton.ac.uk/%7Esjc/raspberrypi/" rel="noreferrer">Raspberry Pi demonstration cluster</a>. I have not found examples of molecular simulation being applied to these clusters (GROMACS or GAMESS type applications). My concern is that they may not be able to handle long term simulations?</p>
<p><strong>Are there any recent examples of these SBC clusters handling prolonged workload, and in particular, are there examples of molecular simulations being successful?</strong> I have tried reaching out to the SBC community, however, molecular dynamics and quantum chemistry gets me blank stares.</p>
<p>I just found this paper comparing the ODroid-MC1 to supercomputer cluster CPU's. The Odroid-MC1 is a 32 cpu cluster for \$220, and the CPU's compared against retail at \$9000+. The test problem was solving Lattice-Boltzmann flow computations. <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167739X18322015" rel="noreferrer">They found</a> that ODroid-MC1 was only 4 times slower, and this was largely due to its use of 32-bit ARMv7. It can therefore likely be expected that using 64-bit ARMv8 and newer would make ODroid-MC1 very competitive, and incredibly cheap. ODroid consistently beats raspberry pi for speed.</p>

Best Answer: <p>Raspberry Pi clusters are okay for studying networked systems and job schedulers, but bad for any real calculations. There are several problems: there's very little memory per CPU, the interconnect is slow, having local disk is hard... but worst of all, the bang per buck is very low, see e.g. <a href="https://www.phoronix.com/scan.php?page=article&amp;item=raspberry-pi-burst&amp;num=6" rel="nofollow noreferrer">a Phoronix benchmark</a>.
So, in summary: Intel/AMD is still cheaper for running actual calculations, but a Raspberry Pi cluster is quite cheap to set up so it could serve as a toy / test system.</p>
<p>More powerful ARM workstations that are actually designed for running calculations (many CPUs, large memory, local storage) could be a game changer, but these are still not common.</p>
<p>Edit: I've just benchmarked a Raspberry Pi 4. According to linpack, it's about the same speed as a cheap celeron laptop; however, it's terribly slow in desktop use. Both are at least 10 times less powerful than my work laptop; I'd estimate that the raspberry pis are at least 3-4 times more expensive per gflop than good PCs at the moment.</p>


================================================================================

Question: Why can&#39;t I reproduce the behavior of an H-saturated graphene flake?
Body: <p>I'm trying to simulate a graphene flake with its edge C atoms saturated by H atoms, in a temperature ramp from 300 K to 1600 K, using LAMMPS and the AIREBO potential proposed by Stuart et al. in 2000.</p>
<p><strong>The problem is when the temperature is close to 500 K the C-H bonds start breaking and at 1600 K all of the H atoms are unbonded.</strong> It also occurs if I use a bilayer graphene flake even if it is two times bigger than the here shown.</p>
<p><strong>What do you think about that?</strong></p>
<p>My input card is below. It looks very similar to the example included in the LAMMPS folder, so I don't know why is this happening.</p>
<p>Thanks a lot in advance!</p>
<p>Pic1: simulation at 300 K
Pic2: simulation at 1600 K (H atoms are randomly distributed in the box)</p>
<pre><code>################
# graphene n14_l1                              
################

units           metal
boundary        p p p
atom_style      atomic
read_data       mainFlake_n14_l1.dat
#read_restart   name.restart

mass        1 12.01
mass            2 1.008

pair_style      airebo 3.0 1 1
pair_coeff      * * CH.airebo C H

neighbor        0.3 bin
neigh_modify    delay 10

#---------------------------------------- relax
timestep        0.0005  # 0.5 fs
thermo          20
dump            1 all custom 2 min.lammpstrj id element x y z
minimize        1.0e-10 1.0e-11 10000 100000

reset_timestep  0
undump          1

#---------------------------------------- MD
timestep        0.002   # 2 fs
velocity        all create 273.00 32345 rot yes mom yes dist gaussian
fix     1 all nvt temp 273.0 1600.0 0.01
thermo          1000
thermo_style    custom step temp pe ke etotal
dump            1 all custom 200 md.lammpstrj id element x y z
dump_modify     1 element C H
restart         20000 restart1 restart2

#---------------------------------------- Run
run             2500000    # 5 ns
</code></pre>
<p><a href="https://i.sstatic.net/DHsNk.png" rel="noreferrer"><img src="https://i.sstatic.net/DHsNk.png" alt="pic1" /></a>
<a href="https://i.sstatic.net/mPSC4.png" rel="noreferrer"><img src="https://i.sstatic.net/mPSC4.png" alt="pi2" /></a></p>

Best Answer: <p>From the available information, I think it is caused by the small value of <span class="math-container">$T_{damp}$</span>. This causes the <span class="math-container">$T$</span> to fluctuate wildly, which can induce unwanted bond-breaking. Best practise is to keep <span class="math-container">$T_{damp}$</span> value around <span class="math-container">$100 \times \text{timestep}$</span>.</p>
<p>Also, I will suggest you use a much larger neighbour bin size (<span class="math-container">$\approx 3 Å $</span>).</p>


================================================================================

Question: Semi-infinite surfaces for adsorption: a valid approach?
Body: <p>I'm currently trying to surface reaction of small molecules on metal oxides in VASP. Several papers I've read have approached surface energy calculations in a variety of ways. I first began looking at how surface energies are obtained without any adsorbates considered. This seemed like a logical starting point since I assumed the surface energy would be calculated via <span class="math-container">$E(\text{Surface+Adsorbate})-(E(\text{Surface})+E(\text{Adsorbate}))$</span>, so I would need to find the clean surface energy to start with.</p>
<p>The best approach to reach a converged seems to have received some debate. Fiorentini and Methfessel [1] finds that the widely used expression:</p>
<p><span class="math-container">$$\sigma=\lim_{N\to\infty}\frac{1}{2}(E_\text{slab}^N-NE_\text{bulk})\tag{1}$$</span></p>
<p>is poor at reaching a stable converged surface energy, where N represents the number of slab layers. Instead they find the expression (linear fit to slab energies):</p>
<p><span class="math-container">$$E_\text{slab}^N\approx2\sigma+NE_\text{bulk}\tag{2}$$</span></p>
<p>reaches a stable convergence. A later study [2] finds the first expression to be adequate at reaching surface energies only when large enough k-point set is used.</p>
<p>When it comes to adsorption energies on surfaces I mostly find that researchers [3] [4] approach the calculations using a semi-infinite slab  where the top layers are allowed to relax and an arbitrary 1 or 2 layers are frozen below. However, I've yet to find a critical evaluation of such approach. Is it worth performing a series of convergence tests on the number of layers frozen, as well as the number of layers themselves? I can imagine this would become quite time consuming.</p>
<p>Alternatively, I have seen others suggest that a better approach is using a symmetrical slab model. That is, putting the same adsorbate on  the &quot;bottom&quot; side of the slab in exactly the same geometry as the top. Again, I would like to hear people's thoughts on this choice of method, and whether this approach has more 'validity' than the semi-infinite approach. Any paper recommendations welcome to, I found the Fiorentini and Methfessel paper in a discussion on the VASP forum.</p>
<ol>
<li><p>Fiorentini, V., &amp; Methfessel, M. (1996). Extracting convergent surface energies from slab calculations. Journal of Physics Condensed Matter, 8(36), 6525–6529.</p>
</li>
<li><p>Da Silva, J. L. F., Stampfl, C., &amp; Scheffler, M. (2006). Converged properties of clean metal surfaces by all-electron first-principles calculations. Surface Science, 600(3), 703–715.</p>
</li>
<li><p>Lischka, M., &amp; Groß, A. (2003). Hydrogen on palladium: a model system for the interaction of atoms and molecules with metal surfaces. Recent Developments in Vacuum Science and Technology, 661(2), 111–132.</p>
</li>
<li><p>Mamun, O., Winther, K. T., Boes, J. R., &amp; Bligaard, T. (2019). High-throughput calculations of catalytic properties of bimetallic alloy surfaces. Scientific Data, 6(1), 1–9.</p>
</li>
</ol>

Best Answer: <blockquote>
<p>Is it worth performing a series of convergence tests on the number of layers frozen, as well as the number of layers themselves? I can imagine this would become quite time consuming.</p>
</blockquote>
<p>Yes, it is generally considered a valid approach, and if you are new to a given material, you should be running such convergence tests both with regards to the number of layers in the slab and the number of layers allowed to move. Of course, you have to keep some layers fixed otherwise it will not be representative of a bulk surface. At the same time, it can be too rigid if you only let the top layer move. For people that work in this field, they often have a good handle on how many layers to use based on prior work, so you will likely not see a convergence test in every paper. However, it is best practice. As a side-note, for high-throughput calculations (such as the <em>Sci. Data</em> paper you mentioned), there is a more significant balance that needs to be had with regards to the accuracy-cost tradeoff since they are purposefully trying to study as many materials as possible.</p>
<blockquote>
<p>Alternatively, I have seen others suggest that a better approach is using a symmetrical slab model. That is, putting the same adsorbate on the &quot;bottom&quot; side of the slab in exactly the same geometry as the top. Again, I would like to hear people's thoughts on this choice of method, and whether this approach has more 'validity' than the semi-infinite approach.</p>
</blockquote>
<p>Generally, the reason for doing this is to avoid the presence of a fictitious dipole moment in your slab. It is a separate issue from the number of layers question you asked. My view is that this is a bit of an archaic route. In VASP and other codes, you can use a dipole correction to offset any dipole present in an asymmetrical slab model. Refer to the <a href="https://www.vasp.at/wiki/index.php/IDIPOL" rel="noreferrer">IDIPOL</a> flag for more details.</p>
<p>In general, the answers to many of your questions are discussed in the &quot;Modeling materials using density functional theory&quot; ebook by Kitchin, found <a href="http://kitchingroup.cheme.cmu.edu/dft-book/dft.html#orgheadline43" rel="noreferrer">here</a>.</p>


================================================================================

Question: What are the pitfalls for new users of DFT?
Body: <p>This question is inspired from a <a href="https://mathematica.stackexchange.com/q/18393">post</a> in another SE. Many users these days use density functional theory codes as 'black boxes' and hence its natural to expect that they would have made many mistakes when first learning.</p>
<p><strong>What are the common pitfalls encountered by new users of density functional theory?</strong></p>

Best Answer: <p>Well, the first pitfall would be <strong>assuming that density functional theory calculations are non-ambiguous</strong>, when the reality is that density functional theory can be implemented in a number of different ways ;)</p>
<p>There are a multitude of ways one can screw up by not setting up the calculation properly:</p>
<ul>
<li><strong>insufficent accuracy of quadrature</strong></li>
<li><strong>insufficient numerical representation</strong> (basis set in general, k-point sampling in periodic calculations)</li>
<li><strong>non-applicability of the model</strong> e.g. problems with strong correlation, etc.</li>
</ul>
<p>It is hard to make anything idiotproof, since idiots tend to be surprisingly inventive :D</p>
<p>The better question is how to run the calculations as well as possible... and here the rule of thumb is generally to <strong>check that the calculation is converged with respect to ALL numerical parameters</strong>.</p>


================================================================================

Question: What methods are available for excited state calculations in solids?
Body: <p>In the spirit of a succint answer (3 paragraphs maximum) to create a useful resource, what are the types of excited state calculation available for solids? Please add to the list:</p>
<p><strong>Quasiparticle excitations</strong></p>
<ul>
<li>DFT: density functional theory</li>
<li><span class="math-container">$\Delta$</span>SCF [<a href="https://mattermodeling.stackexchange.com/a/1574/5_">link to answer</a>]</li>
<li>Constrained DFT</li>
<li><em>GW</em>: Many-body perturbation theory in the <em>GW</em> approximation</li>
<li>VMC: Variational Quantum Monte Carlo</li>
<li>DMC: Diffusion Quantum Monte Carlo</li>
</ul>
<p><strong>Two-particle excitations</strong></p>
<ul>
<li>TDDFT: Time-dependent density functional theory</li>
<li>BSE: Bethe-Salpeter equation</li>
<li>VMC: Variational Quantum Monte Carlo</li>
<li>DMC: Diffusion Quantum Monte Carlo</li>
</ul>

Best Answer: <h2><span class="math-container">$\Delta$</span>SCF</h2>
<p>This method generates excited states by changing the occupancy of a ground state determinant and then carrying out a new SCF with that initial guess, with some restriction throughout to prevent variational collapse back to the ground state [1]. The most common approach to stay out of the ground state is the Maximum Overlap Method (MOM), which fills orbitals based on overlap with the occupied orbitals of the previous step rather than following the <a href="https://en.m.wikipedia.org/wiki/Aufbau_principle" rel="noreferrer">Aufbau principle</a>. Another recently developed approach is the Squared Gradient Method (SGM), which is designed to converge to the closest minima  [2].</p>
<p><span class="math-container">$\Delta$</span>SCF is one of the conceptually simplest ways to generate an excited state and it makes it very easy to target an excited state of a particular symmetry. It has also been shown to be effective for modeling double excitations which is difficult or impossible for standard TDDFT calculations [2]. One drawback is that excited states are often best described with multiple configurations, which <span class="math-container">$\Delta$</span>SCF can't represent. Another issue, and the flip side of being able to target specific symmetry excited states, is that the method is not particularly blackbox and you have to have some sense of the character of the excited state you are looking for.</p>
<p>References:</p>
<ol>
<li>Ziegler, T.; Rauk, A.; Baerends, E. J. Theoretica
chimica acta 1977, 43, 261−271</li>
<li>Diptarka Hait and Martin Head-Gordon J. Chem. Theory Comput. 2020, 16, 3, 1699–1710</li>
</ol>


================================================================================

Question: Does increasing Lennard-Jones cutoff means higher accuracy?
Body: <p>I am currently trying to simulate a large box of QM/MM water molecules with SPC/Fw model and GGA DFT functional (a cube of 40 angstrom sides). To validate the QM and MM simulations separately, I am testing a pure MM box of same dimensions with SPC/Fw model, and I was trying to set the LJ and coulomb cutoff length. I am using CP2K software and pairwise-style cutoff. By the conventions I should set the cutoff to &lt;1/2 of smallest box dimension (~19 angstroms). But I encountered these two papers about this issue for liquid water:</p>
<ul>
<li><p><a href="https://doi.org/10.1063/1.2198208" rel="noreferrer">Liquid water simulation: A critical examination of cutoff length</a></p>
</li>
<li><p><a href="https://doi.org/10.1021/ct0502256" rel="noreferrer">The Origin of Layer Structure Artifacts in Simulations of Liquid Water</a></p>
</li>
</ul>
<p>both published in 2006, which suggests that larger cutoff lengths can lead to &quot;layering&quot; and other artifacts in pure water simulations. Elsewhere, I can find papers which suggest to keep the cutoff length to 9-12 angstroms (as discussed here: <a href="https://mattermodeling.stackexchange.com/questions/776/rule-of-thumb-for-morse-potential-cutoff-in-molecular-dynamics">Rule-of-thumb for Morse potential cutoff in molecular dynamics?</a>). So my question is: Does it makes sense then to limit cutoffs till 12 angstroms? And are there any significant issues particularly for QM/MM simulations?</p>
<p>For more details on the QM/MM setup: I am using an <a href="https://www.cp2k.org/events:2016_summer_school:qmmm" rel="noreferrer">additive QM/MM system with electrostatic embedding</a>.</p>

Best Answer: <p>Very interesting question!</p>
<p>The LJ potential is quite benign in that it has a <span class="math-container">$r^{-6}$</span> decay, which is pretty rapid. However, MD codes typically go a bit farther than a straightaway truncation at <span class="math-container">$r_{\rm max}$</span>, but instead also apply an analytical correction for the region <span class="math-container">$[r_{\rm max},\infty)$</span>. The analytical correction for the cut-off can be justified by long-range statistics: things look more and more uniform the farther you go from a point.</p>
<p>However, the cutoff needs to be large enough so that things really become uniform at that stage, that is, the pair correlation function <span class="math-container">$g(r)=1$</span> for <span class="math-container">$r \ge r_{\rm max}$</span>; otherwise you end up affecting the behavior of the system.</p>
<p>While in principle increasing the cutoff should make simulations more accurate, the findings of the two papers you linked would seem to insinuate that this is not true.</p>
<p>However, one should not think that it is the cutoff that is to blame, but rather the water model. It looks like the water models have either been trained for systems that are too small, and/or the training model calculations have not been fully converged with respect to all computational parameters. What one is seeing is emergent behavior from an incorrect water model: when the calculations are performed numerically accurately, the system stops behaving in a physical manner. A smaller cutoff implies forcing an uniform structure at long range; if you instead use a larger cutoff, the systems starts to exhibit long-range structures.</p>
<p>So, while my general advice is to converge the cutoffs so that the system's properties do not change anymore, if you have a case where it has been reported that too large values for the cutoff result in inaccurate behavior, then you need to limit the cutoffs to smaller values.</p>


================================================================================

Question: What is the difference between Ultrasoft, ONCV and PAW Pseudopotentials? Which is better for a spin-orbit coupled calculation?
Body: <p>I am trying to do spin-orbit coupled calculations for various topological insulators. I have found papers using Quantum Espresso with ONCV pseudopotentials and papers using VASP with PAW pseudopotentials. I know that PAW is also available in Quantum Espresso. But which would be better: ONCV or PAW?</p>
<p>Also, as a general question, why would one prefer one pseudopotential over the other?</p>

Best Answer: <p><strong>Pseudopotentials (PPs)</strong> describe the effective interaction between the valence electrons and a nuclei screened by frozen core electrons. This approximation makes DFT calculations less computationally expensive as only valence electrons are treated explicitly and the resulting valence wavefunctions no longer oscillate rapidly near the cores to ensure orthogonality with core electron wavefunctions, thus, converging with fewer plane waves.</p>
<p><strong>Pseudo-wavefunctions</strong> which arise from PPs are constructed to agree with the true all-electron wavefunction (where all electrons are treated explicitly) beyond a cutoff <span class="math-container">$r_c$</span>. The two most common types of PPs are the norm-conserving PPs (NCPPs) and ultrasoft PPs (USPPs). NCPPs impose the restriction that the total integrated ED within <span class="math-container">$r_c$</span> has to match the all-electron electron density, whilst USPPs relax this condition, requiring fewer planewaves to describe their pseudo-wavefunctions. The ONCV pseudopotential you mention is an example of a NCPP, although it is a more ‘modern’ variant using a similar construction method to USPPs, so that it requires fewer planewaves compared to traditional NCPPs.</p>
<p><strong>Projector augmented waves (PAWs)</strong> are a method of restoring the pseudo- to the all-electron wavefunctions and uses pseudopotentials which are linked closely to USPPs. It expands the all-electron wavefunction within <span class="math-container">$r_c$</span> of an atomic site <span class="math-container">$\mathbf{R}$</span> into a basis set of atomic wavefunctions <span class="math-container">$\Phi_{u}^{\mathbf{R}}$</span>, called partial waves. The pseudo-wavefunctions are also expanded into a basis set of pseudo-partial waves <span class="math-container">$\tilde{\Phi}_{u}^{\mathbf{R}}$</span> which correspond to pseudised versions of the all-electron partial waves. The coefficients for the expansion of both the all-electron and pseudo-wavefunctions are the same,  <span class="math-container">$\langle {\tilde{p}_{u}^{\mathbf{R}}} | {\tilde{\phi}_{i}} \rangle$</span>, and they are found using projector functions <span class="math-container">$\tilde{p}_{u}^{\mathbf{R}}$</span> which are orthonormal to the pseudo-partial waves. In essence, the PAW method maps the pseudo-wavefunction to the all-electron wavefunction through a linear transformation <span class="math-container">$\mathcal{T}$</span> which evaluates the difference between the all-electron and pseudo-wavefunctions:
<span class="math-container">\begin{equation}
| {\phi_{i}} \rangle=  \mathcal{T} | {\tilde{\phi}_{i}} \rangle =  | {\tilde{\phi}_{i}} \rangle + \sum_{\mathbf{R}} \sum_{u} \left ( | {\Phi}_{u} \rangle -   | {\tilde{\Phi}}_{u} \rangle \right ) \langle {\tilde{p}_{u}^{\mathbf{R}}} | {\tilde{\phi}_{i}} \rangle
\end{equation}</span></p>
<p>There is no systematic manner for deciding which pseudopotential is ‘better’ as a pseudopotential that works well for obtaining one property may not work well for another property. In general, you would expect the PAW pseudopotential to be more accurate compared to the ultrasoft pseudopotential since the projector augmented waves should restore the pseudo-wavefunction up to the all-electron wavefunction behaviour, but this does not always guarantee that it would be more accurate than USPPs or NCPPs. I am not familiar with spin-orbit calculations for topological insulators but the only way to truly determine which pseudopotential is ‘best’ for determining a specific materials property is to benchmark the different pseudopotentials against the same property obtained from an all-electron DFT method on an example system as all of these pseudopotentials are trying approximate it.</p>


================================================================================

Question: What do negative phonon frequencies signify?
Body: <p>Negative frequencies &quot;in general&quot; mean the direction of displacement is reverse to that in positive frequencies.</p>
<p>Does it mean two frequencies with opposite signs but the same magnitude give the same energy in the context of the energy-angular frequency relation?</p>
<p><span class="math-container">\begin{equation}
    \omega = \frac{\epsilon}{\bar{h}}
\end{equation}</span></p>
<p>What does this mean for two phonons with frequencies with the same magnitude but opposite signs?</p>
<p>(This question came to mind because the code <code>phonopy</code> outputs DoS for negative frequencies as well.)</p>

Best Answer: <p>Phonons are a measure of the curvature of the potential energy surface about a stationary point. In particular, the matrix of force constants is calculated as:</p>
<p><span class="math-container">$$
D_{i\alpha,i^{\prime}\alpha^{\prime}}(\mathbf{R}_p,\mathbf{R}_{p^{\prime}})=\frac{\partial^2 E}{\partial u_{p\alpha i}\partial u_{p^{\prime}\alpha^{\prime}i^{\prime}}},
$$</span></p>
<p>where <span class="math-container">$E$</span> is the potential energy surface in which the nuclei move, <span class="math-container">$u_{p\alpha i}$</span> is the displacement of atom <span class="math-container">$\alpha$</span> (of all atoms in the basis), in Cartesian direction <span class="math-container">$i$</span> (<span class="math-container">$x$</span>, <span class="math-container">$y$</span>, <span class="math-container">$z$</span>), and located in the cell within the supercell at <span class="math-container">$\mathbf{R}_p$</span>. This quantity is the second-order derivative of the energy in all possible directions, so it measures the curvature about the reference point. To obtain phonons, one transforms the matrix of force constants into the dynamical matrix:</p>
<p><span class="math-container">$$
D_{i\alpha;i^{\prime}\alpha^{\prime}}(\mathbf{q})=\frac{1}{N_p\sqrt{m_{\alpha}m_{\alpha^{\prime}}}}\sum_{\mathbf{R}_p,\mathbf{R}_{p^{\prime}}}D_{i\alpha;i^{\prime}\alpha^{\prime}}(\mathbf{R}_p,\mathbf{R}_{p^{\prime}})e^{i\mathbf{q}\cdot(\mathbf{R}_p-\mathbf{R}_{p^{\prime}})},
$$</span></p>
<p>where <span class="math-container">$N_p$</span> is the number of cells in the supercell over which periodic boundary conditions are applied, and <span class="math-container">$m_{\alpha}$</span> is the mass of atom <span class="math-container">$\alpha$</span>. Using standard mathematical language, these two matrices are essentially Hessians.
Diagonalizing the dynamical matrix gives eigenvalues <span class="math-container">$\omega^2_{\mathbf{q}\nu}$</span> and eigenvectors <span class="math-container">$v_{\mathbf{q}\nu;i\alpha}$</span>. The key quantity for our discussion are the eigenvalues <span class="math-container">$\omega^2_{\mathbf{q}\nu}$</span> which can be:</p>
<ol>
<li>Positive. Positive eigenvalues indicate a positive curvature of the potential energy surface, so the energy increases quadratically if you displace the atoms in the directions given by the associated eigenvector, and the eigenvalue magnitude tells you how &quot;fast&quot; the energy increases.</li>
<li>Negative. Negative eigenvalues indicate a negative curvature of the potential energy surface, so the energy decreases quadratically if you displace the atoms in the directions given by the associated eigenvector, and the eigenvalue magnitude tells you how &quot;fast&quot; the energy decreases.</li>
</ol>
<p>If you are performing calculations for a structure at a (local) minimum of the potential energy surface, then all eigenvalues will be positive (positive-definite Hessian). If you are performing calculations for a structure at a saddle point of the potential energy surface, then most eigenvalues will be positive, but those associated with the directions which lower the energy will be negative.</p>
<p>Now we come to the key point: phonon frequencies are given by the <em>square root</em> of the eigenvalues of the dynamical matrix. As these eigenvalues are either positive or negative, then phonon frequencies are either positive real numbers or purely <em>imaginary</em> numbers. <strong>Phonon frequencies cannot be negative: they are either positive or imaginary.</strong> Many codes output imaginary frequencies as &quot;negative&quot; numbers, but this is a convention that in principle assumes that the user knows that &quot;negative&quot; frequencies are really imaginary, but which I think has traditionally led to big confusions, particularly for new people in the field.</p>
<p>Having clarified this, I will rephrase your question: what does it mean when two eigenvalues of the dynamical matrix, <span class="math-container">$\omega^2_{\mathbf{q}\nu}$</span>, have the same magnitude but opposite sign?
In terms of phonon frequencies, the equivalent question would be: what does it mean that two phonon frequencies, <span class="math-container">$\omega_{\mathbf{q}\nu}$</span>, have the same magnitude when one is real and one imaginary? In both cases, what that means is that the magnitude of the curvature of the potential energy surface is the same, but in one case the energy increases and in the other the energy decreases.</p>


================================================================================

Question: Why is CPHF/CPKS necessary for calculating second derivatives?
Body: <p>This question is coming from an answer <a href="https://mattermodeling.stackexchange.com/questions/2363/analytic-hessians-for-meta-gga-functionals">to one of my previous questions</a>. During optimizations, QM programs usually compute the gradient(first derivative) analytically, and take a guess of the hessian (second derivatives). If the hessian is needed a coupled-perturbed hartree fock (CPHF) or coupled-perturbed Kohn-Sham (CPKS) is usually required which is very computationally expensive. From what I have been told, the gradient does not need CPHF, it can be calculated directly from the SCF.</p>
<p>My question is why the second derivative is so much more expensive than the first derivative? For a compound I was working on, the first derivatives took about 2 minutes, while the second derivatives took almost 15 minutes to run. That's more than 7 times! What I don't understand is that if the SCF solution can be differentiated once w.r.t the coordinates, then why can't it be differentiated twice?</p>

Best Answer: <p>It comes down to the fact that HF and KS both are variational methods. This <a href="https://www.lct.jussieu.fr/pagesperso/toulouse/enseignement/molecular_properties.pdf" rel="noreferrer">short article</a> by Julien Toulouse gives a great description of ways to compute static/dynamic response properties. I'll just summarize the relevant portion.</p>
<p>We can compute derivatives of the energy with respect to any variable <span class="math-container">$x$</span> as:
<span class="math-container">$$\frac{dE}{dx}=\frac{\partial E}{\partial x}+\sum_i \frac{\partial E}{\partial p_i}|_{\mathbf{p}=\mathbf{p}^0}  \frac{\partial p_i^0}{\partial x}$$</span>
Here we are writing the derivative in two terms. The first is due to the explicit dependence of the energy on the variable <span class="math-container">$x$</span>. The latter term is due to <em>implicit</em> dependence, with the energy depending on particular wavefunction parameters <span class="math-container">$\mathbf{p}$</span>, which in turn may depend on <span class="math-container">$x$</span>. For SCF methods, these parameters are just the MO coefficients <span class="math-container">$C$</span>.</p>
<p>For a general method, this would require some type of response calculation to solve, as we typically don't have an explicit form for <span class="math-container">$\frac{\partial p_i^0}{\partial x}$</span>. However, since the energy for HF/KS is  variational <span class="math-container">$\frac{\partial E}{\partial p_i}|_{\mathbf{p}=\mathbf{p}^0}=0$</span>, which zeros out this term.</p>
<p>So to compute the forces with HF/KS, we only need to consider the explicit dependence of the energy on the nuclear positions. However, once we want to compute the Hessian, we can no longer ignore this implicit term. If we write the Hessian as a derivative of the force in the same form as above, the force is not variationally optimal and so this terms doesn't cancel. Thus we need to determine the dependence of the MO coefficients on the nuclear positions, which is typically done using CPHF/CPKS.</p>
<p>You can get a rough order of magnitude estimate of a 2nd order property from just the explicit term. For example, when computing the polarizability (2nd derivative of the energy with respect to an applied electric field) Gaussian will print out an approximate polarizability, which it calculates by just contracting the dipole with itself rather than the perturbed electric density. While this can sometimes be close to the final result, I'm not aware of any formal bound on the size of the implicit contribution, so in general it would be a major approximation to neglect the perturbation of the density.</p>


================================================================================

Question: Is it possible to calculate the Curie temperature for magnetic systems?
Body: <p>Using DFT calculations we can know the electronic and magnetic property of the system at 0 K. I am wondering if there is any computational method to know the curie temperature of magnetic materials? Can I know the Curie temperature using Quantum Espresso?</p>
<p>Any suggestion will be helpful.<br />
Thank you!</p>

Best Answer: <p>This is an important question, and I will do my best to answer it in a way that adds to the nice suggestions made in earlier answers.</p>
<p>In short, the answer is yes, but only with the help of some other models and approaches. In this response, I will focus on the conventional (computational) approach to this question, using lattice models and Monte Carlo Methods.</p>
<p>This approach, generally speaking, involves running a series of spin-polarized DFT calculations for different magnetic configurations. The next step is to map the results of these calculations (total energies and/or electronic structure) to a lattice model. There a two main ways to do this, which I touch on below. The transition temperature (Curie temperature for ferromagnets) can then be identified by running Monte Carlo (MC) simulations on this lattice model, and observing at what temperature your order parameter (in this case total magnetization) goes to zero (paramagnetic transition), or heat capacity &quot;spikes,&quot; etc... The VAMPIRE documentation for this is included here:</p>
<p><a href="https://vampire.york.ac.uk/tutorials/curie-temperature-simulation/" rel="noreferrer">https://vampire.york.ac.uk/tutorials/curie-temperature-simulation/</a></p>
<p>In theory, you could perform MC runs with DFT energy evaluations. However, practically speaking the lattice model provides a cheap alternative to exploring the magnetic configuration space of your system.</p>
<p>I encourage you explore the magnetic exchange workflow recently implemented in <em>atomate</em>:</p>
<p><a href="https://atomate.org/atomate.vasp.workflows.base.html#module-atomate.vasp.workflows.base.exchange" rel="noreferrer">https://atomate.org/atomate.vasp.workflows.base.html#module-atomate.vasp.workflows.base.exchange</a></p>
<p>which calculates the parameters of a lattice model from DFT energies (using <em>pymatgen</em> functions linked below), and estimates the transition temperature using the MC method implemented in the VAMPIRE code suite (suggested in a previous answer).</p>
<p>The workflow calls the Heisenberg model mapper implemented in <em>pymatgen</em>:
<a href="https://pymatgen.org/pymatgen.analysis.magnetism.heisenberg.html" rel="noreferrer">https://pymatgen.org/pymatgen.analysis.magnetism.heisenberg.html</a></p>
<p><strong>Some more details</strong></p>
<p>In the lattice picture, the energy of arrangements of different spins can be calculated in a relatively straightforward way using the effective Hamiltonian:</p>
<p><span class="math-container">$$\mathcal{H} = - \sum_{\langle i,j \rangle} J_{ij} \delta_{ij} $$</span></p>
<p>where the sum is over all pairwise interactions between sites (the pair of sites <span class="math-container">$i$</span> and <span class="math-container">$j$</span> is indicated by <span class="math-container">$\langle i,j \rangle$</span>). In the <a href="https://en.wikipedia.org/wiki/Ising_model" rel="noreferrer">Ising</a> picture, <span class="math-container">$\delta_{ij} = \sigma_i \sigma_j$</span> (where <span class="math-container">$\sigma_k = \pm 1$</span> indicates the up/down spin state at site <span class="math-container">$k$</span>). In the <a href="https://en.wikipedia.org/wiki/Classical_Heisenberg_model" rel="noreferrer">classical Heisenberg</a> picture, <span class="math-container">$\delta_{ij} = \vec{s}_i \cdot \vec{s}_j$</span> (the dot product between spins represented as vectors in 3D space).</p>
<p>One way to calculate <span class="math-container">$J_{ij}$</span> values from DFT using the <a href="https://www.sciencedirect.com/science/article/abs/pii/0304885386903367" rel="noreferrer">KKR-Green's function method</a>, another is by solving a matrix vector system (implemented in the <em>atomate</em> workflow). For a physical intuition behind these exchange interaction values, I suggest referring to a book in solid state physics/chemistry, such as the well known text by Ashcroft and Mermin.</p>
<p>Famous examples of these lattice models include Ising and Heisenberg (both &quot;classical&quot; and &quot;quantum&quot; formulations). However, there are many more, such as the Potts and XY models. Theorists have studied these models extensively for years. Despite the simplicity of these models compared to density functional theory approaches, these lattice models are able to capture the physics relevant to the order-to-disorder (ordered to paramagnetic) phase transition in magnets.</p>
<p>Common assumptions underlying these models include the approximation that each atom has an effective &quot;spin&quot; or magnetic moment on each atom. This assumption is reasonable for many systems with &quot;localized&quot; moments - more on some of the major limitations of this simplification <a href="https://en.wikipedia.org/wiki/Exchange_interaction#Limitations_of_the_Heisenberg_Hamiltonian_and_the_localized_electron_model_in_solids" rel="noreferrer">here</a>. Alternatively, there are other approaches that include &quot;higher order&quot; expansions to the free energy of your system using &quot;cluster expansion&quot; or &quot;cluster multipole expansion,&quot; however, I will not focus on those approaches here.</p>
<p>I am currently working on extending this mapping to include the effect of atomic displacements in the model, in addition to magnetic interactions. These spin-lattice (magnon-phonon) coupling effects have been shown to also shift the phase transition temperature. I've run out of allowed references, however, if your curious, search for &quot;compressible Heisenberg/Ising,&quot; &quot;spin-lattice coupling Hamiltonian,&quot; or &quot;Bean-Rodbell&quot; model.</p>
<p>A helpful lesson that I've learned is that it's important to be aware of the assumptions of different lattice models, and understanding the relevant physics that we want to capture. As the famous saying goes:</p>
<p>&quot;All models are wrong but some are useful.&quot;</p>


================================================================================

Question: What are the recent developments of TDDFT to simulate the excited properties of materials?
Body: <p>The <a href="https://en.wikipedia.org/wiki/Time-dependent_density_functional_theory#:%7E:text=Time%2Ddependent%20density%2Dfunctional%20theory,as%20electric%20or%20magnetic%20fields." rel="noreferrer">time-dependent density functional theory (TDDFT)</a> and the many-body perturbation method (GW@BSE) are considered as the two most popular and successful methods to describe the excited-stated properties of materials. In fact, both methods have been reviewed in <a href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.74.601" rel="noreferrer">this classical paper published in the Review of Morden Physics</a>. From that almost twenty years have passed. What are the recent developments of TDDFT? What's the most popular and successful package that implements TDDFT? And what're the main challenges to realize the simulation of real materials with TDDFT?</p>
<p><strong>Disclaimer:</strong> According to the one-topic-per-answer, <a href="https://mattermodeling.stackexchange.com/questions/4088/what-are-the-recent-developments-of-gwbse-to-simulate-the-excitated-properteis">a twin question</a> related to the GW@BSE method is asked in another post. But any comparisons between both methods in recent developments are welcome.</p>

Best Answer: <p>Time-dependent density functional theory (TDDFT) has indeed seen significant developments since its review in the classical paper you mentioned. One of the recent advancements in TDDFT is:</p>
<ol>
<li>Nonlinear Response Functions</li>
</ol>
<ul>
<li>In the context of Time-Dependent Density Functional Theory (TDDFT), Nonlinear response functions refer to the exploration of phenomena beyond linear response properties. While traditional TDDFT focuses on linear response, there has been increasing interest in investigating nonlinear optical processes and strong light-matter interactions using TDDFT. This extension allows for the study of phenomena such as harmonic generation, two-photon absorption, and higher-order nonlinear optics. Nonlinear TDDFT has been applied to gain insights into materials with significant light-matter interactions. For example, a study <a href="https://doi.org/10.1063/1.4867271" rel="nofollow noreferrer">https://doi.org/10.1063/1.4867271</a> published by Federico Zahariev and  Mark S. Gordon. in the Journal of Chemical Phyiscs (2014) titled &quot;Nonlinear response time-dependent density functional theory combined with the effective fragment potential method&quot; investigated the calculattio of the two-photon absorption cross section and incorporated solvent effects via the EFP method. The nonlinear-response TDDFT/EFP method was shown to be  able to make correct qualitative predictions for both gas phase values and aqueous solvent shifts of several important nonlinear properties. The package that was used in this study was GAMESS.</li>
</ul>
<p>Among the challenges in realizing the simulation of real materials with TDDFT, strongly correlated systems is challenging.</p>
<ul>
<li>Strongly correlated systems pose a challenge for TDDFT due to the intricate electron-electron interactions involved. Examples of such systems include transition metal complexes and strongly correlated materials, where the standard TDDFT approach may not be sufficient to capture their properties accurately. To address the treatment of strongly correlated systems within TDDFT, researchers have explored advanced theoretical approaches, such as the combination of TDDFT with dynamical mean-field theory (DMFT). This hybrid approach aims to incorporate the effects of strong electron-electron correlations, which are crucial for accurately describing the electronic structure and excited states of these systems.
A study <a href="https://doi.org/10.1103/PhysRevLett.106.116401" rel="nofollow noreferrer">https://doi.org/10.1103/PhysRevLett.106.116401</a> published by Daniel Karlsson et al. in Physical Review Letters (2011) titled &quot;Time-Dependent Density-Functional Theory Meets Dynamical Mean-Field Theory: Real-Time Dynamics for the 3D Hubbard Model&quot;. The authors proposed a new class of exchange-correlation potentials for a static and time-dependent density-functional theory of strongly correlated systems in three-dimensions
Those studies exemplify the ongoing efforts to develop theoretical approaches that extend TDDFT to treat strongly correlated systems accurately. The combination of TDDFT with advanced methods like DMFT offers a promising avenue for understanding the electronic structure and excited states of materials with strong electron-electron correlations.</li>
</ul>
<p>Regarding the most popular and successful package that implements TDDFT, there are several widely used software packages available, each with its own strengths and capabilities. Some of the popular packages include:</p>
<ul>
<li>Gaussian: Widely used computational chemistry software that offers TD-DFT functionality. It provides a comprehensive suite of methods for studying excited-state properties, including UV/Vis spectra, fluorescence, and excited-state dynamics. Gaussian is known for its versatility and extensive user community.</li>
<li>GPAW: GPAW (Generalized Projector Augmented Wave) is a popular electronic structure code that implements TDDFT. It is known for its efficiency and scalability, making it suitable for large-scale simulations.</li>
<li>NWChem: NWChem is a computational chemistry package that includes TDDFT functionality. It offers a wide range of methods, including both TDDFT and GW calculations, making it versatile for studying excited-state properties.</li>
</ul>


================================================================================

Question: What software will allow me to combine two images?
Body: <p>I have seen in many presentations and publications, things like <strong>a human on a molecule</strong> (for example). What software can do this? It's for a presentation I'm doing.</p>

Best Answer: <h1>Blender</h1>
<p>I usually do 3D images with molecules using <a href="https://www.blender.org" rel="noreferrer">Blender</a>. You can export your molecule 3D model with a chemistry visualization program (such as vmd or jmol) into Blender (I usually do it using OBJ files, it just needs a couple of adjustments after the import), OR you can write your own python loader from within Blender (harder to do, but possible). It is NOT an easy route, Blender is a complex program that takes years to master, but it definitely gives amazing results if you put time into it.</p>
<p><a href="https://i.sstatic.net/Y764xm.jpg" rel="noreferrer"><img src="https://i.sstatic.net/Y764xm.jpg" alt="enter image description here" /></a></p>


================================================================================

Question: When using the &quot;even tempered method&quot; for augmenting basis sets, what justification is there for the numerical factor which is used?
Body: <p>Diffuse functions are often added to basis sets using "even-tempered" exponents, or sometimes I have heard the phrase "even tempering". </p>

<p>In <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.23931" rel="noreferrer">this paper</a> by Jacek Koput, the author says "the customary even-tempered exponents were calculated in this work by multiplying the exponent of the outermost primitive function of a given angular symmetry in the valence basis set by a factor of 0.4."</p>

<p>The factor of 0.4 (or factors with a very similar value, such as 0.3) have been used in various other papers, but in my experience the factor is chosen without any detailed explanation. Is there any reason why the numerical factor should be specifically 0.4 (or similar)? </p>

Best Answer: <p>Often it's the Stetson-Harrison method. Instead of multiplying, one often specifies the diffuse extrapolation by division. E.g. 0.4 corresponds to dividing the smallest exponent by 1/0.4 = 2.5. This is indeed in the usual ball park; a factor of 3 is quite commonly used.</p>

<p>The numerical value of the factor is related to the completeness of the basis set, which can be "measured", see e.g. <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.23802" rel="nofollow noreferrer">my paper on completeness-optimization</a> for a discussion. As the basis set becomes more and more complete, the spacing factor approaches the value 1.</p>

<p>If the basis set you are trying to augment has been optimized as even tempered, IMHO one should not change the spacing of the exponents, but rather re-use the even-tempered spacing of the optimized primitives. (I also used a similar scheme <a href="https://aip.scitation.org/doi/10.1063/1.5144964" rel="nofollow noreferrer">here</a>)</p>


================================================================================

Question: What software are used to model the optical behavior of metallic nanoparticles?
Body: <p>It's fairly routine to calculate absorption of single molecules, essentially this can be done by calculating the excited-state wavefunctions and the oscillator strengths between the states. For example, using TD-DFT or configuration interaction methods. </p>

<p>However nanoparticles behave much differently, from what I understand the electrons behave more like a particle in a box. The size and shape of the particle directly controls the optical properties. </p>

<p>What type of software can be used to model optical properties of nanoparticles and how do these approaches differ from molecules? </p>

Best Answer: <p>There are several DFT based commercial or open source softwares that could be used to simulate the optical properties of nanomaterials, particularly metallic nanoparticles. Basically, what you are looking for is calculating the susceptibility tensor from Kubo-Greenwood relation:</p>
<p><span class="math-container">$$\chi_{ij}(\omega) = \frac{e^{2}}{\hbar m_{e}^{2} V} \sum_{n,m,\mathbf{k}} \frac{f_{m,\mathbf{k}} - f_{n,\mathbf{k}}}{\omega_{nm}^{2}(\mathbf{k}) (\omega_{nm}(\mathbf{k})-\omega-i\frac{\Gamma}{\hbar})}p_{nm}^{i}(\mathbf{k})p_{mn}^{j}(\mathbf{k})$$</span></p>
<p>Where <span class="math-container">$p_{nm}^{i}(\mathbf{k}) = \langle n\mathbf{k} | \mathbf{p}^{i} | m\mathbf{k}\rangle$</span> is i-th component of the momentum operator between states n and m. <span class="math-container">$m_{e}$</span> is the electron mass, <span class="math-container">$e$</span> is electron charge, <span class="math-container">$V$</span> is the volume, <span class="math-container">$\Gamma$</span> the energy broadening, <span class="math-container">$\hbar\omega_{nm}(\mathbf{k})= E_{n}(\mathbf{k}) - E_{m}(\mathbf{k})$</span>, and finally <span class="math-container">$f_{n,\mathbf{k}}$</span> is the Fermi function evaluated at the band energy of <span class="math-container">$E_{n}(\mathbf{k})$</span>.</p>
<p>Relative dielectric constant (<span class="math-container">$\epsilon_{r}$</span>), polarizability (<span class="math-container">$\alpha$</span>), and optical conductivity (<span class="math-container">$\sigma$</span>) are related to susceptibility as:</p>
<p><span class="math-container">$$\epsilon_{r}(\omega) = 1 + \chi(\omega)$$</span></p>
<p><span class="math-container">$$\alpha(\omega) = V \epsilon_{0} \chi(\omega)$$</span></p>
<p><span class="math-container">$$\sigma(\omega) = -i \omega \epsilon_{0} \chi(\omega)$$</span></p>
<p>Finally, the refractve indices are derived from relative dielectric constant (<span class="math-container">$\epsilon_{r}$</span>) as:</p>
<p><span class="math-container">$$n(\omega) + i\kappa(\omega) = \sqrt{\epsilon_{r}(\omega)}$$</span></p>
<h1>QuantumATK</h1>
<p>An example of commercial software that could be used to extract these optical properties for metallic nanoparticles is <a href="https://www.synopsys.com/silicon/quantumatk.html" rel="noreferrer">QuantumATK</a>.</p>


================================================================================

Question: Are there any online course/video lectures available on Density functional theory?
Body: <p>Please mention good online video lectures on theoretical foundations and practical implementation of DFT.</p>

Best Answer: <p><strong>1. <a href="http://compmatphys.epotentia.com/" rel="noreferrer">Online course on Computational Materials Physics</a> by <a href="https://molmod.ugent.be/members/stefaan-cottenier" rel="noreferrer">Prof. Stefaan Cottenier</a></strong></p>

<blockquote>
  <p>All content is offered via prerecorded videos. Each video comes with a task, and after submitting the task there is in many cases either automated feedback or peer feedback. At any other time of the year, you can still study this course in a self-paced way. There is the opportunity to be part of a team to work on a project. The course is free of cost and open to anyone. Emphasis is on conceptual understanding and hands-on training, not on mathematical theory.</p>
</blockquote>

<p><strong>2. <a href="https://www.paradim.org/cu_ss_2018" rel="noreferrer">An Introduction to Density Functional Theory for Experimentalists</a> by <a href="https://www.oden.utexas.edu/people/1669/" rel="noreferrer">Prof. Feliciano Giustino</a> during DFT Summer School 2018 at Cornell University</strong></p>

<blockquote>
  <p>The goal of this Summer School is to introduce experimentalists to density-functional theory calculations and first-principles materials modelling. This course answers the basic questions: “Can DFT help me with my experimental problem? Which  materials  properties  can  be predicted  and  how reliable are the results? How difficult would it be to run the calculation that I need? Can I do this on my own or I better seek for help from the theory group next door?”. By the end of the school the participants will be able to perform basic DFT calculations in complete autonomy, and will have a better understanding of the current literature on atomistic modelling using DFT. The course is articulated along three parallel tracks: theory lectures, practical lectures, and hands-on sessions.</p>
</blockquote>

<p><strong>3. <a href="https://www.materialscloud.org/learn/sections/lfYatW/hpc-and-high-throughput-materials-modeling-ictp-trieste-2017" rel="noreferrer">Advanced Workshop on High-Performance &amp; High-Throughput Materials Simulations using Quantum ESPRESSO | (smr 3102), Trieste, January 16 - 31, 2017</a></strong></p>


================================================================================

Question: What type of education is appropriate for people interested in Materials Modeling?
Body: <p>What type of education is appropriate for people interested in Materials Modeling?  Obviously, the more education, the better.  I'm sure a PhD in physics, another in chemistry, topped off with one in structural engineering will help.  But most people will not choose to pursue that much education.</p>

<p>What is a good baseline education for people interested in Materials Modeling?</p>

Best Answer: <p>As evident from the name, Materials Modelling (or Computational Materials Science as it is sometimes called) lies at the intersection of materials science and computational engineering. To answer this question, the level of education required depends entirely on what one intends to do with materials modelling.</p>

<p>Materials science is quite interdisciplinary, and it is possible to work on different aspects of materials modelling (like the application of existing models/software to new materials/problems, building new models/software etc.). If one is interested in doing research in the field of materials modelling, typically, it is possible to start participating in research while pursuing an undergraduate degree in a related discipline. So, the undergraduate major could be materials science/engineering itself, physics, chemistry, chemical engineering, mechanical engineering, mathematics, computer science, the list cannot be exhaustive. Also, one can work on a materials modelling problem for their PhD while being officially affiliated to any of these disciplines. So, one often pursues some form of education while already working with materials modelling, and then continues to work in the field.</p>

<p>If the question intended to know of any prerequisites to start doing materials modelling, then the answer could be some understanding of materials behaviour, and the ability to use computational tools to solve problems (which can also be learnt as one starts). The book <a href="https://www.cambridge.org/in/academic/subjects/engineering/materials-science/introduction-computational-materials-science-fundamentals-applications?format=HB" rel="noreferrer">Introduction to Computational Materials Science</a> by Prof. Richard LeSar is an accessible starting point.</p>


================================================================================

Question: How are continued fractions related to quantum materials?
Body: <p>In my spare time, I have been studying and analysing continued fractions. </p>

<p>I was having a conversation with someone on Discord in a Mathematics server and he was telling me that continued fractions can be related to quantum physics. He didn't go into it too much and the concept he was describing seemed a little vague to me. I am aware that Pincherle's theorem<sup>[1]</sup> asserts there is an intimate relationship with three-term recurrence relations of the form <span class="math-container">$x_{n+1}=b_nx_n+a_nx_{n-1}$</span> and continued fractions, more accurately with their partial convergents, given that this recurrence relation has a minimum if a related cfrac converges. But I am not quite the physicist myself to compare physics with the properties of cfracs.</p>

<p>Although I can go on Google and do some research on this, I figured it might serve useful to have a post on this in this beta community, but I do apologise if it is too broad or open-ended and thus upsets any regulations here.</p>

<p>Any thoughts on this?</p>

<h3>References</h3>

<p>[1] Pincherle, S. (1894). Delle funzioni ipergeometriche e di varie questioni ad esse attinenti. <em>Giorn. Mat. Battaglini</em>. 32:209–29</p>

<p>[2] Parusnikov, V. I. A Generalization of Pincherle's Theorem to k-Term Recursion Relations. <em>Math Notes</em> <strong>2005,</strong> <em>78</em> (5-6), 827–840. <a href="https://doi.org/10.1007/s11006-005-0188-7" rel="noreferrer">DOI: 10.1007/s11006-005-0188-7</a>.</p>

Best Answer: <p>In the paper &quot;<a href="https://academic.oup.com/ptp/article/34/3/399/1943170" rel="nofollow noreferrer">A Continued-Fraction Representation of the Time-Correlation Functions</a>&quot;, generalized susceptibilities and transport coefficients for materials are obtained using a continued-fraction expansion of the Laplace transform of the time-correlation functions.</p>
<p>This was the precursor to what is now called the &quot;<a href="https://en.wikipedia.org/wiki/Hierarchical_equations_of_motion" rel="nofollow noreferrer">hierarchical equations of motion</a>&quot; which are used to study the dynamics of a quantum system (such as an electron) coupled to a bosonic bath (for example the vibrations of the lattice in a GaAs semi-conductor quantum dot). This area is called &quot;dissipative quantum dynamics&quot; or &quot;open quantum systems&quot; and is used to study for example, the decoherence of qubits in solid-state quantum computers.</p>


================================================================================

Question: What makes PBE the most preferred functional over other GGA functionals?
Body: <p>There are many GGA functionals like PW91, PBE, and BLYP. However the most popular functional is the PBE. </p>

<p>What is the reason PBE functional so popular?</p>

Best Answer: <p>Here is an introduction to DFT functionals: <a href="https://dft.uci.edu/pubs/RCFB08.pdf" rel="noreferrer">https://dft.uci.edu/pubs/RCFB08.pdf</a></p>

<p>In short, the Perdew-Burke-Ernzerhof (PBE) functional is very popular because it is a non-empirical functional with reasonable accuracy over a wide range of systems. While PBE is typically not the most accurate GGA (generalized gradient approximation) functional for a given system, it usually is not too far off either. Empirical functionals offer better accuracy for systems they are parametrized for, e.g. "BLYP has smaller errors for main-group organic molecule energetics", but fail in others.</p>


================================================================================

Question: What do phonon dispersion (or lattice dynamics) studies include?
Body: <p>I use the codes <code>phonopy</code> and <code>phonopy-qha</code> for phonon dispersion studies. <code>phonopy-qha</code> gives quite a lot of information about the material: bulk modulus vs temperature, Gibbs energy vs temperature, etc.</p>
<p>I was curious about the information that goes into calculating these properties. Since it diagonalizes the dynamical matrix and gets phonon frequencies, I assume we have no information about electronic properties in the output.</p>
<p>To be specific, in the Gibbs energy that <code>phonopy-qha</code> calculates, other than the vibrational entropy contribution,  would it have any other contribution? What about enthalpy contribution, and would it consider the zero-point kinetic energy of ions (by virtue of Heisenberg's uncertainty principle) in that?</p>
<p>Edit: I realised it does account for the electronic energy (DFT calculated energy) as well. <strong>What else is included other than phonon vibrations and electronic energy?</strong></p>

Best Answer: <p>TLDR: When you calculate phonons, you can describe electrons at different levels of theory, typically semilocal DFT, but also hybrids or dynamical mean-field theory. Phonons do include zero-point motion, as they are essentially a set of uncoupled quantum harmonic oscillators. Enthalpy can be calculated without reference to phonons, simply adding a PV term to the Hamiltonian. Gibbs free energy is calculated by adding a PV term to the phonon calculation.</p>

<p>Longer answer: The starting point of any phonon calculation is the Born-Oppenheimer approximation, which allows you to separate the electron and nuclear degrees of freedom. After applying this Born-Oppenheimer approximation, you end up with two eigenvalue equations. The first one corresponds to the electrons, in which the nuclei occupy fixed positions and their coordinates only appear as parameters. This electronic eigenvalue equation is what DFT codes solve.</p>

<p>Your question refers to the second eigenvalue equation that results from the Born-Oppenheimer approximation, which is the nuclear equation. The Hamiltonian in this equation reads (in atomic units):</p>

<p><span class="math-container">$$
\hat{H}=-\sum_i\frac{1}{2m_i}\nabla_i^2+V(\mathbf{R}),
$$</span></p>

<p>in which the sum is the kinetic energy of the nuclei, and it runs over all nuclei <span class="math-container">$i$</span> in the system, and the second term is the potential energy felt by the nuclei, in which <span class="math-container">$\mathbf{R}=(\mathbf{r}_1,\mathbf{r}_2,\ldots)$</span> is a collective variable containing all individual nuclear coordinates <span class="math-container">$\{\mathbf{r}_i\}$</span>. The value of this potential energy, typically called the potential energy surface, at a given collective nuclear coordinate <span class="math-container">$\mathbf{R}$</span> is given by the electronic eigenvalue for the nuclei fixed at this coordinate. This means that, unlike the electronic equation solved in DFT for which you know the Hamiltonian, in the case of the equation for the nuclei, you don't even know the Hamiltonian, as you don't know what <span class="math-container">$V(\mathbf{R})$</span> is. You first need to figure out what <span class="math-container">$V(\mathbf{R})$</span> is, and to do this you need to solve the electronic equation <strong>many</strong> times, once at each potential value of <span class="math-container">$\mathbf{R}$</span>. This is clearly unfeasible, as <span class="math-container">$\mathbf{R}$</span> spans a 3N dimensional space, where N is the number of atoms in your system.</p>

<p>This is where the harmonic approximation that you mention comes in. For a material, we assume that the nuclei don't move much from their equilibrium positions. This is because they are relatively heavy (compared to the electrons), so rather than exploring the entire potential <span class="math-container">$V(\mathbf{R})$</span>, they only explore this potential in the region near their equilibrium position, which corresponds to a minimum of <span class="math-container">$V(\mathbf{R})$</span>. To proceed, I will make a change of coordinates <span class="math-container">$\mathbf{u}_i=\mathbf{r}_i-\mathbf{r}_i^0$</span>, to collective coordinates <span class="math-container">$\mathbf{U}$</span> which are relative coordinates with respect to the equilibrium coordinates <span class="math-container">$\mathbf{R}^0$</span>. In this way, equilibrium corresponds to <span class="math-container">$V(\mathbf{U}=0)$</span>. In the harmonic approximation, we approximate this potential by a second-order Taylor expansion about equilibrium:</p>

<p><span class="math-container">$$
V(\mathbf{U})\simeq V(0)+\sum_{\alpha,\beta}\frac{\partial^2V}{\partial u_{\alpha}\partial u_{\beta} }u_{\alpha}u_{\beta}.
$$</span></p>

<p>In the sum, <span class="math-container">$\alpha$</span> and <span class="math-container">$\beta$</span> are collective indices capturing the degrees of freedom of the 3N-dimensional energy surface (cell in the crystal, atoms in the basis, and Cartesian direction). To proceed, you replace this second-order approximation to the potential into the nuclear Hamiltonian I wrote above, and you can diagonalize it in terms of phonons. This second-order approximation works very well because, in terms of phonons, it essentially allows you to replace a 3N-dimensional potential with 3N 1-dimensional potentials, and the latter are much easier to calculate.</p>

<p>Therefore, to go back to your question: all that goes into a phonon calculation are the second order derivatives of the potential energy surface <span class="math-container">$V(\mathbf{R})$</span>. What information does this contain? As <span class="math-container">$V(\mathbf{R})$</span> corresponds to the electronic energy at <span class="math-container">$\mathbf{R}$</span>, then the level at which you treat the electrons affects <span class="math-container">$V(\mathbf{R})$</span>. Typically, this is calculated using semi-local DFT (LDA, GGA), but there are a few studies that calculate <span class="math-container">$V(\mathbf{R})$</span> using hybrid DFT, which is much more expensive but more accurate, or even other beyond-DFT methods like dynamical mean-field theory.</p>

<p>Once you solve the nuclear problem in terms of phonons, you can calculate the vibrational contribution at finite temperature to the Helmholtz free energy (TS term). In these calculations, the phonons <strong>do</strong> have a zero-point contribution to the energy, as they are described by a set of uncoupled quantum harmonic oscillators.</p>

<p>For your other specific questions:</p>

<ol>
<li><p>Enthalpy: for that you simply need to add a PV term to the Hamiltonian, so no need for phonons there. The calculations aren't any more expensive than a typical DFT calculation as PV is very cheap to evaluate.</p></li>
<li><p>Gibbs free energy: for this you need to add both PV and TS terms, you get the TS term from the Helmholtz free energy, and the PV term from the enthalpy.</p></li>
</ol>


================================================================================

Question: Is there any complete list of open source hubs specifically designed for materials modeling and computational materials science?
Body: <p>Several years ago, I became familiar with <a href="https://nanohub.org/" rel="noreferrer">nanoHUB</a> for accessing open-source codes and software specifically for computational materials science at the nano-scale as well as <a href="https://pages.nist.gov/pfhub/" rel="noreferrer">PFHub</a> that gives you access to phase-field and microstructural modeling codes and software. Both of these hubs are open-source and you could use other codes or share your own code there. I'm wondering if there is a more complete list of similar hubs specifically for computational materials science (for a whole range of materials modeling from atomistic, quantum mechanics, molecular dynamics to mesoscopic and even macroscopic modeling) including these two that I know. I really appreciate any recommendations or suggestions here.</p>

Best Answer: <p>The <a href="https://materials.registry.nist.gov/" rel="noreferrer">NIST Materials Resource Registry</a> is not limited to computational resources, but refers to many of the codes you're looking for. Searching for "hub" only turns up two listings, both of which are tied to nanoHUB and Purdue.</p>


================================================================================

Question: Calculation of the relative energies in a Spin Crossover material
Body: <p><a href="https://en.wikipedia.org/wiki/Spin_crossover" rel="nofollow noreferrer">Spin Crossover (SCO) complexes</a> are a particular type of molecular entities which are usually formed by a metal ion (general <span class="math-container">$\ce{Fe(II)}$</span> or <span class="math-container">$\ce{Co(III)}$</span>) complexed by several ligands with donor N atoms. The <em>d</em> orbitals splitting (<span class="math-container">$\Delta$</span> or 10Dq) produced in certain octahedral coordination geometries allows two spin configurations which are metastable and accesible with a stimuli such as temperature, light, pressure and electric field among others, producing a well-known hysteretic behavior.</p>
<p><a href="https://i.sstatic.net/pXlvg.png" rel="nofollow noreferrer"><img src="https://i.sstatic.net/pXlvg.png" alt="Metal d orbitals splitting under a octahedral ligand field" /></a></p>
<p>Unfortunately, the relative energies of HS and LS in a SCO complex are extremely difficult to tackle in a DFT molecular calculation. Although some elaborated approaches have been considered, such as considering the effect of the <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.24676" rel="nofollow noreferrer">dipolar interactions</a> or using <a href="https://pubs.acs.org/doi/abs/10.1021/acs.inorgchem.8b01821" rel="nofollow noreferrer">intrincated exchange-interaction funcionals</a>, no evident recipe has been proposed to correctly simulate such materials.</p>
<p>In example, in the previous work by Cirera et al., they presented a selection of SCO complexes and calculated an estimation of the transition temperature by using Gibbs free energy expression:</p>
<p><span class="math-container">$$\tag{1}\Delta G = \Delta H - T\Delta S $$</span></p>
<p>Since at equilibrium, <span class="math-container">$\Delta G$</span> vanishes, the transition temperature (<span class="math-container">$T_{1/2}$</span>) is determined by:</p>
<p><span class="math-container">$$\tag{2}T_{1/2} =  {{\Delta H} \over {\Delta S}}$$</span></p>
<p>From this equation (more details on the article), the key contribution to obtain the transition temperature is the enthalpy difference between the high and low spin state, which can be computed using DFT.</p>
<p>To illustrate the complexity of this procedure, I have chosen the molecule <span class="math-container">$\ce{Fe(SCN)_2(Phen)_2}$</span>, considered the Droshophila of SCO materials, which has been modeled with TPSSh Funcional. The results obtained are:</p>
<p>Experimental <span class="math-container">$T_{1/2}$</span> = 176.5 K</p>
<p>Theoretical (TPSSh Functional) <span class="math-container">$T_{1/2}$</span> = 454 K (basis set 1) and 237 K (basis set 2)</p>
<p>As can be easily seen, we are still far from the accurate result.</p>
<p>My question is:
Up to now, which is the most correct way of simulating the SCO transition? How realistic is it to reproduce the hysteresis on the magnetization properties in a temperature sweep?</p>

Best Answer: <p>The question highlights the difficulty in calculating an <em>ab initio</em> value for <span class="math-container">$T_{1/2}$</span> for a molecule like <span class="math-container">$\textrm{Fe(Phen)}_2\textrm{(SCN)}_2$</span>, and specifically the fact that a highly cited 2018 paper predicts values that are between 60K and 278K higher than the experimental value of 176.5K.</p>

<p>The paper highlights a major danger of using DFT, and in using theory to try to predict something that is out of reach with today's computing technology and algorithms.</p>

<p>The paper uses many approximations to calculate <span class="math-container">$T_{1/2}$</span> for a molecule. They begin with the approximation that it can be calculated as follows:</p>

<p><span class="math-container">\begin{align}T_{1/2} &amp;= \frac{\Delta H}{\Delta S} \\  &amp;\approx  \frac{\Delta E_\textrm{electronic}+\Delta E_\textrm{vibrational}}{\Delta S},
\end{align}</span></p>

<p>where only <span class="math-container">$\Delta E_\textrm{electronic}$</span> is attempted accurately because they claim near their Eq. 3 that "the harmonic approximation" is good enough for <span class="math-container">$\Delta E_\textrm{vibrational}$</span> and <span class="math-container">$\Delta S$</span> (see their Eq. 8 for more details). In general if you want to calculate <span class="math-container">$\Delta S$</span> and <span class="math-container">$\Delta H$</span> <em>ab initio</em>, resources that might help you are  <a href="https://pubs.acs.org/doi/abs/10.1021/jp981195o" rel="noreferrer">this paper</a> for <span class="math-container">$\Delta S$</span>, and <a href="https://cccbdb.nist.gov/enthformation.asp" rel="noreferrer">this step-by-step guide</a> for <span class="math-container">$\Delta S$</span>, but as the authors claim that the harmonic approximation is good enough for everything except <span class="math-container">$\Delta E_\textrm{electronic}$</span>, let's zoom into how <span class="math-container">$\Delta E_\textrm{electronic}$</span> is calculated.</p>

<p>The system mentioned in the question is labelled S11 in the table below, and you can see that with 8 different functionals, values for the HS vs LS energy gap vary from -14.6 to +9.6 kcal/mol (<strong><em>a 25 kcal/mol range for a number estimated to be at most 15 kcal/mol in magnitude</em></strong>), with only 3 out of 8 functionals even giving the correct sign. For reference, the term "chemical accuracy" means an accuracy of +/- 1 kcal/mol. As someone who mainly works in the "high-precision <em>ab initio</em>" field, I would stop right here and start working on a different project, however the brave scientists noticed that the TPSSh functional predicted the sign correctly for all 20 systems:</p>

<p><a href="https://i.sstatic.net/yPooM.png" rel="noreferrer"><img src="https://i.sstatic.net/yPooM.png" alt="enter image description here"></a></p>

<p>I lack experience with DFT, so I personally would try to develop more experience to decide how significant it is to me that one functional is getting all the signs correctly (I think I personally would try more functionals until I find another one that gets all signs correctly, and then compare the magnitudes, since what i see in the table indicates that the magnitudes vary wildly from what people call "chemical precision"). However the authors have more experience with DFT than me, and they have decided to use the TPSSh functional for the rest of their calculations. The <span class="math-container">$\Delta E_\textrm{electronic}$</span> could have been 1/3 of what it was for TPSSh if they used B3LYP* instead, and could have been the opposite sign and almost 2x larger in magnitude if they used M06 (which I understand is reputed to be good for transition-metal containing systems like the system in question), so you can see that we can get a very wide range of <span class="math-container">$T_{1/2}$</span> values if we wish.</p>

<p>After deciding that the TPSSh functional was the only one that got the signs correctly for all 20 systems studied in the above table, they increased the size of the basis set from what they call BS1 to what they call BS2.</p>

<p><strong>Your question notes that there is a 217K difference between the result obtained with BS2 vs BS1, a different 2 times larger than the amount needed to boil water.</strong></p>

<p>Furthermore, the difference between the two basis sets is really not very big:</p>

<ul>
<li>BS1 = QZVP for the Fe atom and TZV for all other atoms</li>
<li>BS2 = TZVP for all atoms (this may have been a typo, since it would be strange that they choose not to use QZVP for the Fe atom in the bigger basis set, but maybe that would make the overall basis set too big, but I find that hard to believe and it doesn't matter much because it's just one of the dozens of overall atoms).</li>
</ul>

<p>This tiny difference in the basis set (keeping <span class="math-container">$\zeta$</span> the same but just increasing polarization functions, and perhaps also reducing <span class="math-container">$\zeta$</span> for the Fe atom) caused a difference of 217K in <span class="math-container">$T_{1/2}$</span>, so again I would probably <strong><em>stop here and try to do a basis set convergence study</em></strong> (for example, looking at cc-pVDZ, cc-pVTZ, and an extrapolation, and comparing it with another 2Z/3Z extrapolation). Personally I probably would have tried to study 5 molecules more thoroughly (with a basis set convergence study and more supporting evidence for my choice of functional) rather than 20 molecules the way they did, but I also do not work in this field and cannot fully appreciate the motivations behind the study.</p>

<hr>

<p>So how can the prediction for <span class="math-container">$T_{1/2}$</span> be calculated more accurately? I would recommend the following things:</p>

<ul>
<li><p>This study showed that a minor difference in basis set resulted in a 217K change in <span class="math-container">$T_{1/2}$</span>, so I would do a basis set extrapolation with at least two different basis set series and see if there is some reasonable number on which the different series agree. If the basis set extrapolations give numbers that are vastly different, and I cannot afford to use a larger basis set or to incorporate explicit correlation through something like an F12 method, then there is no shame in saying that the problem has defeated me. With so many atoms, it is understood that the complete basis set is not approachable easily (if at all).</p></li>
<li><p>This study shows that different functionals give vastly different results. The number of atoms is rather large, but not out of reach for CCSD and CCSD(T). Transition metal systems can be challenging, especially for multi-reference spin states, but this system does not look <em>impossible</em> for coupled cluster. A comparison between MP2, CCSD, and CCSD(T) may show energies that are closer together than the different functionals used in this paper, and the agreement between CCSD and CCSD(T) can be an inidication of the overall accuracy of the calculation. Another option would be CASSCF, RASSCF or GASSCF and CASPT2/RASPT2 with <a href="https://pubs.acs.org/doi/abs/10.1021/acs.jctc.9b00532" rel="noreferrer">OpenMOLCAS</a>, which may allow for better accuracy but will cost more human time since these are not black box methods.</p></li>
<li><p>The DKH relativistic treatment in the paper is most likely okay, but for the same cost they could have done X2C which is equivalent to "infinite order DKH".</p></li>
</ul>


================================================================================

Question: What is the importance of electron interaction on dielectric response of crystals?
Body: <p>After obtaining the Kohn-Sham orbitals from a plane-wave-based self-consistent-field calculation, the dipole matrix elements could be calculated in order to determine electro-optical properties such as the dieletric response of the material. For instance, Quantum ESPRESSO has two different approaches to accomplish it, namely the epsilon.x program, and TDDFPT (time-dependent density functional perturbation theory) with the turbo_eels.x program. The first one does not take into account electron-electron interaction, the latter does. Both of them are very time consuming and sometimes don't converge at all. It has been shown that such interaction is relevant for molecules (small ones, at least). What about periodic crystals, should I be concerned about it?  </p>

Best Answer: <p>I think you should take care of all possible interactions to get close to the real picture. In periodic solids, there might be electron-hole interaction (solve BSE equation for it), el-phonon coupling, etc. Note that, QE epsilon.x is the lowest level of approximation for the solids (IPA) and it doesn't include any non-local part and local field effects. Moreover, you can incorporate the many-body effects in the solids using certain TDDFT kernels as well, like TDDFT-LRC (static and dynamic) and if you have solid with low electronic dielectric function and small lattice screening than the excitonic effects might play a big role even in the bulk systems, so you better solve BSE in those cases instead of RPA level of spectra. 
I recommend to take your system into account carefully and then choose the right theory to apply.</p>

<p>Best of luck,</p>

<p>Haseeb Ahmad.</p>


================================================================================

Question: What class of materials are closest to realizing the tunable coupling Hamiltonian?
Body: <p>From a physics point of view, there is an effective (approximation to second-order coupling Jaynes-Cummings) Hamiltonian of the form [1]
<span class="math-container">\begin{equation}
H=\sum_j\omega_j(t)\sigma_j^z+\sum_{\langle i,j\rangle}J_{ij}(t)(\sigma_i^+\sigma_j^- + \text{h.c.}) 
\end{equation}</span>
where <span class="math-container">$J_{ij}(t)$</span> is tunable, meaning that it can always be tuned continuously w.r.t. a parameter (such as capacitance in superconducting circuits). <span class="math-container">$\langle i,j\rangle$</span> denotes the sum over adjacent spin-1/2's that are coupled to each other. We may assumed ferromagnetic or anti-ferromagnetic configuration. But my focus shall be in just the functional form of the Hamiltonian. This coupling Hamiltonian actually resembles the <span class="math-container">$XY$</span> model
<span class="math-container">\begin{equation}
H_{XY}=\sum_{\langle i,j\rangle}2J_{ij}(\sigma_i^+\sigma_j^- + \text{h.c.})=\sum_{\langle i,j\rangle}J_{ij}(\sigma_i^x\sigma_j^x +\sigma_i^y\sigma_j^y)
\end{equation}</span>
where <span class="math-container">$J_{ij}$</span> are fixed in experiment, not tunable. My aim is to see the purely material implementation for the same where we have some tunable parameter to control and we can selectively and dynamically choose each <span class="math-container">$J_{ij}$</span> for the coupling for each pair.
Please suggest such materials or theoretical models for such materials if they exist or are proposed.</p>
<p>[1] Fei Yan,  Philip Krantz,  Youngkyu Sung,  Morten Kjaergaard,  Daniel L. Campbell,  Terry P. Orlando, Simon Gustavsson, and William D. Oliver.  <a href="https://journals.aps.org/prapplied/abstract/10.1103/PhysRevApplied.10.054062%5D" rel="nofollow noreferrer">Tunable Coupling Scheme for Implementing High-Fidelity Two-Qubit Gates</a>. Phys. Rev. Applied, 10:054062, Nov 2018.</p>

Best Answer: <h2>Realizing this Hamiltonian in a natural material</h2>
<ul>
<li>I cannot imagine a material in which all nearest-neighbor spin-spin interactions can be adjusted arbitrarily at the same time. Spin-spin couplings that are stronger when the spins are closer together, and weaker when the spins are farther apart, can be adjusted by moving the spins relative to each other; so if we have a 2D sheet of spins and we pull it from all four corners, all the spin-spin distances will get larger, and in that sense we might have <span class="math-container">$J_{ij}(t) = J_{ij}/t$</span>, meaning that the coupling potential decreases linear as we do the stretching. <em>But not only will the couplings of the nearest neighbors be affected, but <strong>all</strong> couplings will, since all distances change.</em> There would also not be much control over the functions <span class="math-container">$J_{ij}(t)$</span>. Perhaps we would have to change the couplings without changing relative distances, but in some other way, such as with a laser, but I don't know how this would happen (an NMR expert might).</li>
<li>Any Hamiltonian with only Pauli operators is quite an approximation of real-world materials existing in nature. The electronic Hamiltonian of a material does not really resemble this Hamiltonian, but the nuclear Hamiltonian used in NMR studies might resemble it more closely (if spin-orbit coupling and other types of terms can be ignored, and <em>if the nuclear spins indeed are <span class="math-container">$\pm 1/2$</span> like in the case of hydrogen atoms</em>). Hopefully an NMR expert can give a better answer here.</li>
</ul>

<h2>Realizing this Hamiltonian in an artificial material</h2>
<p>Apart from the example in the paper you gave, the closest example I know is from a <a href="https://journals.aps.org/prapplied/abstract/10.1103/PhysRevApplied.13.034037" rel="noreferrer">paper published</a> fewer than 2 months before you asked this question. The Hamiltonian is given in the image I posted in <a href="https://quantumcomputing.stackexchange.com/q/4314/2293">this question</a>, where the time-dependent &quot;driver&quot; part is shown in a bit more general form in Eq. 3 of the paper I mentioned (whether viewing on arXiv or Physical Review, it's still Eq. 3). I will repeat the driver and &quot;problem&quot; Hamiltonians here for the case where the coupling in the driver and the problem Hamiltonians are chosen to be the same (which needs to be perfectly doable anyway):</p>
<p><span class="math-container">\begin{align}
\tag{1}
H_{\rm{driver}} &amp;= \sum_{i}\omega_i \sigma^x_i + \sum_{ij}J_{ij}\sigma_i^y\sigma_j^y \\
\tag{2}
H_{\rm{problem}} &amp;= \sum_{ij}J_{ij}\sigma_i^z\sigma_j^z\\
\end{align}</span></p>
<p>Now assuming that the normal AQC driving is done, we get:</p>
<p><span class="math-container">\begin{align}
\tag{3}
H(t) &amp; = (1-t)H_{\rm{driver}} + tH_{\rm{problem}} \\
\tag{4}
     &amp; = \sum_{i}(1-t)\omega_i \sigma^x_i + \sum_{ij}(1-t)J_{ij}\sigma_i^y\sigma_j^y 
+ \sum_{ij}tJ_{ij}\sigma_i^z\sigma_j^z\\
\tag{5}\label{similar}
     &amp; =\sum_{i}\omega_i(t) \sigma^x_i + \sum_{ij}J_{ij}(t)\sigma_i^y\sigma_j^y 
+ \sum_{ij}K_{ij}(t)\sigma_i^z\sigma_j^z,\\
\end{align}</span></p>
<p>where I have defined:</p>
<p><span class="math-container">\begin{align}
\tag{6}
\label{defineTimeDependence}
\omega_i(t) \equiv (1-t)\omega, &amp; ~~~~~~J_{ij}(t)\equiv (1-t)J_{ij}, &amp; K_{ij}(t)\equiv tJ_{ij}.
\end{align}</span></p>
<p>When  <span class="math-container">$t=1/2$</span>, we get:</p>
<p><span class="math-container">\begin{align}
\tag{7}
  H(t)  &amp; =\sum_{i}\omega_i(t) \sigma^x_i + \sum_{ij}J_{ij}(t)\left(\sigma_i^y\sigma_j^y +
\sigma_i^z\sigma_j^z\right).\\
\end{align}</span></p>
<p>Now let's apply the unitary transormation:</p>
<p><span class="math-container">\begin{align}
\tag{8}
  H(t)  &amp; =\sum_{i}\omega_i(t) \sigma^x_i + \sum_{ij}J_{ij}(t)\left(\sigma_i^y\sigma_j^y +
\sigma_i^z\sigma_j^z\right),\\
\end{align}</span></p>
<p>which is strikingly close to your Hamiltonian:</p>
<p><span class="math-container">\begin{align}
\tag{9}
  H(t)  &amp; =\sum_{i}\omega_i(t) \sigma^z_i + \sum_{ij}J_{ij}(t)\left(\sigma_i^y\sigma_j^y +
\sigma_i^x\sigma_j^x\right).\\
\end{align}</span></p>
<p>So the closest I could get was the same Hamiltonian except it's only precisely the same as yours when:</p>
<ul>
<li><span class="math-container">$t=1/2$</span> holds true in my case (Eq. <span class="math-container">$\eqref{similar}$</span> is quite similar for other values of <span class="math-container">$t$</span> though).</li>
<li>Eq. <span class="math-container">$\eqref{defineTimeDependence}$</span> holds true in your case.</li>
<li><span class="math-container">$X$</span> is interchanged with <span class="math-container">$Z$</span>.</li>
</ul>
<p>In the 2-spin case, if a material needs to behave like Eq. <span class="math-container">$\eqref{similar}$</span>, it can work if it contains the following superconducting circuit with Josephson junctions represented by the x's (it looks only slightly different from the circuit <a href="https://quantumcomputing.stackexchange.com/q/4314/2293">here</a> maybe because this circuit is actually reported to be able to have linear <span class="math-container">$Z$</span> terms in addition to the linear <span class="math-container">$X$</span> terms, but I've set those field strengths to 0 in my analysis, which is a perfectly fine thing to do):</p>
<p><a href="https://i.sstatic.net/Jiaca.png" rel="noreferrer"><img src="https://i.sstatic.net/Jiaca.png" alt="enter image description here" /></a></p>


================================================================================

Question: How can very small lattices be sufficient for Quantum Monte Carlo simulations?
Body: <p>Quantum Monte Carlo simulations are often performed with very modest lattice sizes (<a href="https://doi.org/10.1103/PhysRevB.96.035129" rel="noreferrer">such as</a> [<a href="https://journals.aps.org/prb/pdf/10.1103/PhysRevB.96.035129?casa_token=yZdAejuhHagAAAAA%3AGDpAOW46hZvGnqthfK2Y9g-OzI1z53kS-EqD3GPJce9OzNWG5Wg3hvoSH9OIbQ9c1OdttL_F1kl7" rel="noreferrer">e-print</a>], <span class="math-container">$64$</span> sites), due to their computational cost increasing exponentially with the system volume; while classic Monte Carlo simulations usually require much larger lattice sizes, even when making use of finite-size analysis.</p>

<p>Why are these small lattices enough for obtaining physically meaningful results for quantum systems? Is it simply a case of "we take what we can get", or is there some intrinsically quantum mechanism in action<sup>[1]</sup>?</p>

<p>[1]<sub> Such as a symmetry-breaking magnetic field removing degeneracies for free electrons in a tight binding approximation. Ref.: NIC Series Vol. 10 (<a href="http://www.john-von-neumann-institut.de/nic/EN/Publications/NICSeries/nic-series-1-10.html?nn=1487798" rel="noreferrer">ISBN 3-00-009057-6</a>) (<a href="https://juser.fz-juelich.de/record/24560/files/NIC-Band-10.pdf" rel="noreferrer">pdf 1</a>, <a href="https://core.ac.uk/download/pdf/35009945.pdf" rel="noreferrer">2</a>), page 131.</sub></p>

Best Answer: <p>I think you are correct that there is an aspect of "take what you can get" to the sizes that are typically used in numerical methods. Even with finite size scaling (FSS), you usually try to go to the largest size that is practical with your computational resources. Case in point: people do finite size scaling with <em>extremely</em> small sizes for exact diagonalization calculations. </p>

<p>It's worth noting that the finite size effects are themselves interesting and often <a href="https://arxiv.org/abs/1603.04359" rel="nofollow noreferrer">contain important physical quantities</a></p>

<p>There is one more specifically-quantum feature to consider: the finite size gap and the size-temperature tradeoff. In finite-size quantum systems, even gapless quantities have a finite-size gap <span class="math-container">$\Delta \propto 1/L$</span> (basically because <span class="math-container">$L$</span> becomes the maximum wavelength). This gap contributes directly to the finite-size effects, but also separates the "finite-size zero-temperature" and "finite-temperature large-size" regimes:</p>

<ul>
<li>When <span class="math-container">$\beta \Delta \gg 1$</span>, then effectively the temperature is zero, because all the excited states, even the 'gapless' ones, are suppressed. </li>
<li>When <span class="math-container">$\beta \Delta \ll 1$</span>, the finite size effects are weaker because the system 'can't see' the finite-size gap.  </li>
</ul>

<p>The <strong>computational cost</strong> of QMC (at least the form I use, SSE) roughly scales like <span class="math-container">$\beta L^d$</span>. Because of this finite-size gap, you need <span class="math-container">$\beta \propto L$</span> to maintain the same `effective' temperature as you go to larger and larger sizes. As a result, zero-temperature properties are <strong>much</strong> easier to access for small sizes (with a large gap, and not-too-low <span class="math-container">$T$</span> requirement). </p>

<p>If you want to avoid finite-size effects, then you want to <strong>avoid</strong> low temperatures so you aren't detecting this finite-size gap. (Or you need a large system so <span class="math-container">$\Delta$</span> is small).  </p>


================================================================================

Question: Norm-Conserving or Ultrasoft, which pseudopotential to choose?
Body: <p>Ultrasoft pseudopotential (USPP), in general, requires a lower energy-cutoff to the plane waves when compared to Norm-Conserving ones (NC). On the other hand (at least with Quantum ESPRESSO) the charge-density cutoff should be bigger for the USPP, where it is always kept 4 times the energy cutoff for the NC. Apart from that, which other characteristics should be taken into account when choosing the pseudopotential? Are there specific properties more or less sensitive to one or another?</p>

Best Answer: <p>Three criteria to consider are</p>

<ol>
<li><strong>Performance and size of basis set</strong> 
USPPs generally require a lower plane-wave cut-off and smaller basis, but a larger density grid. However performance may not be straightforwardly related to cut off as there are additional terms to compute which may have a significant computational cost.</li>
<li><strong>Accuracy</strong>
USPPs usually have 2 (or more) projectors per angular momentum state, which linearizes the energy. Traditional Troullier-Martins and Rappe/Cambridge NCPs had only one and were consequently less accurate. Do beware of older generations of NCPs for this reason, but recent multi-projector NCPs, nowadays known as "Optimized Norm-Consetving Vanderbilt" are nearly as accurate as USPs.  See the <a href="https://molmod.ugent.be/deltacodesdft" rel="noreferrer">Delta project website </a> for some examples.</li>
<li><strong>Implementation restrictions and constraints</strong>
Some functionality may be implemented only for NCPs, so USPs may not be used.</li>
</ol>


================================================================================

Question: How to choose replica exchange interval?
Body: <p>I was wondering what goes into the consideration of choosing the frequency to attempt swaps for replica exchange? <a href="https://doi.org/10.1063/1.2816560" rel="noreferrer">D. Sindhikara, 2008</a> proposes the faster the better, but this goes against the view that you should allow the systems to relax to their new conditions.</p>

Best Answer: <p>The sampling enhancement in replica exchange (REMD) comes completely from transitions between your different distributions, so the more transitions you attempt, the faster you will achieve equilibrium between all your different distributions. However, MD doesn't start generating locally decorrelated samples before ~1-10 ps of sampling (the true number will depend on the nature of the system and the topology of the local minimum you are exploring), so below this timescale you are very unlikely to observe any transitions of interest, even in your most "mobile" replica. Your samples are never completely decorrelated in the global sense, however, but this is something we accept and is the reason we use REMD in the first place.</p>

<p>Therefore, any swap attempts after less time than your decorrelation time are still "correct" in the sense that they will converge to the right distributions but they are very unlikely to provide any increase in sampling quality (remember that the only transitions of interest are the ones that won't occur in the replica of interest in that timescale). And as was already stated, swaps need energy evaluations, which need CPU time / computational overhead and if you attempt swaps below your decorrelation time, while not technically wrong, you are quite literally wasting your time. If that computational overhead was not needed, there wouldn't be any reason not to run swaps at each timestep - there would be nothing to lose by doing that.</p>

<p>So I would say anything between 1-10 ps is fair game in the sense that in order to obtain the optimal parameter, you need to have knowledge which you can only obtain by running enhanced sampling in the first place! This agrees with the idea of "relaxation", which actually just means that you obtain a typical sample around your local minimum which has no local memory of the previous sample. Again, global memory is still there, so "relaxation" (and "local") is fundamentally a vague and timescale-dependent term - the space you sample locally within 1 ps is much more narrow than the space you sample within 1 ms.</p>


================================================================================

Question: I am a beginner in DFT. What are some resources that could help me to learn the basics?
Body: <p>I am a beginner in DFT. Please suggest me some good material to understand the basics. </p>

Best Answer: <p>Assuming you are interested in pursuing plane-wave periodic DFT, there is a clear best answer in my opinion and that's <a href="https://www.wiley.com/en-us/Density+Functional+Theory%3A+A+Practical+Introduction-p-9780470373170" rel="noreferrer">"Density Functional Theory: A Practical Introduction"</a> by Sholl and Steckel. For a relatively complex topic, they truly manage to accomplish the goal of making this a <em>practical</em> introduction. As they say in the intro, "you don't need to learn how to build a car to learn how to drive from Point A to Point B", and I think that mentality helps a lot with getting started with DFT. It still has enough detail to make it not overly superficial though.</p>

<p>If you're using VASP, I also recommend <a href="http://kitchingroup.cheme.cmu.edu/dft-book/dft.html" rel="noreferrer">"Modeling materials using density functional theory"</a> by Kitchin. It's a useful hands-on resource for learning how to use VASP, particularly through the useful program the <a href="https://wiki.fysik.dtu.dk/ase/" rel="noreferrer">Atomic Simulation Environment (ASE)</a>.</p>


================================================================================

Question: Hexagonal lattice volume EOS fit
Body: <p>I have a little problem concerning hexagonal volume fitting using VASP.
I have followed these steps:</p>

<ol>
<li><p>Relax the structure from a given volume using <code>ISIF =4</code></p></li>
<li><p>Copy CONTCAR to POSCAR and relax it again</p></li>
<li><p>Run with <code>ISMEAR = -5</code> and without relaxation</p></li>
<li><p>Repeat for different volumes (+10% +5% 0% -5% -10%)</p></li>
<li><p>Use your script for EOS fitting.</p></li>
</ol>

<p>It worked successfully with cubic systems. But for a hexagonal system (e.g. Carbon), I got the figure below. I don't know why I always get wrong results for the first volume point. Even with 7 volume points the first point always wrong.<a href="https://i.sstatic.net/x6hYC.png" rel="noreferrer"><img src="https://i.sstatic.net/x6hYC.png" alt="Equation of state fitting"></a></p>

Best Answer: <p>First of all, thanks to everyone who contributed to answer and discuss this question, your responses helped me to found the solution. Which is the vdW correction was missing in my calculations. Since that WS2 is a layered material, vdW interlayer correction must be added. So, I have added IVDW=10 in INCAR file to include the DFT-D2 method and this gaves me good results.</p>


================================================================================

Question: Running Quantum ESPRESSO calculations in Google Colab
Body: <p>So one big problem I experience in doing calculations with Quantum ESPRESSO is that many calculations require a lot of computing power: you need large RAM capacities and powerful processors especially when there are many atoms in a solid state model and the computation requires a dense k-point grid. It becomes really impractical for a desktop setup at some point.</p>

<p>Some peers of mine from the computer science department of our university said that since we don't have an HPC here (I'm from a 3rd world country), what they do is they run their programs (which they wrote themselves) in free cloud computing services such as the Google Colab.  </p>

<p>My question is this: Is there a way to run Quantum Espresso on Google Colab?</p>

Best Answer: <p>A little background on why I asked this question: </p>

<p>I have a desktop PC with an AMD Ryzen 5 processor, 4 cores 8 logical threads and 32GB DDR4 RAM. I'm trying to do DFT-GGA calculations on a 2D Xene with hydrogen atoms adsorbed at the surface. I'm using a 4x4 supercell as a model for a total of 32 atoms (16 for X and 16 for H). Without spin-orbit coupling (SOC), the total dynamically allocated RAM required for relaxation and SCF calculations is somewhere in the range of 4-6 GB. When I try to account for SOC, the memory required jumps to around 20 GB, so I'm guessing performing phonon or band calculations would require much much more than 30 GB, and enlarging the cell to 5x5 or 6x6 would require an even greater memory capacity. Not to mention, it would also require more processing power to finish the calculations at a decent amount of time. </p>

<p>So I checked out Google Colab some more, after seeing the replies to this question. While it looks really possible to install an executable like Quantum Espresso into Google Colab, the free RAM can be boosted only up to 25 GB, which is in my opinion, less RAM than what I would need. </p>

<p>Update/Edit: I actually also found a page which seems to walk you through how to do it: <a href="https://qiita.com/cometscome_phys/items/a31ab8a4a4f7217a70ff" rel="noreferrer">https://qiita.com/cometscome_phys/items/a31ab8a4a4f7217a70ff</a>. It's in Japanese so I don't understand everything it says but I get the basic gist.</p>


================================================================================

Question: What are the ways to ensure thermodynamic stability of a DFT modelled new structure?
Body: <p>One way of predicting the thermodynamic stability of a DFT modelled structure is to calculate the energy above convex hull, which was used as the criterion in <a href="https://materialsproject.org/" rel="noreferrer">The Materials Project</a> database. For example, if I modelled the compound <span class="math-container">$\ce{BaSr(FeO3)2}$</span>, I could compare its potential energy with that of the possible decompositions such as <span class="math-container">$\ce{SrFeO3}$</span> &amp; <span class="math-container">$\ce{BaFeO3}$</span> and make a judgement on the stability. One might think this process is tedious because there can be several possible decompositions.</p>

<p>Another way of predicting thermodynamic stability at <strong>room temperature</strong> is to perform molecular dynamic (MD) simulations<a href="https://onlinelibrary.wiley.com/doi/full/10.1002/smtd.201900360" rel="noreferrer"> [1]</a>. VASP supports MD simulations (<code>IBRION=0</code>) too.</p>

<p>What are the other commonly used methods of predicting stability of new structures?</p>

<p>[ 1 ]  Lu, S., Zhou, Q., Ma, L., Guo, Y., Wang, J., Rapid Discovery of Ferroelectric Photovoltaic Perovskites and Material Descriptors via Machine Learning. Small Methods 2019, 3, 1900360. <a href="https://doi.org/10.1002/smtd.201900360" rel="noreferrer">https://doi.org/10.1002/smtd.201900360</a> </p>

Best Answer: <p>This is a tricky question and there is a whole area of research dedicated to solving it. The definition of the thermodynamically stable phase of a system is that occupying the <em>global</em> minimum of the potential energy surface, or more precisely, the global minimum of the potential <em>free energy</em> surface at the temperature of interest. Let me focus on solid phases, and divide the answer into two cases:</p>

<ol>
<li><p>Elemental compounds. For compounds containig a single element, then the question simply becomes: what is the structure that has the lowest energy? As trivial as this question may sound, it has no general solution. In principle you should try all possible structures (with any number of atoms in the primitive cell) in all possible atomic arrangements to find the global minimum. This is a computational problem with exponential scaling, so it has no solution. Although there is no general answer, the past 15 years or so have seen great progress, with DFT-based <a href="https://en.wikipedia.org/wiki/Crystal_structure_prediction" rel="noreferrer">structure prediction methods</a> using a variety of algorithms (stochastic, genetic, etc.) capable of exploring many structures and providing reasonable answers to the question posed. Despite the success of these methods, note that they do not provide a general solution: to give an example, structure searches are limited to about <span class="math-container">$100$</span> atoms per cell, but it could very well be that the primitive cell of a material has <span class="math-container">$1000$</span> atoms per cell, which means that the structure could never be found unless that many atoms are used in the simulation. </p></li>
<li><p>Non-elemental compounds. For compounds with more than one element, you need to add an extra degree of freedom to the search space: the stoichiometry of the compound. What you should do is the same structure searching in point 1, but now for every possible stoichiometry. Again, it may be that the correct stoichiometry is one you never try, so you would completely miss the thermodynamically stable structure in this case.</p></li>
</ol>

<p>Up to this point, I have discussed the difficulties in scanning through all possible structures, arising from the extremely high dimensionality of the structural and composition spaces of materials. But let's imagine you have gone through the exercise of generating a range of candidate structures for your system. Then the next step is to figure out which one is the lowest (free) energy structure. The first step taken is typically to do a static lattice DFT calculation, which gives the electronic contribution to the energy. Then a finite temperature calculation is done to include the vibrational contribution to the energy at a given temperature. This calculation can be done either using the harmonic approximation to lattice dynamics (phonons), or a molecular dynamics simulation as you described. The former is fine at relatively low temperatures for most materials, and the latter necessary at higher temperatures when anharmonic vibrations become important. </p>

<p>The free energies you get from these calculations allow you to decided which is the lowest energy of your elemental system (for example if you do this for carbon, you will find that graphite is the ground state as it has a lower energy than diamond), and it will also tell you the correct stoichiometry of a compound by building the corresponding Hull diagram (as you pointed out, this is being done in the Materials Project). To be precise, these calculations will tell you what the lowest free energy structure/compound is amongst those you try. However, remember that you can never be sure that you didn't miss a lower energy material in your initial searches.</p>

<p>At this point the situation may look desperate. However, in practise it is many times possible to determine the actual thermodynamically stable structure. The obvious answer is that there may be experimental data. Even if there is only partial experimental data (for example no hydrogen positions, which are difficult to determine using X-rays), then these experiments can provide important constraints that can guide the theory and dramatically reduce the dimensionality of the search space, for example by providing the space group of the material. Finally, even if there is no data, many systems adopt relatively high-symmetry structures with relatively few atoms in the primitive cell. This makes the structure searches easier, as symmetry constraints reduce the dimensionality of the space being explored, and small primitive cells maximize the chances that you are exploring the correct number of atoms in the primitive cell. Indeed, structure prediction has been able to predict multiple structures that have subsequently been confirmed experimentally.</p>


================================================================================

Question: Is there a set of updated, comprehensive benchmarks for speed comparison between different quantum chemistry packages?
Body: <p>I've found a page dedicated to quantum chemistry packages benchmarks, on GitHub, <a href="https://github.com/PedroJSilva/qmspeedtest" rel="noreferrer">qmspeedtest</a>. But most results there are several years old, and so probably outdated. Is there some place where we can find comparisons like these, but updated often, or at least more recently?</p>
<p>I specified quantum chemistry in the question because I'm more interested in molecular systems, modeled with atom-centered gaussian function basis sets, for example. I have almost no familiarity with software that deals with periodic systems, plane-wave based. But I think it could be a good idea if someone with more familiarity with periodic systems opened a similar question for the respective packages.</p>

Best Answer: <p>The problem is that this is a highly volatile question. In order to meaningfully benchmark programs, you have to use the exact same compiler flags (may require heavy hacking) and use the same algorithms and parameters (accuracy, cutoffs, quadrature grids, etc). But, if a program supports many kinds of algorithms, then each of them would have to be benchmarked. In contrast, qmspeedtest is  comparing apples to oranges; it is making no effort to actually ensure that the core algorithms and parameters are the same. It is for good reason that some programs explicitly ban publishing benchmark comparisons.</p>
<p>If you still intend to proceed, a good benchmark should look at these two core questions first:</p>
<ul>
<li>speed of a single Fock build i.e. how quickly do you get a single-point energy from a given density</li>
<li>speed of gradient evaluation i.e. how quickly do you evaluate forces from a converged wave function</li>
</ul>
<p>These are well-posed problems which are reproducible and where there is a single meaningful answer. This also means that the energy and Fock matrix / the nuclear gradient you get out from the benchmarks should agree numerically exactly between different codes. (You still do have several choices in the way to evaluate the final solution, e.g. density fitting, Cholesky decomposition, fast multipoles, etc, which may give different answers!)</p>
<p>Now, running a full calculation also depends on these issues:</p>
<ul>
<li>cycles taken until SCF convergence i.e. how good is the default SCF guess and the default convergence accelerator for the system you're looking at</li>
<li>steps taken until geometry optimization converges i.e. how sophisticated is the geometry optimizer (use of internal coordinates? empirical force constants / exact second derivatives?)</li>
</ul>
<p>While the first two issues, which are purely a question of speed, are somewhat important in practical applications, it is actually the latter two issues that in many cases are the most important for a workflow. If you're studying challenging molecules, you may face cases of poor SCF convergence, and this is where a flexible algorithm makes all the difference. You shouldn't care if program A solves an easy molecule in 5 steps while program B takes 7 steps to solve it, if for a challenging case program A takes 3000 steps but program B only 40. But, these issues are highly system dependent, and depend heavily on the algorithm. Using a second-order algorithm (e.g. trust region) yields more robust convergence, but even though the calculation now may converge in few steps they are much more expensive than with a simple gradient descent method; this is why you should compare apples to apples and use the exact same algorithms in all programs, and study a large variety of systems to try to cover a large sample of both &quot;easy&quot; and &quot;difficult&quot; cases.</p>
<p>I would note last that speed is not everything. Also the ease of use of the program and its general availability are key questions in determining which tool to use. If program A is 3x faster than program B, but B is easier/safer to use, most people would opt for program B.</p>
<p>Programs have also become more modular than before; this may also affect your choice: if it's easy to modify one program to do exactly what you want, it becomes your tool of choice even if it's not as fast as its competitors.</p>


================================================================================

Question: Meaning of atomic positional parameters
Body: <p>I am trying to model some materials with DFT, so I intended to start out with the structure. However, some texts that present XRD data, use a term called atomic positional parameter. I can't seem to figure out what this means. Here's an image from <a href="https://www.researchgate.net/publication/244638749_Bis1a8b-dihydro11-2H2-2H-benzoacyclopropaccyclohepten-2-onehydrogenI_hexachloroantimonate_HC12H8D2O2SbCl6" rel="noreferrer">a paper</a>:
<a href="https://i.sstatic.net/yRAIk.png" rel="noreferrer"><img src="https://i.sstatic.net/yRAIk.png" alt="enter image description here" /></a></p>
<p>Can someone help breakdown what format the coordinates are represented in ?</p>

Best Answer: <p>Above the table that you provided in the screenshot, is a caption that says what these numbers mean:</p>
<p><a href="https://i.sstatic.net/jo1Qx.png" rel="noreferrer"><img src="https://i.sstatic.net/jo1Qx.png" alt="Table header" /></a></p>
<p>I don't blame you for coming here to ask this, because the notation (<span class="math-container">$\times 10^4$</span>) can be confusing for someone that's not used to it (and I don't think it's taught in school, generally). But what it means is that numbers like 1383, which is the <span class="math-container">$x$</span>-coordinate (along the <span class="math-container">$a$</span> axis of the unit cell) given for the 6th carbon atom, C(6), actually mean 0.1383. <strong>The number 1383(3) means (0.1383 +/- 0.0003).</strong> Similarly, the <span class="math-container">$y$</span> and <span class="math-container">$z$</span> coordinates are actually fractional lengths along the <span class="math-container">$b$</span> and <span class="math-container">$c$</span> axes of the crystal unit cell.</p>
<p>I also don't blame you if you're confused by what these numbers mean, because the table's caption says &quot;atomic positional parameters&quot; which seems (at least to me) as an unconventional way of saying &quot;atomic coordinates&quot;. At least a different part of the exact same paper describes this table as listing atomic coordinates though:</p>
<img src="https://i.sstatic.net/2TrRQ.png" width="400">
<p>Table 2 also has the &quot;internal coordinates&quot; that you would use if your modeling program requires input in ZMAT format.</p>
<hr>
<p><strong>In summary:</strong> The table in your question corresponds to the fractional coordinates for the atoms with respect to the unit cell (although it also has uncertainties on each coordinate value). You mentioned in the comments that you're used to seeing exact fractions like 1/4 and 3/4, but in this case the molecule is not so simple like textbook lattice structures such as NaCl. So, when they are measured experimentally, they won't correspond to simple fractions and  will have an associated uncertainty.</p>
<p>In the above case the fraction would be <em>approximately</em> 1383/10000, but remember that there's an uncertainty, so in fractional form the <span class="math-container">$x$</span>-coordinate would actually be:</p>
<p><span class="math-container">$$
\frac{1383 \pm 3}{10000}.
$$</span></p>


================================================================================

Question: What are the types of SCF?
Body: <p><strong>Many of us know the most common types of SCF</strong> <br>
(though <a href="https://en.wikipedia.org/wiki/Wikipedia:Stack_Exchange_is_eating_our_lunch_" rel="nofollow noreferrer">we can do better than Wikipedia</a> at explaining them):</p>
<ul>
<li>RHF (Restricted Hartree-Fock), RKS (Restricted Kohn-Sham) [<a href="https://mattermodeling.stackexchange.com/a/1573/5">link to answer</a>]</li>
<li>UHF (Unrestricted Hartree-Fock), UKS (Unrestricted Kohn-Sham) [<a href="https://mattermodeling.stackexchange.com/a/1592/794">link to answer</a>]</li>
<li>ROHF (Restricted Open-Shell Hartree-Fock), ROKS (Restricted open-shell KS)</li>
</ul>
<p><strong>But there's also less commonly known (and more powerful!) single-reference SCF methods:</strong></p>
<ul>
<li>GHF (Generalized Hartree-Fock), GKS (Generalized Kohn-Sham) [<a href="https://mattermodeling.stackexchange.com/a/1571/5">link to answer</a>]</li>
<li><a href="https://aip.scitation.org/doi/10.1063/1.4705280" rel="nofollow noreferrer">PHF</a> (Projected Hartree-Fock)</li>
<li>DHF (Dirac-Hartree-Fock) or Dirac-Fock [<a href="https://mattermodeling.stackexchange.com/a/1589/5">link to answer</a>]</li>
<li>KR-DHF (Kramers Restricted DHF)</li>
<li>KU-DHF (Kramers Unrestricted DHF)</li>
<li>SOSCF (Second-order Hartree-Fock)</li>
<li>TD-SCF (Time-dependent SCF)</li>
<li><a href="https://www.youtube.com/watch?v=LyvqjepHrnI" rel="nofollow noreferrer">Complex GHF</a></li>
<li><a href="https://journals.aps.org/pr/abstract/10.1103/PhysRev.122.1012" rel="nofollow noreferrer">GHF</a> (Generalized HF of Valatin in 1961).</li>
<li>GVB (Generalized Valence Bond) [<a href="https://mattermodeling.stackexchange.com/a/2272/5">link to answer</a>]</li>
</ul>
<p><strong>Let's be the resource where people can learn these methods succintly!</strong> <br>
(3 or fewer paragraphs please).</p>
<ul>
<li><a href="https://mattermodeling.stackexchange.com/q/1566/5">Multi-configurational SCF methods can instead go here.</a></li>
<li>Note that RHF <a href="https://aip.scitation.org/doi/10.1063/1.470488" rel="nofollow noreferrer">sometimes</a> stands for Roothan-Hartree-Fock.</li>
</ul>

Best Answer: <h2>DHF: Dirac-Hartree-Fock (or &quot;Dirack-Fock&quot;)</h2>
<p>the DHF (Dirac-Hartree-Fock) or Dirac-Fock is the SCF method based upon four-component spinors (simply four-spinors), because of the four-component Dirac-Coulomb(-Breit/Gaunt) Hamiltonian. The 4-spinors decribe both positive - electronic - solutions as well as  negative, or &quot;positronic&quot; solutions.  We are interested in electronic solutions, which are considered for correlated calculations.</p>
<p>4-spinors can be either Kramers restricted (KR) or Kramers unrestricted (KU). Four-component calculations are demanding because of extra small-component (S) basis functions. Good approximations to the Dirac Hamiltonian are two-component (2c) methods. 2c SCF then becomes identical with GHF.</p>
<p><strong>Program packages of my choice:</strong></p>
<ul>
<li><a href="http://www.diracprogram.org/doku.php" rel="noreferrer">DIRAC</a> based upon 4c/2c KR spinors (has both DFT and <em>ab initio</em> methods)</li>
<li><a href="http://www.respectprogram.org/" rel="noreferrer">ReSpect</a> upon KU spinors (has effective HF/DFT implementations)</li>
</ul>


================================================================================

Question: Is the number of possible Bravais lattices a mathematical fact?
Body: <p>Almost anyone that has taken a Solid State Physics course will know, that any 3D crystal must be in one of the 14 <a href="https://en.wikipedia.org/wiki/Bravais_lattice" rel="noreferrer">Bravais lattices</a> due to symmetry operations and space filling.</p>
<p>In 2015, a paper was published in <em>Scientific Reports</em><sup><a href="https://en.wikipedia.org/wiki/Bravais_lattice" rel="noreferrer">1</a></sup> (from <em>Nature publishing group</em>) that reports the findings of a crystal in a forbidden symmetry. The work attracted some <a href="https://www.forbes.com/sites/trevornace/2016/06/27/forbidden-symmetry-found-in-4-5-billion-year-old-meteorite/#2f2f4b8c4988" rel="noreferrer">attention</a> as the crystal was found in the Khatyrka meteorite. The composition of the crystal was <span class="math-container">$\ce{Al_{71}Ni_{24}Fe_5}$</span>. The figure below shows the high-resolution transmission electron microscopy (HRTEM).</p>
<img src="https://i.sstatic.net/NEasP.jpg" width="300" height="300">
<p>A similar issue came up in the same year when a new 2D tile was <a href="https://www.theguardian.com/science/alexs-adventures-in-numberland/2015/aug/10/attack-on-the-pentagon-results-in-discovery-of-new-mathematical-tile" rel="noreferrer">found</a>.</p>
<p>Are the claims about the crystals' &quot;forbidden&quot; symmetries due to a mistaken empirical hypothesis, a faulty mathematical theorem, or something else?</p>
<p><a href="https://en.wikipedia.org/wiki/Bravais_lattice" rel="noreferrer">1</a> Bindi, L., Yao, N., Lin, C. et al. <em>Natural quasicrystal with decagonal symmetry</em>. Sci Rep 5, 9111 (2015). (DOI: <a href="https://doi.org/10.1038/srep09111" rel="noreferrer">10.1038/srep09111</a>)</p>

Best Answer: <p><a href="https://en.wikipedia.org/wiki/Quasicrystal" rel="noreferrer">Quasicrystals</a> were known well before 2015. The recent developments is in showing that such crystals exist naturally, rather than just in synthesized samples. The first (nowadays generally accepted) clear demonstration of an aperiodic crystal was <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.53.1951" rel="noreferrer">published in 1984</a>. Examples of such crystals remain very rare compared to periodic crystals, however. So rare in fact, that the initial discovery was far from universally accepted or believed at first, rather it was ridiculed. For example, Linus Pauling was quoted as saying &quot;There is no such thing as quasicrystals, only quasi-scientists.&quot;. However, despite the controversy the first author of that 1984 paper, <a href="https://en.wikipedia.org/wiki/Dan_Shechtman" rel="noreferrer">Dan Schechtman</a>, was vindicated and later awarded the 2011 Nobel prize in Chemistry for this discovery. I would hope this matter is at least mentioned in Solid State courses these days, if only as a lesson in humility.</p>
<blockquote>
<p>The claims about the crystals &quot;forbidden&quot; symmetries are human made due to a lack of knowledge, due to mathematical failure or due something else?</p>
</blockquote>
<p>You could call it a lack of mathematical knowledge, or maybe imagination. Bravais' work on lattices was done in the 1840s, and remains true for translation-invariant lattices (specifically, lattices with translational invariance in all spatial dimensions of the crystal). Much later, in the 1960s mathematicians started being interested in aperiodic tilings, but they mostly remained a curiosity until the demonstration of quasicrystals. These break the basic assumption of Bravais lattices, namely that every lattice position can be written
<span class="math-container">$$
\mathbf{R}=n_1 \mathbf{a}_1+n_2 \mathbf{a}_2+n_3 \mathbf{a}_3,
$$</span>
where <span class="math-container">$n_i$</span> are integers, and <span class="math-container">$\mathbf{a}_i$</span> are primitive lattice vectors. Such a lattice structure strongly restricts the possible tilings, and hence the possible patterns in reciprocal space. This restriction produces a set of &quot;allowed&quot; symmetries in reciprocal space through what's known as the crystallographic restriction theorem.</p>
<p>Empirically, the same symmetries kept showing up in crystallographic experiments, all the way back to von Laue's seminal x-ray diffraction experiments. As a result, prior to Schechtman et al.'s work, the term crystal <a href="https://www.nobelprize.org/uploads/2018/06/shechtman-lecture_slides.pdf" rel="noreferrer">was generally defined</a> to require the atoms to be arranged periodically. Up to this point, the problem isn't really that the symmetries were called &quot;forbidden&quot;, rather it's how dogmatically the theorem was believed to describe nature, without exploring effects of breaking the underlying assumptions.</p>
<p>In contrast to Bravais lattices, quasicrystals have some kind of aperiodic or quasiperiodic way of tiling the plane / filling space. Such objects are a lot less natural to study than their periodic counterparts, so without direct evidence of them and the requisite mathematical machinery to make sense of them, they were basically not considered at all, or deemed unstable or impossible. One exception to the rule was a 1981 prediction by Alan Lindsay Mackay that a type of quasicrystals known as Penrose tilings would display a five-fold symmetry in reciprocal space, which would be one of the previously &quot;forbidden&quot; symmetries. Note that as this is a symmetry of a scattering pattern, it relies on how sites are arranged/ordered. Random impurity effects would not introduce a new symmetry like this.</p>
<hr />
<p>Finally, to directly address the question in the title</p>
<blockquote>
<p>Is the number of possible Bravais lattices a mathematical fact?</p>
</blockquote>
<p>Yes, it is. But Bravais lattices are periodic by definition, which crystals need not be.</p>


================================================================================

Question: How to deduce phase transitions from a phonon calculation?
Body: <p>I came across the concept of using phonons to establish a material's dynamical stability, based on whether or not imaginary frequencies are present in its phonon band structure.</p>
<p>What I am struggling with is how to determine the phase transitions based on a phonon band structure with such imaginary frequencies? From what I have read, the recipe is to <em>follow the imaginary phonons</em>. What does this mean? How does one follow such imaginary modes?</p>
<p>An example illustrating this using perovskite transitions from the high symmetry <span class="math-container">$Pm\overline{3}m$</span> space group to lower symmetries will be appreciated.</p>

Best Answer: <p><strong>Background theory.</strong> In the harmonic approximation, the potential energy surface (PES) is expanded about an equilibrium point to second order, to obtain the Hamiltonian:</p>
<p><span class="math-container">$$
\hat{H}=\sum_{p,\alpha}-\frac{1}{2m_{\alpha}}\nabla_{p\alpha}^2+\frac{1}{2}\sum_{p,\alpha,i}\sum_{p^{\prime},\alpha^{\prime},i^{\prime}}D_{i\alpha;i^{\prime}\alpha^{\prime}}(\mathbf{R}_p,\mathbf{R}_{p^{\prime}})u_{pi\alpha}u_{p^{\prime}i^{\prime}\alpha^{\prime}}.
$$</span></p>
<p>The basic quantity one builds when calculating phonons is the <em>matrix of force constants</em>:</p>
<p><span class="math-container">$$
D_{i\alpha;i^{\prime}\alpha^{\prime}}(\mathbf{R}_p,\mathbf{R}_{p^{\prime}})=\frac{\partial^2E}{\partial u_{pi\alpha}u_{p^{\prime}i^{\prime}\alpha^{\prime}}},
$$</span></p>
<p>which is the expansion coefficient of the second order term in the potential energy surface <span class="math-container">$E$</span>, with <span class="math-container">$i$</span> labelling the Cartesian direction, <span class="math-container">$\alpha$</span> the atom in the basis, <span class="math-container">$\mathbf{R}_p$</span> is the position of the cell <span class="math-container">$p$</span> in the crystal, and <span class="math-container">$u_{pi\alpha}$</span> is the amplitude of displacement of the corresponding atom. Using periodicity of the crystal, we can define the <em>dynamical matrix</em> at each <span class="math-container">$\mathbf{q}$</span>-point of the Brillouin zone as:</p>
<p><span class="math-container">$$
D_{i\alpha;i^{\prime}\alpha^{\prime}}(\mathbf{q})=\frac{1}{N_p\sqrt{m_{\alpha}m_{\alpha^{\prime}}}}\sum_{\mathbf{R}_p,\mathbf{R}_{p^{\prime}}}D_{i\alpha;i^{\prime}\alpha^{\prime}}(\mathbf{R}_p,\mathbf{R}_{p^{\prime}})e^{i\mathbf{q}\cdot(\mathbf{R}_p-\mathbf{R}_{p^{\prime}})},
$$</span></p>
<p>where <span class="math-container">$N_p$</span> is the number of cells in the supercell over which periodic boundary conditions are applied, and <span class="math-container">$m_{\alpha}$</span> is the mass of atom <span class="math-container">$\alpha$</span>. Diagonalizing the dynamical matrix gives eigenvalues <span class="math-container">$\omega^2_{\mathbf{q}\nu}$</span> and eigenvectors <span class="math-container">$v_{\mathbf{q}\nu;i\alpha}$</span>. From these, it is possible to define a set of normal coordinates:</p>
<p><span class="math-container">$$
u_{\mathbf{q}\nu}=\frac{1}{\sqrt{N_p}}\sum_{\mathbf{R}_p,i,\alpha}\sqrt{m_{\alpha}}u_{pi\alpha}e^{-i\mathbf{q}\cdot{\mathbf{R}_p}}v_{-\mathbf{q}\nu;i\alpha},
$$</span></p>
<p>in terms of which the Hamiltonian becomes a sum over uncoupled simple harmonic oscillators:</p>
<p><span class="math-container">$$
\hat{H}=\sum_{\mathbf{q},\nu}-\frac{1}{2}\frac{\partial^2}{\partial u_{\mathbf{q}\nu}^2}+\frac{1}{2}\omega^2_{\mathbf{q}\nu}u_{\mathbf{q}\nu}^2.
$$</span></p>
<p>The bosonic quasiparticles labelled by quantum numbers <span class="math-container">$(\mathbf{q},\nu)$</span> are called phonons, and have energy <span class="math-container">$\omega_{\mathbf{q}\nu}$</span> and momentum <span class="math-container">$\mathbf{q}$</span>.</p>
<p><strong>Dynamically stable structure.</strong> A dynamically stable structure is one whose equilibrium position is at a local minimum of the PES. As such, the eigenvalues <span class="math-container">$\omega^2_{\mathbf{q}\nu}$</span> of the dynamical matrix (Hessian) are all positive numbers, and as a consequence the phonon frequencies <span class="math-container">$\omega_{\mathbf{q}\nu}$</span> are all real.</p>
<p><strong>Dynamically unstable structure.</strong> A dynamically unstable structure is one whose equilibrium position is at a saddle point of the PES. As such, some of the eigenvalues of the dynamical matrix are negative, and the corresponding phonon frequencies imaginary.</p>
<p><strong>Physical interpretation.</strong> Phonons measure the <em>curvature</em> of the PES around the equilibrium position of the material. As we have seen, an imaginary frequency corresponds to a negative curvature, so it corresponds to a direction in the PES along which the energy <em>decreases</em>. This means that there is a lower energy configuration of the material, and we say that the structure is then <em>dynamically unstable</em>.</p>
<p><strong>&quot;Follow the imaginary modes&quot;.</strong> How can we find such lower-energy structure? The eigenvectors of the dynamical matrix associated with the imaginary phonons tell us the direction along which the energy decreases, so we can &quot;follow those modes&quot; to find the lower energy structure. This can be done by simply constructing a sequence of structures on which you displace the atoms by an amplitude <span class="math-container">$u_{\mathbf{q}\nu}$</span> (see equation above) of the imaginary phonon <span class="math-container">$(\mathbf{q},\nu)$</span>, and calculating the total energy of each of the resulting structures. For a saddle point, the resulting curve will be something like a double well, and the minima of the double well correspond to your new lower-energy structure.</p>
<p><strong>Finite temperature.</strong> The discussion up to this point concerns the potential energy surface, so temperature is neglected. If you are interested in a calculation at finite temperature, then you need the <em>free energy</em> surface. This is much harder to calculate and you need anharmonic terms in your Hamiltonian to describe it properly.</p>
<p><strong>Perovskites.</strong> Perovskites typically have a cubic structure at high temperature, and then upon lowering the temperature undergo a number of phase transitions to lower-symmetry structures (tetragonal, orthorhombic, and so on). Imagine a perovskite that has only two phases, tetragonal at low temperature and cubic at high temperature (generalizing to more phases is trivial). Then if you calculate the phonons in the cubic structure (saddle point) you will find imaginary modes, and following them will take you to the tetragonal structure (minimum). If you calculate the phonons in the tetragonal structure, they will all have real frequencies. So why is the cubic phase stable at high temperatures? This is because, although the cubic phase corresponds to a saddle point of the potential energy surface, above some critical temperature it corresponds to a minimum of the free energy surface. As such, above that critical temperature the cubic phase becomes dynamically stable. As I mentioned above, to investigate this phase transition (e.g. to calculate the critical temperature), you need to include anharmonic terms (phonon-phonon interactions), which is much harder computationally.</p>


================================================================================

Question: Artificial muscles and their efficiency
Body: <p>Most of the artificial muscles are based on either pneumatic power or nylon fibers being heated so they contract. These methods consume more energy than what can be utilized.</p>
<p>Likewise, in early days when the light bulbs that produced photons by heating the filament (which also requires considerable energy), they were replaced by energy efficient light-emitting diodes.</p>
<p>Are there any promising materials that can do the same for artificial muscles by converting electric energy directly to actuation instead of into heat and actuation being the side product of the heat itself?</p>

Best Answer: <p>Hasel actuators can contact and expand fast and use veggtable oil and plastic. Here is a video on how the work <a href="https://youtu.be/Yi8tUJowAuo" rel="noreferrer">https://youtu.be/Yi8tUJowAuo</a></p>


================================================================================

Question: What is a chemputer?
Body: <p>I first saw the term &quot;chemputer&quot; when reading about the work of Neil Ostlund (who you may know as one of the authors of the famous book &quot;Modern Quantum Chemistry&quot;). Based on the description <a href="http://www.hyper.com/AboutUs/CompanyHistory/tabid/385/Default.aspx" rel="noreferrer">here</a>, my understanding is that a chemputer is a computer specifically designed to do computational chemistry calculations (and for clarity, I will say that I wouldn't consider any quantum computer a chemputer since none of them have enough qubits to do any useful chemistry calculation).</p>
<p>Last week the term &quot;chemputer&quot; arose again, along with the word ChemPU (analogous to CPU, GPU, and GPU) in the context of a paper published in Science a week ago, and discussed <a href="https://www.cnbc.com/2020/10/24/how-a-digital-breakthrough-could-revolutionize-drug-industry.html" rel="noreferrer">here</a>. This time the &quot;chemputer&quot; is described as a &quot;robotic chemist that can produce chemicals from XDL programs&quot;. The group that published the Science paper has been using the word chemputer in this context <a href="https://phys.org/news/2018-11-chemputer-app-controlled-revolution-drug-production.html" rel="noreferrer">since some years ago</a>.</p>
<p>There is also the phrase &quot;<a href="https://en.wikipedia.org/wiki/Chemical_computer" rel="noreferrer">chemical computer</a>&quot; whcih is described in the preceding link as a computer where computations are performed by naturally occurring chemical reactions.</p>
<p>I am curious to know if the word has been used in any way other than the above three ways. One may also pick one of the above three meanings of chemputer, and describe it in more detail in the <a href="/questions/tagged/one-topic-per-answer" class="post-tag" title="show questions tagged &#39;one-topic-per-answer&#39;" rel="tag">one-topic-per-answer</a> format. I am also curious to know if there's examples other than Neil Ostlund's one, of computers specifically made to do computational chemistry calculations (the first type of chemputer), but that seems different enough from my first question that it may better be asked separately.</p>

Best Answer: <blockquote>
<p>I am curious to know if the word has been used in any way other than the above three ways.</p>
</blockquote>
<p><a href="http://winter.staff.shef.ac.uk/" rel="noreferrer">Mark. J. Winter</a> at University of Sheffield has used the word differently — not as a term meaning something specific, but as a name for the so-called <a href="https://winter.group.shef.ac.uk/chemputer/" rel="noreferrer">Sheffield ChemPuter</a>:</p>
<blockquote>
<p>Welcome to the University of Sheffield's ChemPuter, a set of simple interactive calculators for chemistry on the World-Wide Web.</p>
</blockquote>
<p>This usage of the word clearly doesn't define &quot;<em><strong>a</strong> chemputer</em>&quot;, at least not in the sense expected in the question. Nevertheless, a Google Scholar search reveals that the set of codes has been cited or used a nontrivial number of times, so it seemed worthwhile to document it in an answer here.</p>
<p><a href="https://i.sstatic.net/zRoWH.png" rel="noreferrer"><img src="https://i.sstatic.net/zRoWH.png" alt="Screenshot from the Sheffield ChemPuter page" /></a></p>


================================================================================

Question: Convert XYZ coordinates to Z-matrix
Body: <p>I want to make a python script that will load an xyz file. From the xyz parameters, I need to find the distance between atoms, angle and dihedral between atoms.</p>
<p>The file xyz have this structure:</p>
<pre><code> 60  Buckminsterfullerene  (C60 Bucky Ball)
 1  C      0.56182991    1.03720708   -3.34153745     2     2     6    16
 2  C     -0.02867841   -0.20922332   -3.53733326     2     1     3    15
 3  C      0.68868170   -1.41687735   -3.17419275     2     2     4     7
 4  C      1.96798784   -1.33001731   -2.62971513     2     3     5     8
 5  C      2.58298268   -0.03190141   -2.42580025     2     4     6    12
 6  C      1.89418484    1.12766893   -2.77448204     2     1     5    11
 7  C     -0.26911535   -2.36054907   -2.62920302     2     3     9    21
 8  C      2.34254586   -2.18322716   -1.51766993     2     4    10    19
 9  C      0.09052931   -3.17978756   -1.56143494     2     7    10    43
10  C      1.42288437   -3.08932569   -0.99437947     2     8     9    36
11  C      1.93147043    2.28439304   -1.89955106     2     6    14    20
12  C      3.33762840   -0.08283139   -1.18772874     2     5    13    19
13  C      3.37342937    1.02783673   -0.34763393     2    12    14    32
14  C      2.65606922    2.23549084   -0.71077448     2    11    13    26
15  C     -1.42982841   -0.40652370   -3.21677674     2     2    17    21
16  C     -0.22432522    2.13802273   -2.81706602     2     1    18    20
17  C     -2.18468203    0.65046195   -2.71318765     2    15    18    27
18  C     -1.56968716    1.94857800   -2.50927274     2    16    17    22
19  C      3.18903027   -1.41242386   -0.62647337     2     8    12    34
20  C      0.62215914    2.90882602   -1.92586946     2    11    16    23
21  C     -1.57842655   -1.73611615   -2.65552140     2     7    15    29
22  C     -2.12435266    2.52208091   -1.29751971     2    18    24    28
23  C      0.08957811    3.45949446   -0.76236333     2    20    24    25
24  C     -1.31157175    3.26219407   -0.44180686     2    22    23    37
25  C      0.84422386    3.40856444    0.47570821     2    23    26    42
26  C      2.10140370    2.80899380    0.50097868     2    14    25    31
27  C     -3.11943529    0.42168510   -1.62746090     2    17    28    30
28  C     -3.08214969    1.57840920   -0.75252997     2    22    27    39
29  C     -2.47596183   -1.95578408   -1.61302381     2    21    30    44
30  C     -3.26211692   -0.85496848   -1.08855240     2    27    29    40
31  C      2.47596171    1.95578403    1.61302382     2    26    32    46
32  C      3.26211681    0.85496841    1.08855240     2    13    31    33
33  C      3.11943527   -0.42168513    1.62746090     2    32    34    57
34  C      3.08214970   -1.57840928    0.75252996     2    19    33    35
35  C      2.12435273   -2.52208091    1.29751964     2    34    36    56
36  C      1.31157182   -3.26219401    0.44180679     2    10    35    47
37  C     -1.42288432    3.08932572    0.99437953     2    24    38    42
38  C     -2.34254574    2.18322716    1.51766996     2    37    39    52
39  C     -3.18903022    1.41242378    0.62647331     2    28    38    41
40  C     -3.37342947   -1.02783679    0.34763388     2    30    41    48
41  C     -3.33762837    0.08283136    1.18772868     2    39    40    49
42  C     -0.09052935    3.17978761    1.56143497     2    25    37    45
43  C     -0.84422391   -3.40856443   -0.47570819     2     9    44    47
44  C     -2.10140382   -2.80899386   -0.50097868     2    29    43    48
45  C      0.26911532    2.36054910    2.62920311     2    42    46    54
46  C      1.57842653    1.73611617    2.65552148     2    31    45    60
47  C     -0.08957818   -3.45949441    0.76236329     2    36    43    53
48  C     -2.65606930   -2.23549092    0.71077442     2    40    44    50
49  C     -2.58298263    0.03190137    2.42580019     2    41    51    52
50  C     -1.93147042   -2.28439306    1.89955101     2    48    51    53
51  C     -1.89418482   -1.12766893    2.77448197     2    49    50    58
52  C     -1.96798776    1.33001737    2.62971509     2    38    49    54
53  C     -0.62215921   -2.90882598    1.92586939     2    47    50    55
54  C     -0.68868164    1.41687744    3.17419279     2    45    52    59
55  C      0.22432525   -2.13802261    2.81706605     2    53    56    58
56  C      1.56968719   -1.94857793    2.50927279     2    35    55    57
57  C      2.18468202   -0.65046198    2.71318771     2    33    56    60
58  C     -0.56182981   -1.03720706    3.34153744     2    51    55    59
59  C      0.02867849    0.20922332    3.53733334     2    54    58    60
60  C      1.42982839    0.40652370    3.21677684     2    46    57    59
</code></pre>
<p>Does anyone know how to do this?</p>

Best Answer: <h2>ASE</h2>
<p>The ASE library has an <a href="https://wiki.fysik.dtu.dk/ase/ase/atoms.html" rel="nofollow noreferrer">atom object</a>  with built-in <code>get_angle</code>, <code>get_dihedral</code> and <code>get_distance</code> methods that do just that.</p>
<p>Here's an example script inspired by <a href="https://mattermodeling.stackexchange.com/a/7114/1609">rsdel's answer</a> which uses ASE:</p>
<pre><code>import ase.io
import io      

xyz_file = &quot;&quot;&quot;60
            Buckminsterfullerene  (C60 Bucky Ball)
            C      0.56182991    1.03720708   -3.34153745   
            C     -0.02867841   -0.20922332   -3.53733326   
            C      0.68868170   -1.41687735   -3.17419275   
            C      1.96798784   -1.33001731   -2.62971513   
            C      2.58298268   -0.03190141   -2.42580025   
            C      1.89418484    1.12766893   -2.77448204   
            C     -0.26911535   -2.36054907   -2.62920302   
            C      2.34254586   -2.18322716   -1.51766993   
            C      0.09052931   -3.17978756   -1.56143494   
            C      1.42288437   -3.08932569   -0.99437947   
            C      1.93147043    2.28439304   -1.89955106   
            C      3.33762840   -0.08283139   -1.18772874   
            C      3.37342937    1.02783673   -0.34763393   
            C      2.65606922    2.23549084   -0.71077448   
            C     -1.42982841   -0.40652370   -3.21677674   
            C     -0.22432522    2.13802273   -2.81706602   
            C     -2.18468203    0.65046195   -2.71318765   
            C     -1.56968716    1.94857800   -2.50927274   
            C      3.18903027   -1.41242386   -0.62647337   
            C      0.62215914    2.90882602   -1.92586946   
            C     -1.57842655   -1.73611615   -2.65552140   
            C     -2.12435266    2.52208091   -1.29751971   
            C      0.08957811    3.45949446   -0.76236333   
            C     -1.31157175    3.26219407   -0.44180686   
            C      0.84422386    3.40856444    0.47570821   
            C      2.10140370    2.80899380    0.50097868   
            C     -3.11943529    0.42168510   -1.62746090   
            C     -3.08214969    1.57840920   -0.75252997   
            C     -2.47596183   -1.95578408   -1.61302381   
            C     -3.26211692   -0.85496848   -1.08855240   
            C      2.47596171    1.95578403    1.61302382   
            C      3.26211681    0.85496841    1.08855240   
            C      3.11943527   -0.42168513    1.62746090   
            C      3.08214970   -1.57840928    0.75252996   
            C      2.12435273   -2.52208091    1.29751964   
            C      1.31157182   -3.26219401    0.44180679   
            C     -1.42288432    3.08932572    0.99437953   
            C     -2.34254574    2.18322716    1.51766996   
            C     -3.18903022    1.41242378    0.62647331   
            C     -3.37342947   -1.02783679    0.34763388   
            C     -3.33762837    0.08283136    1.18772868   
            C     -0.09052935    3.17978761    1.56143497   
            C     -0.84422391   -3.40856443   -0.47570819   
            C     -2.10140382   -2.80899386   -0.50097868   
            C      0.26911532    2.36054910    2.62920311   
            C      1.57842653    1.73611617    2.65552148   
            C     -0.08957818   -3.45949441    0.76236329   
            C     -2.65606930   -2.23549092    0.71077442   
            C     -2.58298263    0.03190137    2.42580019   
            C     -1.93147042   -2.28439306    1.89955101   
            C     -1.89418482   -1.12766893    2.77448197   
            C     -1.96798776    1.33001737    2.62971509   
            C     -0.62215921   -2.90882598    1.92586939   
            C     -0.68868164    1.41687744    3.17419279   
            C      0.22432525   -2.13802261    2.81706605   
            C      1.56968719   -1.94857793    2.50927279   
            C      2.18468202   -0.65046198    2.71318771   
            C     -0.56182981   -1.03720706    3.34153744   
            C      0.02867849    0.20922332    3.53733334   
            C      1.42982839    0.40652370    3.21677684  
            &quot;&quot;&quot;

#read xyz file
atom = ase.io.read(io.StringIO(xyz_file), format = 'xyz')

# Take some example indices
i, j, k, l = 0, 1, 2, 3

# Distances
# between two indices 
atom.get_distance(i, j)
# between all atoms (distance matrix)
atom.get_all_distances()

# Angles
atom.get_angle(i, j, k)

# Dihedrals
atom.get_dihedral(i, j, k, l)
</code></pre>


================================================================================

Question: Are there dashboards displaying experimental properties of molecular materials?
Body: <p>A <b>dataset</b> is generally a collection of data.</p>
<p>A <b>database</b> is an organized collection of data, allowing different kinds of queries.</p>
<p>A <b>dashboard</b> is a graphical user interface that is employed to make sense of a dataset that is too large or complex to be simply visualized in a small number of plots.</p>
<p>We all know crystallographic databases have been around for a long time, and an effort is now ongoing to include the results of materials modelling in databases too. My question is however on <strong>graphically navigating experimental data on molecular materials</strong> beyond X-ray crystallography.</p>
<p>I have found several specialized databases and dashboards, oriented towards accelerated materials discovery, for &quot;solid state&quot; (extended) materials , e.g. <a href="https://www.topologicalquantumchemistry.com/" rel="nofollow noreferrer">this topological materials database</a> or this <a href="http://aam.ceb.cam.ac.uk/mof-explorer/" rel="nofollow noreferrer">dashboard for Metal Organic Frameworks</a> (see figure 1 below). By collecting existing data, they facilitate finding trends and exploring in new directions. There are of course also many similar efforts in the pharmaceutical industry.</p>
<p>It seems however that in other fields of chemistry and molecular materials science these tools are less common. In our group we recently manually data-mined the bibliography to build a dataset and a dashboard for <a href="https://rosaleny.shinyapps.io/simdavis_dashboard/" rel="nofollow noreferrer">experimental data visualization in molecular nanomagnets</a>  (see figure 2 below). We plan to use this to open new routes for materials modelling in this field. I am sure there have to be other examples beyond solid state materials and molecules oriented towards pharmaceutical applications, but am having a hard time finding them.</p>
<p>Can people point towards analogous <strong>dashboards</strong> beyond these two fields? A couple of example screenshots are included to give a graphical idea beyond the very brief definition for <em>dashboard</em> above.</p>
<p><a href="https://i.sstatic.net/lH0dR.png" rel="nofollow noreferrer"><img src="https://i.sstatic.net/lH0dR.png" alt="Dashboard for Metal Organic Frameworks" /></a>
Figure 1: Dashboard for Metal Organic Frameworks at <a href="http://aam.ceb.cam.ac.uk/mof-explorer/" rel="nofollow noreferrer">MOF Data Explorer</a>. (Moghadam, P.Z., Islamoglu, T., Goswami, S. et al. Computer-aided discovery of a metal–organic framework with superior oxygen uptake. Nat Commun 9, 1378 (2018). <a href="https://doi.org/10.1038/s41467-018-03892-8" rel="nofollow noreferrer">https://doi.org/10.1038/s41467-018-03892-8</a>)</p>
<p><a href="https://i.sstatic.net/ojcoc.png" rel="nofollow noreferrer"><img src="https://i.sstatic.net/ojcoc.png" alt="Dashboard for Single Ion Magnets" /></a>
Figure 2: Dashboard for Single Ion Magnets at <a href="https://rosaleny.shinyapps.io/simdavis_dashboard/" rel="nofollow noreferrer">SIMDAVIS App</a>. (Duan, Y., Rosaleny, L.E., Coutinho, J.T. et al. Data-driven design of molecular nanomagnets. Nat Commun 13, 7626 (2022). <a href="https://doi.org/10.1038/s41467-022-35336-9" rel="nofollow noreferrer">https://doi.org/10.1038/s41467-022-35336-9</a>)</p>

Best Answer: <p>I'm not sure if this will exactly answer your question because dashboard is loosely defined, but I find &quot;networks&quot; to be a great way to represent reaction data.</p>
<p>For example, Rxn4Chemistry categorizes reactions in a way that &quot;would be too large or complex to be simply visualized in a small number of plots.&quot;</p>
<p><a href="https://i.sstatic.net/s5Gl7.png" rel="noreferrer"><img src="https://i.sstatic.net/s5Gl7.png" alt="enter image description here" /></a>
<a href="https://rxn4chemistry.github.io/rxnfp/tmaps/tmap_ft_10k.html" rel="noreferrer">https://rxn4chemistry.github.io/rxnfp/tmaps/tmap_ft_10k.html</a></p>


================================================================================

Question: What is a Pad&#233; approximant?
Body: <p>I have been looking at using Goedecker-Teter-Hutter (GTH) pseudo-potentials and I came across the abbreviation PADE.</p>
<p>I was wondering what this abbreviation actually stood for and how it is related to LDA?</p>

Best Answer: <p>To complement <a href="https://mattermodeling.stackexchange.com/a/6552/5">mykd's already excellent answer</a>, I will just add that the approximant we all learn in school (the Taylor approximant) is nice to teach and easy to help students learn the concept of approximations, but in practice it's one of the worst options in terms of it's accuracy-to-complexity ratio.</p>
<p>The Taylor approximant matches the <span class="math-container">$n^{\textrm{th}}$</span> derivative at a single point. It is <em><strong>extremely rare</strong></em> that you would actually care about the <em><strong>8th derivative</strong></em> (for example) matching at a <em><strong>single point</strong></em>. More likely, you would want the first, second and maybe third derviatives to match <em><strong>at several points</strong></em> (i.e. you want the whole function to be well approximated). But in the example below, even the 7th order Taylor approximant is not enough for the approximant to match the original function at <span class="math-container">$x=7$</span>:</p>
<p><a href="https://i.sstatic.net/5esgKm.png" rel="noreferrer"><img src="https://i.sstatic.net/5esgKm.png" alt="enter image description here" /></a></p>
<p>That is very unfortunate since a 7th degree polynomial will shoot off to <span class="math-container">$\pm \infty$</span> extremely rapidly (as high-order polynomials always do), so even if such an approximant was excellent at describing the original function in some small region, it would extrapolate horrendously.</p>
<p>Furthermore, imagine trying to approximate a very simple function like <span class="math-container">$1/x$</span> with a (Taylor) polynomial near <span class="math-container">$x=0$</span>. This would be quite impossible, but imagine if instead of trying to come up with the best polynomial approximation, we instead tried to come up with the best <em><strong>rational</strong></em> function approximation. Now we have the flexibility of adjusting the coefficients in <em><strong>two</strong></em> polynomials (the one in the numerator and the one in the denominator) instead of just one of them like in the Taylor series, and the <em><strong>singularities</strong></em> where the denominator of the original function is zero, can now be treated very naturally.</p>
<p><em><strong>That is precisely the reason for Padé functions: Trying to give the best rational approximation to a function, rather than just the best polynomial approximation to a function.</strong></em></p>
<p>Here are just some of the many different types of approximants which are often superior to the Taylor series, for various practical applications, but are unfortunately not given much attention, even in advanced-level calculus courses:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Pad%C3%A9_approximant" rel="noreferrer">Padé approximants</a></li>
<li><a href="https://en.wikipedia.org/wiki/B%C3%A9zier_curve" rel="noreferrer">Bézier curves</a></li>
<li><a href="https://en.wikipedia.org/wiki/Spline_(mathematics)" rel="noreferrer">Splines</a></li>
<li><a href="https://en.wikipedia.org/wiki/Chebyshev_polynomials" rel="noreferrer">Chebyshev approximants</a></li>
</ul>
<p><strong>For discussion about generalizations of Padé approximants to functions of more than one variable, you can see <a href="https://mathoverflow.net/a/413017/66334">my answer on the topic at MathOverflow</a>.</strong></p>


================================================================================

Question: GUIs for Quantum Chemistry... Where are they?
Body: <p>I have a general and maybe a little silly / funny question.</p>
<p>Why don't most CompChem or QuantumChem software have a GUI? SIESTA is trying with Simune as far as I know, but even proprietary software like GAUSSIAN or VASP is not doing any good.</p>
<p>It seems that the development of a GUI for such software is somewhat obvious and would cause an explosion of interest. Essentially, with a good GUI all you would need to do DFT calculations, say as an experimentalist then, would be an understanding of DFT and even a marginal understanding of the parameters. You can then create an * input * file from a series of lists and choices, then run the calculation with a button (a bit like the optimization function in AVOGADRO but with DFT and many more parameters). You can then add a graphical interface for your spectra and a visualization program. You can maybe interface this with the multiple software that are available (sort of like what ASE is doing ?).</p>
<p>I'm a bit lost with what I am trying to say. Maybe a sort of MATLAB for matter modelling would be the more appropriate way of defining what I feel is lacking in this space.</p>
<p>EDIT on 18/11/2021 : Thank you everyone for these very clear answers !</p>

Best Answer: <p>Gaussian has <a href="https://gaussian.com/gaussview6/" rel="nofollow noreferrer">Gaussview</a>, QChem has <a href="http://iqmol.org/index.html" rel="nofollow noreferrer">IQMol</a>, Schrodinger has <a href="https://www.schrodinger.com/products/maestro" rel="nofollow noreferrer">Maestro</a>, <a href="https://www.synopsys.com/silicon/quantumatk/atomistic-simulation-products.html" rel="nofollow noreferrer">Quantum ATK</a> and <a href="https://www.3ds.com/products-services/biovia/products/molecular-modeling-simulation/biovia-materials-studio/" rel="nofollow noreferrer">BIOVIA Material Studio</a> each have their own GUI. Notice, that both the program and GUI in these cases are paid products (except IQMol). There are also paid GUIs that work for several different programs, such as the <a href="https://www.scm.com/amsterdam-modeling-suite/" rel="nofollow noreferrer">Amsterdam Modeling Suite</a> and <a href="https://www.chemcraftprog.com/" rel="nofollow noreferrer">Chemcraft</a>. This hints at part of the problem: for developers in academia/industry, time spent on a GUI is time spent not doing other research or developing their core software. This leads to developers charging for their GUIs, only incrementally developing them, or just not making one at all.</p>
<p>However there are also a number of free GUI, both for setting up and actually running calculations. To give just a few examples:</p>
<ul>
<li><a href="https://onlinelibrary.wiley.com/doi/full/10.1002/jcc.21600" rel="nofollow noreferrer">Gabedit</a>- allows you build molecules, make/run input files for several different electronic structure programs, and view results from output.</li>
<li><a href="https://avogadro.cc/" rel="nofollow noreferrer">Avogadro</a> and <a href="https://github.com/OpenChemistry/avogadroapp" rel="nofollow noreferrer">Avogadro2</a>- builds molecules, makes input files, and views results.</li>
<li><a href="https://sourceforge.net/projects/mocalc2012/" rel="nofollow noreferrer">MoCalc2012</a>- interface to Avogadro/JSmol to make molecules, can make/run input files for several different electronic structure programs.</li>
<li><a href="https://www.webmo.net/" rel="nofollow noreferrer">WebMO</a>- build molecules, make/run input, view results.</li>
</ul>
<p>Some of these are supported by donations, some have tiers where only the basic program is free, and some have ceased development altogether. While there are a number of programs that can build/visualize molecules, it can challenging to make a program that goes from build-&gt;input-&gt;results because of the variety of programs that one could potentially have to support for the latter two steps, not to mention possible licensing issues with interfacing to these external programs.</p>
<p>The takeaways from all this are:</p>
<ol>
<li>There <em>are</em> a wide variety of GUI programs, both free and proprietary, available for various stages of setting-up/post-processing a calculation. Some even work end to end.</li>
<li>It might be more challenging than you would think to make a general purpose GUI for even just Quantum Chemistry simulations, let alone the various other types of Computational Matter Modeling (Molecular Dynamics, Docking, Cheminformatics, Material Design, etc).</li>
</ol>


================================================================================

Question: What major advances in theoretical and computational chemistry have been made in recent decades?
Body: <p>Computational modeling has become a mainstay in chemistry, materials science and physics. However, the methods that are typically employed are already decades old: for instance, the B3LYP and PBE functionals that dominate simulation in molecular and solid-state contexts date back to 1994 and 1996, respectively; some 30 years ago!</p>
<p>To convince the general public on the importance of continued investment in methods development, I want to ask what major breakthroughs have been made in theoretical and computational chemistry in the last 30 years. What can we do now that we couldn't do before? What kinds of methods are scientists unaware of, yet which could significantly affect the way chemistry research is carried out in academia and the private sector?</p>

Best Answer: <h1><a href="https://www.frontiersin.org/articles/10.3389/fchem.2021.749779/full" rel="nofollow noreferrer">The GW miracle</a></h1>
<h2>Intro: What sets GW developments apart from others?</h2>
<p>This is a great question! The top answer to &quot;<a href="https://mattermodeling.stackexchange.com/a/1132/5">What are some recent developments in density functional theory?</a>&quot; mentioned double hybrid functionals, but these and the other answers don't constitute methods that were nearly as prolific as the introduction of hybrid functinals (1993, Becke) or TD-DFT (1984, Runge &amp; Gross) or CPMD (1985, Car &amp; Parinello). Double hybrids  can significantly improve accuracy while significantly increasing computational expense, but also having largely the same shortfalls as previously existing DFT methods (the major one being the lack of a way to systematically improve functionals). Similarly, other recent methods that come to mind, such as arbitrary-order coupled cluster (2001, Kállay &amp; Surján) and newer approaches to FCI starting with FCIQMC (2009, Both, Thom &amp; Alavi) offer previously unimaginable accuracy but at an expense that is too great for the type of popularity that your examples of B3LYP and PBE have enjoyed. Also, it feels hard to think of something entirely &quot;new&quot;, as I have so-far mentioned DFT, CC, and CI, which were all well-known since far before the 1990s, until I consider <strong>DMRG</strong> which is very different from all of those approaches, but again it's too computationally expensive to reach the level of popularity of B3LYP and PBE.</p>
<p>Using <a href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.85.693" rel="nofollow noreferrer">explicitly correlated Gaussians on systems with more than 2 electons</a>, or abandoning Gaussians altogether with <a href="https://pubs.acs.org/doi/10.1021/acs.jctc.2c00982" rel="nofollow noreferrer">multiresolution appraoches</a>, still fall into the category of improving accuracy for small systems but failing to help with treating the size of systems that B3LYP and PBE can. The invention of better Hamiltonians such as the X2C, SFDC or eQED Hamiltonians, and post-HF implementations of <a href="https://pubs.aip.org/aip/jcp/article-abstract/125/14/144111/295640/Analytic-calculation-of-the-diagonal-Born?redirectedFrom=fulltext" rel="nofollow noreferrer">DBOC</a>, were great achievements but don't have the type of impact that B3LYP and PBE did, because their importance is limited to a smaller subset of systems, and &quot;decent&quot; alternatives did previously exist (DK1 from 1974, or <a href="https://pubs.aip.org/aip/jcp/article-abstract/84/8/4481/89782/The-diagonal-correction-to-the-Born-Oppenheimer?redirectedFrom=fulltext" rel="nofollow noreferrer">HF level DBOC from 1986</a>).</p>
<p><em><strong>The GW method offers superior accuracy to B3LYP and PBE, but with a lower cost than double-hybrids, coupled cluster, DMRG, and FCIQMC or other approaches to FCI; and the improvement in accuracy offered by GW methods exceed in magnitude or applicability, what we get from X2C vs DK1 or other mentioned method developments from the last 30 years.</strong></em></p>
<h2>G<sub>0</sub>W<sub>0</sub> was introduced in 1965, so what's different now?</h2>
<p>The original G<sub>0</sub>W<sub>0</sub> method appeared in 1965 <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.86.472" rel="nofollow noreferrer">here</a>, and it was used for electron gases in the 1960s, on periodic solids through the 1970s and 1980s, and <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.47.15404" rel="nofollow noreferrer">on atoms in 1993</a> but it's application to non-periodic molecules <strong><a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.86.472" rel="nofollow noreferrer">first appeared around 2001</a></strong> and has started to become very popular since the introduction of the <a href="https://pubs.acs.org/doi/full/10.1021/acs.jctc.5b00453" rel="nofollow noreferrer">GW100 benchmark set</a> (along wtih its recent implementation in ABINIT, BerkeleyGW, FHI-AIMS, GPAW, MolGW, PySCF, Quantum ESPRESSO, TURBOMOLE, VASP, YAMBO, and many other software packages).</p>
<h2>Why is GW becoming so popular for chemistry?</h2>
<p>Nearly 5 years ago, <a href="https://arxiv.org/abs/2001.08954" rel="nofollow noreferrer">it was already possible to do GW calculations on the GW5000 dataset</a> (5239 molecules, some of them with more than 100 nuclei, which is beyond what can comforatably be done even with coupled-cluster, even in packages like ORCA that specialize in doing CC for systems wtih dozens of nuclei). <em><strong>GW is very computationally facile compared to other attempts to go beyond B3LYP or PBE in accuracy.</strong></em> Furthermore, GW methods were traditionally benchmarked against the CCSD(T) &quot;gold standard&quot; for the GW100 benchmark set, but in the latest comparisons (see <a href="https://www.frontiersin.org/articles/10.3389/fchem.2021.749779/full" rel="nofollow noreferrer">here</a> and papers that cite it), for many molecules it has become hard to tell whether the CCSD(T) or GW calculations are more accurate, as the properties calculated with GW methods seem to match experimental data close to as well as CCSD(T) does (and in many cases the GW calculations seem to match experiments even better than the &quot;benchmark&quot; CCSD(T) calculations).</p>
<p>Also, new GW infrastructure seems to be constantly developed these days, for example a <a href="https://arxiv.org/abs/2401.11303" rel="nofollow noreferrer">relativistic GW implementation recently appeared on arXiv in 2024</a>.</p>
<h2>What about systematic improvement?</h2>
<p>One of the main criticisms of DFT, which I mentioned before, is that it's not very systematically improvable. <em><strong>GW methods fall in the category of &quot;perturbation theories&quot;, which can be systematically improved with higher perturbation orders</strong></em>. In the paper in which we introduced the term &quot;GW miracle&quot;, we complemented GW in five different ways: with second-order exchange (SOX), with second-order screened exchange (SOSEX), with interacting electron-hole pairs, and with a GW density matrix. The &quot;GW miracle&quot; is that although  <em><strong>we immediately know how to improve GW</strong></em> if we desire (unlike DFT), comparison to such complements showed that the &quot;basic&quot; GW method is already in a sweet-spot in terms of the balance between accuracy and speed. Also, the paper showed that GW calculations can be improved simply by improving the functional used for the non-interacting Green's function, meaning that whatever improvements in DFT that the future brings us, will likely also further improve GW calculations.</p>


================================================================================

Question: How to evaluate spin exchange parameters of a magnetic material using DFT?
Body: <p>I think we have to calculate total energies using different FM, AFM configurations and solve a set of linear equations. How to arrive at the set of equations for any given structure?</p>

<p>An example from the literature: <a href="https://doi.org/10.1016/j.ssc.2009.01.030" rel="noreferrer"><em>Solid State Communications</em> <strong>2009</strong>, 149 (15–16), 602–604</a>.</p>

Best Answer: <p>First of all, you need to be familiar with <strong>models for magnetic exchange</strong>. Two important examples are <a href="https://en.wikipedia.org/wiki/Quantum_Heisenberg_model" rel="nofollow noreferrer">Heisenberg</a> and <a href="https://en.wikipedia.org/wiki/Ising_model" rel="nofollow noreferrer">Ising</a>:</p>
<p>Depending on the system you are studying, you will be using one of these. Typically, for very anisotropic spins with a very well defined axis for the spin projection, you should use the Ising model, which is simpler. On the other hand, more isotropic spins will require a Heisenberg model.</p>
<p>These models will give you the energy difference per ion pair between the parallel and antiparallel configuration. Your calculation, for extended solids, will typically give you energies for different spin alignings in a fragment of the crystal containing several magnetic ions. The energies of the fragments will therefore differ in a larger value than the magnetic exchange parameter; you need to adjust the energies accordingly. This can be as simple as a linear factor.</p>
<p>In the example you provide, the set of equations arises because there are different (super)exchange interaction pathways, that present different strenghts. These need to be estimated independently, and this is why you need more than two energies to extract the needed information univocally.</p>


================================================================================

Question: Computational tool for identification of activity cliff
Body: <p>Activity cliffs are defined as pairs of similar compounds with substantial differences in activity (<a href="https://doi.org/10.1007/s11030-015-9609-z" rel="nofollow noreferrer"><em>Molecular Diversity</em>, 19, 1021–1035 (<strong>2015</strong>)</a>). Are there computational tools for the identification of activity cliffs between two analog compounds? This paper lists chemotypes that have a high propensity to induce activity cliff. </p>

<p><a href="https://i.sstatic.net/kT3Zf.gif" rel="nofollow noreferrer"><img src="https://i.sstatic.net/kT3Zf.gif" alt="enter image description here"></a></p>

Best Answer: <p>A 2015 paper <sup>[1]</sup> looked at the ability to predict activity cliffs structurally using docking models that could combine experimental and computational information about the conformation of the receptor and/or binding molecule. They found that they could achieve reasonable prediction accuracy and planned to implement further improvements to make a more robust predictor model.</p>

<p>[1] J. Husby, G. Bottegoni, Irina Kufareva, Ruben Abagyan, Andrea Cavalli, <em>J. Chem. Inf. Model.</em> <strong>2015</strong>, 55, 5, 1062-1076 <a href="https://doi.org/10.1021/ci500742b" rel="noreferrer">DOI:10.1021/ci500742b</a> (Also freely available <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4553268/" rel="noreferrer">here</a>)</p>


================================================================================

Question: Correct jobs sequence for transition-state (TS) search
Body: <p>I've seen many papers analyzing reaction pathway using a small basis set like 6-31G* for all optimization steps and later single point calculations in bigger one (like 6-311++G**).
What is the correct sequence of jobs for TS search in such a case?
Should it be:</p>
<ol>
<li>Frequency (6-31G*)</li>
<li>TS_search (6-31G*)</li>
<li>Single Point Energy (6-311++G**)</li>
<li>Frequency (6-311++G**)</li>
</ol>
<p>or</p>
<ol>
<li>Frequency (6-31G*)</li>
<li>TS_search (6-31G*)</li>
<li>Frequency (6-31G*)</li>
<li>Single point Energy (6-311++G**)</li>
<li>Frequency (6-311++G**)</li>
</ol>
<p>?</p>

Best Answer: <p>J1 Freq (6-31G*); J2 TS_search (6-31G*); J3 Freq (6-31G*);</p>

<p>is good because it let's you optimize the saddle point and then get the thermodynamic data </p>

<p>For the single-point correction don't recalculate the frequencies because the thermodynamic corrections won't change much anyway. You can also include implicit solvent in J4. </p>


================================================================================

Question: When should I include semi-core electrons in DFT calculations?
Body: <p>Many DFT codes use pseudopotentials (for the core electrons) and basis set functions (for the valence electrons) in order to solve the Schrodinger equation. This because simulates each electron wavefunction is very costly.</p>
<p>I found works where the authors generate the pseudopotentials dividing the core electrons in electrons from the inner core and electrons for the external core (called semi-core). These semi-core electrons were also considered as &quot;valence&quot; electrons.</p>
<p>When should I include semi-core electrons in DFT calculations?</p>

Best Answer: <p>I am quoting from the book <a href="https://books.google.co.in/books?id=FzOTAwAAQBAJ&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false" rel="noreferrer">Materials Modelling Using Density Functional Theory: Properties and Predictions</a> by Feliciano Giustino</p>

<blockquote>
  <p>How do we decide which wavefunctions should be considered ‘core’ and which ones ‘valence’ states? As a rule of thumb, in the context of DFT calculations the ‘valence’ corresponds to the outermost shell of the atom in the periodic table; for example, for tungsten we would have 6s<sup>2</sup>5d<sup>4</sup>. However, there are cases where one might need to include more electronic states in the set of ‘valence electrons’. <strong>For example, in the case of bismuth it is important to describe on an equal footing both the nominal valence shell, 6s<sup>2</sup>6p<sup>3</sup> , and the ‘semi-core’ shell, 5d<sup>10</sup></strong>. In practice the distinction between core and valence is not a strict one, and depends on the level of accuracy that one is trying to achieve. <strong>When in doubt, an inspection of the spatial extent of all the atomic wavefunctions  represents the first point of call for identifying core and valence states.</strong></p>
</blockquote>


================================================================================

Question: How trustworthy is the Tran-Blaha modified Becke-Johnson potential and how does it perform across basis sets?
Body: <p>When modeling solid-state materials and particularly semiconductors, one must go beyond LDA and GGA. One alternative is to use hybrid functionals or the <span class="math-container">$GW$</span> method. However, this can be very prohibitive for certain systems.</p>

<p>A popular alternative is to use the Tran-Blaha modified Becke-Johnson potential<a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.102.226401" rel="noreferrer"> [Phys. Rev. Lett. <strong>102</strong>, 226401 (2009)]</a>, </p>

<p><span class="math-container">\begin{align}
\mathbf{v}_{x,\sigma}^{TB-mBJ}(\textbf{r})=c \mathbf{v}_{x,\sigma}^{BR}(\textbf{r}) + (3c-2)\frac{1}{\pi}\sqrt{\frac{5}{12}}\sqrt{\frac{2t_\sigma(\textbf{r})}{\rho_\sigma(\textbf{r})}}
\end{align}</span>
where <span class="math-container">$\rho_\sigma$</span> is the electronic density, <span class="math-container">$t_\sigma$</span> is the kinetic energy density and <span class="math-container">$\mathbf{v}_{x,\sigma}^{BR}(\textbf{r})$</span> is the original Becke-Roussel potential.</p>

<p>The authors propose the TB-mBJ (aka TB09) potential and implement it in Wien2K,a code based on the Augmented Planewave + local orbitals [APW+lo] method. Wien2K is an "all electron code". Over the years, most papers I have read that <em>use</em> the TB</p>

<p>How trustworthy is TB-mBJ (a.k.a. TB09) and how is it seen by the community <em>today</em>? </p>

<p>Does it yield accurate results with pseudo-potential codes like Quantum ESPRESSO or VASP? Are there any references that study the performance of TB-mBJ accross codes and basis sets?</p>

Best Answer: <p>The Tran-Blaha functional is not a generally useful functional, so I would not describe it as &quot;trustworthy&quot; at all. For example, it is not size-extensive so twice as much material does not have twice the energy, and hence you can't use it for any thermodynamics or phase stability or finite-displacement phonons etc. There isn't a proper potential either, so you can't use it self-consistently, and it's very fiddly to get pseudopotentials right since it couples the core and valence states via one of its parameters.</p>
<p>The usual claim is that the Tran-Blaha functional is &quot;good for band-gaps&quot;. I think a single scalar quantity is a poor indicator of functional quality, and band-gaps are especially poor since it is not at all clear what the Kohn-Sham gap with the &quot;true&quot; functional would be. My experience is that this functional is usually very poor for band-structures compared to, say, PBE, although it does tend to open up the Kohn-Sham gap more.</p>
<p>I think the SCAN functional has far more potential (pun intended) to be generally useful, once its various pathologies have been addressed. It may be used self-consistently, obeys many exact conditions and it is possible to construct pseudopotentials for it -- which, incidentally, is crucial to getting reasonable results with it in a pseudopotential program.</p>


================================================================================

Question: Are there any High-Throughput studies that aim to discover High Tc superconductors?
Body: <p>High-Throughput materials modeling based on Density Functional Theory has become very popular recently. If, for example, we search <a href="https://scholar.google.com/scholar?q=High-Throughput%20perovskites&amp;hl=en&amp;as_sdt=0,5" rel="noreferrer">"High-Throughput Perovskites"</a> in Google Scholar, we get over ~14,000 results. Not all are computational and not all are peer-reviewed publications but it shows just how important High Performance Computing is in current materials research. Needless to say, <a href="https://materialsproject.org/" rel="noreferrer">The Materials Project</a> and <a href="http://aflowlib.org/" rel="noreferrer">AFLOW</a> are big players in the High-Throughput Materials Simulation game. There is even a <a href="https://www.topologicalquantumchemistry.org/#/" rel="noreferrer">database</a> for Topological Insulators and Weyl Semimetals</p>

<p>However I am not aware of a database or High-Throughput study that is completely focused on the discovery of high <span class="math-container">$T_C$</span> superconductors. Can anyone point me toward some references? Are there any studies of this kind at all?</p>

Best Answer: <p>There are several such studies, particularly focusing on the machine-learning of critical temperatures.</p>

<ul>
<li><a href="https://doi.org/10.1038/s41524-018-0085-8" rel="noreferrer">"Machine learning modeling of superconducting critical temperature"</a></li>
<li><a href="https://doi.org/10.7567/1882-0786/ab2922" rel="noreferrer">"An acceleration search method of higher T c superconductors by a machine learning algorithm"</a></li>
<li><a href="https://doi.org/10.1039/C8ME00012C" rel="noreferrer">"Can machine learning identify the next high-temperature superconductor? Examining extrapolation performance for materials discovery"</a></li>
</ul>

<p>They all rely on the <a href="https://supercon.nims.go.jp/index_en.html" rel="noreferrer">"Supercon"</a> database of ~12,000 critical temperatures, which has also been extracted as supporting information for the first paper to <a href="https://github.com/vstanev1/Supercon" rel="noreferrer">GitHub</a>.</p>

<p>In general, the ML methods offer suggestions for new compositions, but I'm not sure if any have yet been attempted.</p>


================================================================================

Question: How to incorporate the effect of spin-orbit coupling in electronic structure calculation
Body: <p>Since the effect of spin-orbit coupling plays an important role in many transition metal complexes, what are the common methods to incorporate the effect of spin-orbit coupling?</p>

Best Answer: <p>Basically there are two kinds of approaches which may be found in many text books, L-S coupling and j-j coupling.</p>

<p>L-S coupling means that scalar electronic states (e.g. atomic L-S states and linear molecular Lambda-S states) are calculated first, and then the SO matrix is constructed with the help of 1-e (and optional 2-e) SO integrals. After diagonalizaion, energies of spinor states (atomic J levels and linear Omega states) may be obtained. Most of Q.C. programs like Molpro, Molcas, Orca, and Gamess do SOC in this way.</p>

<p>In j-j coupling, orbitals and spins are combined into spinors (atomic j and linear omega) at the very beginning, so there are no orbitals and scalar electronic states any more. The most representative program is Dirac. In addition, some Q.C. programs can do two-component HF/DFT only, including NWchem (sodft), Turbomole, Gaussian (int=dkh4), and so on. ADF can do j-j coupling DFT also (spinorbit zora), whereas L-S coupling DFT has to be performed through TDDFT.</p>

<p>From the perspective of methodology, in addition to L-S coupling and j-j coupling, there are also some intermediate approaches, which do scalar SCF/MCSCF calculatons first but SOC calculations at the post-HF/MCSCF stage. The programs I can think of are Columbus (soci), Cfour (ccsdso), and (maybe) Nooijen's STEOM-CC which is integrated in Orca.</p>


================================================================================

Question: What are some servers that are able to host and share our research data?
Body: <p>Now a days, due to some <em>publish-or-perish</em> politics that gives place to many cases of plagiarism and fake data/results, some journals are asking to submit (and share) the data used in the research. In this way, not only the referees can take a careful look to the data but also any reader can use it and reproduce your results.</p>

<p>Following this idea, the <a href="https://zenodo.org/" rel="noreferrer">Zenodo</a> server (built and operated by CERN) permit the storage of your research data.</p>

<p>I wonder what other options exist and what advantages they have over Zenodo?</p>

Best Answer: <p>First, there are general-use repositories that are not specific to materials modeling.</p>

<ol>
<li><p><a href="https://zenodo.org/" rel="noreferrer">Zenodo</a>. Like several other approaches, each project gets a permanent DOI associated with it, and new versions can be uploaded with the same unique parent DOI for the project and version-specific DOI. One major upside of Zenodo is that it is managed by CERN, which is likely to be around for a while. They also <a href="https://help.zenodo.org/" rel="noreferrer">guarantee</a> that should operations be ceased, they will transfer all data to a separate repository. Since the DOI is static but what it links to can be changed, this does not impact the findability of the data. One downside is that there is currently a 50 GB per-record file size limit, which may not be suitable for large-scale materials modeling.</p></li>
<li><p><a href="https://figshare.com/" rel="noreferrer">Figshare</a>. It operates in a very similar way with regards to have project- and version-specific DOIs, although they make no public claims about how they will ensure the longevity of the data. One major benefit of Figshare compared to Zenodo is that there are no project-specific file size limits (there is just a 5 TB limit on a per-file basis). Otherwise, the two platforms are very similar aside from user-interface.</p></li>
<li><p><a href="https://datadryad.org/stash" rel="noreferrer">Dryad</a>. Also operates similarly as the above but specifically has partnered with various publishers, such as Wiley, RSC, and PLOS so there are some ease-of-use features if you're submitting a manuscript to one of those journals and wish to also deposit the data.</p></li>
<li><p>Of course, there are other field-agnostic platforms to host data, such as on <a href="https://github.com/" rel="noreferrer">GitHub</a> or <a href="https://about.gitlab.com/" rel="noreferrer">GitLab</a> or <a href="https://bitbucket.org/product" rel="noreferrer">Bit-bucket</a>, but in my opinion these are not suitable for ensuring the longevity of data. The repositories can be deleted at any time, and there is no associated DOI.</p></li>
</ol>

<p>Then there are topic-specific platforms. For materials modeling in particular, some big names are:</p>

<ol>
<li><p><a href="https://www.iochem-bd.org/" rel="noreferrer">ioChem-BD</a>. This platform is meant for hosting computational chemistry results. Each data repository gets a DOI. You can read more about it in <a href="https://pubs.acs.org/doi/abs/10.1021/ci500593j" rel="noreferrer">this paper</a>, although it is slightly outdated now. There is also this <a href="https://www.nature.com/articles/s41929-018-0176-4" rel="noreferrer">perspective</a> on hosting data for catalysis research, written by the folks of ioChem-BD.</p></li>
<li><p><a href="https://qcarchive.molssi.org/" rel="noreferrer">QCArchive</a>. This is a repository fulfilling a similar niche as ioChem-BD, hosting data for computational chemistry simulations and making sure the data is findable while also providing interactive data visualizations. This is mainly meant for molecules. You can read more about the QCArchive in <a href="https://chemrxiv.org/articles/The_MolSSI_QCArchive_Project_An_Open-Source_Platform_to_Compute_Organize_and_Share_Quantum_Chemistry_Data/11908356/1" rel="noreferrer">this paper</a>.</p></li>
<li><p><a href="https://nomad-repository.eu/" rel="noreferrer">NOMAD repository</a>. This repository is also specifically suited for materials modeling (less so for computational chemistry). Once again, all projects get a DOI, and there is an API that can be used to interface with the data hosted on NOMAD. One subtle benefit of repositories like this is that they take care of issues you might not have thought of when hosting files for a given materials modeling code. For instance, it is against copyright to share VASP's pseudopotential files, but the NOMAD repository will automatically remove those (while storing the key information about what pseudopotentials you used) upon upload. You can read more about NOMAD in <a href="https://www.cambridge.org/core/journals/mrs-bulletin/article/nomad-the-fair-concept-for-big-datadriven-materials-science/1EEF321F62D41997CA16AD367B74C4B0" rel="noreferrer">this paper</a>.</p></li>
<li><p><a href="https://www.materialscloud.org/" rel="noreferrer">Materials Cloud</a>. This is a platform that is specific to materials modeling, but unlike ioChem-BD and NOMAD, it is not specifically focused on quantum-chemical calculations. You can read more about the Materials Cloud in <a href="https://arxiv.org/abs/2003.12510" rel="noreferrer">this paper</a>. One of the main distinguishing factors of the Materials Cloud is that it also focuses on making sure the workflows themselves are reproducible.</p></li>
<li><p><a href="https://www.catalysis-hub.org/" rel="noreferrer">Catalysis-Hub</a>. This is a data repository specific to quantum-chemical calculations related to catalysis and surface science. You can read more about Catalysis-Hub in <a href="https://www.nature.com/articles/s41597-019-0081-y" rel="noreferrer">this paper</a>.</p></li>
</ol>

<p>The main benefits of using a platform specific to computational chemistry or materials modeling is that they often make it easy to find other data or carry out meta-analyses, which are not practical on broader repositories like Zenodo or Figshare. In other words, they make your data more re-useable. They also are constructed to deal with common input/output file formats, oftentimes parsing the results automatically and displaying them in an intuitive way. This is also in contrast with more general data-hosting platforms, which effectively are just ways to host files as-is with some degree of longevity.</p>

<p>The main disadvantage in using a materials modeling-specific platform is that it may require more work to set up. You could, in principle, just dump all your files on a repository like Zenodo or Figshare. That doesn't mean they'll be easily findable and it's not necessarily advisable, but you could do it and it'd require little time on your part. In contrast, materials modeling-specific repositories are trying to make sure the data is consistently structured and readily accessible, so it will require a little bit more organizing time for the person uploading the data. Personally, I think that is a price worth paying.</p>


================================================================================

Question: What are some common techniques for building multiscale models for materials?
Body: <p>Most, if not all, material systems are multiscale in nature that are described by different physics at different length and time scales, ranging from density functional theory (DFT) to phase field modeling to finite element analysis.</p>

<p>How are these different levels of theory/model coupled across length and time scales?</p>

Best Answer: <p>One example of multi-scale modeling in materials modeling is <a href="https://en.wikipedia.org/wiki/Coarse-grained_modeling" rel="noreferrer">coarse-grain modeling</a> </p>

<p>In these examples larger systems can be studied in a more efficient way by treating clusters of atoms, often monomers or residues of a polymer as a single structure or "bead".</p>

<p>Each bead is parameterized to capture the feature of the underlying atoms, for example polarity and steric bulk. Thus the  material properties of the smaller atomic level affect the larger level. </p>


================================================================================

Question: What are some approaches to modeling charged point defects in materials?
Body: <p>From what I understand, one major issue in modeling charged point defects in materials is the existence of artificial interactions between charged defects and their periodic images. Even when using large supercells, as is typical when modeling uncharged point defects, the problem persists.</p>

<p>In the past, I briefly used the <a href="https://doi.org/10.1016/j.cpc.2018.01.004" rel="noreferrer">PyCDT</a> <a href="https://bitbucket.org/mbkumar/pycdt/src/master/" rel="noreferrer">Python package</a>, which uses <a href="https://doi.org/10.1103/PhysRevLett.102.016402" rel="noreferrer">sxdefectalign</a>, and VASP to calculate the formation energies of charged point defects. Besides using plane-wave DFT and the correction scheme in sxdefectalign, are there other approaches to modeling charged defects that might trade speed for accuracy or vice-versa? What other challenges are associated with modeling charged defects?</p>

Best Answer: <p>This is an important question that isn't answered yet, and I've been doing some work with charged defects recently so I'll attempt an answer--though I readily admit I am not really an expert (<em>i.e.</em> I don't work on developing these correction approaches). I'm not familiar with many approaches apart from DFT, so I'll limit my answer to that area.</p>

<p>In DFT at least, a charged cell is compensated by a uniform jellium background of equal and opposite charge, to keep the energy finite. The problem is that the long-range potential of the charged defect in this medium decays very slowly, as you mentioned. It depends on the defect and the dielectric properties of the material under study, but can be significant (on the order of eV). So something needs to be done to correct for this spurious interaction. There are also several potential-alignment terms that come into play when calculating energy differences between different systems. A good place to start reading is references <a href="https://link.aps.org/doi/10.1103/RevModPhys.86.253" rel="noreferrer">1</a> and <a href="https://link.aps.org/doi/10.1103/PhysRevB.86.045112" rel="noreferrer">2</a> below. I'll keep this answer less theoretical since the references do a much better job of explaining. I'll summarize the concepts quickly and mention some practical issues.</p>

<p>One approach you may have heard of is the Makov-Payne correction, which was derived based on a Madelung-type sum in a cubic cell. I haven't used this correction but I understand it can be quite inaccurate in realistic systems, typically overcorrecting <a href="https://www.sciencedirect.com/science/article/pii/S0921452611007812" rel="noreferrer">3</a>. In the situations I've encountered it, it's also been limited to systems with cubic symmetry, though I'm not sure if this is always the case.</p>

<p>Freysoldt's scheme is a common approach. The basic idea is that you use a simple model to describe the defect's charge, such that you can calculate it's isolated energy interacting with the jellium background, as well as the energy of a periodic system that is interacting with it's images, using Poisson's equation. If you align the potential of your model with the DFT calculations, you can use the difference between the isolated and periodic energies of your model as a correction term. There is also related correction by Kumegai and Oba <a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.89.195205" rel="noreferrer">4</a>. </p>

<p>The third correction I've seen is from Lany and Zunger, which goes beyond the Makov-Payne correction by using DFT-calculated difference in charge between a charged and neutral defect to calculate a higher-order term in the correction. I have the least amount of experience with this method.</p>

<p>When it comes down to implementing these schemes I have the most experience with the Freysoldt method. I have used <a href="https://sxrepo.mpie.de/" rel="noreferrer">sxdefectalign</a> and <a href="http://www.physics.iisc.ernet.in/~mjain/pages/software.html" rel="noreferrer">CoFFEE</a>. One thing I learned early on was that all the nice plots in the papers and code examples are typically <em>unrelaxed</em> calculations where the atomic positions are the same in all the calculations. This makes differences in potentials smooth and easy to analyze. When you want an accurate formation energy you need to allow the system to relax; the differences in the atomic positions of the different calculations (pristine vs. defect) cause significant variation in the potential. You typically need to do some smoothing or averaging, which can be challenging to get right.</p>

<p>I've also encountered challenges when using these codes with cells that have non-orthogonal lattice vectors (e.g. monoclinic). Planar averaging and model calculations can be especially difficult. It can be hard to determine if you're doing something wrong or if there is a bug in the code somewhere. You need to do a lot of experimenting.</p>

<p>There are other codes that automate the process more. You mentioned <a href="https://bitbucket.org/mbkumar/pycdt/src/master/" rel="noreferrer">PyCDT</a>; <a href="https://github.com/PyDEF2/PyDEF-2.0" rel="noreferrer">PyDEF</a> and <a href="https://github.com/pylada/pylada-defects" rel="noreferrer">Pylada</a> are also interesting, but these three only really support VASP at the moment. I know PyCDT has a wrapper for sxdefectalign, but I thought they had a separate module for performing the calculation without sxdefectalign. <a href="https://pymatgen.org/" rel="noreferrer">Pymatgen</a> can perform various corrections taken from the PyCDT code, but I haven't had much time to spend with it yet. </p>

<p>I'm always interested to hear if there are other approaches and codes out there.</p>

<p><strong>References</strong></p>

<ol>
<li><a href="https://link.aps.org/doi/10.1103/RevModPhys.86.253" rel="noreferrer">C. Freysoldt <em>et al.</em>, <em>Rev. Mod. Phys.</em> <strong>86</strong>, 253
(2014)</a>.</li>
<li><a href="https://link.aps.org/doi/10.1103/PhysRevB.86.045112" rel="noreferrer">H.-P. Komsa, T. T. Rantala, and A. Pasquarello, <em>Phys. Rev. B</em> <strong>86</strong>,
045112 (2012)</a>.</li>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0921452611007812" rel="noreferrer">H.-P. Komsa, T. Rantala, and A. Pasquarello, <em>Physica B: Condensed Matter</em> <strong>407</strong>, 3063 (2012)</a>.</li>
<li><a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.89.195205" rel="noreferrer">Y. Kumagai and F. Oba, <em>Phys. Rev. B</em> <strong>89</strong>, 195205 (2014)</a>.</li>
</ol>


================================================================================

Question: How to model heterogeneous catalysis?
Body: <p><a href="https://en.wikipedia.org/wiki/Heterogeneous_catalysis" rel="noreferrer">Heterogeneous catalysis</a> is known to be very important:</p>

<blockquote>
  <p>Approximately 35% of the world's GDP is influenced by catalysis. The production of 90% of chemicals (by volume) is assisted by solid catalysts.</p>
</blockquote>

<ul>
<li><p>What considerations go into a model of these types of reactions? For example, how would one generally go about modeling surface catalysis (e.g. software/algorithms)? </p></li>
<li><p>Besides software what considerations should be taken into account that are different from homogeneous catalysis?</p></li>
</ul>

Best Answer: <p>I did some work during my rotation in the Brooks group  on surfaces that could act as abiogenesis sources (self-sustaining chemical reactions that become living organisms). </p>

<p>Key considerations for Heterogeneous catalysis are:</p>

<ul>
<li><p>The timescales of your reaction, in our case it was a 24hr high pressure/temperature reaction so we required a driving force to have an initiation event. This isn't uncommon, but generally you want a surface that reacts faster than that.</p></li>
<li><p>Crystal cuts, also called Miller Indices, generally have to be investigated thoroughly. Essentially the angle of a cut through a unit cell is entirely up to chance, so a 001 crystal surface may react differently than a 010 or 110 etc. As a result you have to do some research of the common fracture angles for your surface and generate them as unique catalyst surfaces.</p></li>
<li><p>Solvation almost always has to be explicit to get a realistic reaction behavior. But oftentimes you'll add some type of force or implicit solvent on top to contain the top of your surface. This is especially important with the periodic boundary conditions that are typically used, so that your molecules don't interact with the bottom of the crystal which is normally buried.</p></li>
<li><p>Additionally how deep you layer your surface can matter. Many crystals at the atomic level can "breathe" up to 3 or 4 layers deep (some iron minerals can react even farther) so you'll want to have at least enough layers for the full reaction space, and some below. However, you can freeze portions of your surface to avoid additional calculation expenses.</p></li>
</ul>

<p>For software we used VASP and TBDFT with periodic boundary conditions with explicit waters. So those are the most familiar to me and the ones I would recommend</p>

<p>For input files I found Avogadro has the most user friendly for generating input files for VASP and TBDFT, however VESTA is another freely available package for opening CIF files and generating computation inputs.</p>


================================================================================

Question: How can I create a .CIF file from x-ray diffraction data in a paper, for compound not found in crystallographic database
Body: <p>I tried to recover a .CIF file (Crystallographic Information File) with the crystal structure for the compound discussed in a paper by Reuter <em>et. al.</em><sup>[1]</sup>, strontium hydroxide octahydrate - <span class="math-container">$\ce{Sr(OH)2·8H2O}$</span> - from Cambridge Crystallographic Data Centre (CCDC), but couldn't find it. The paper itself has tables with information recovered from x-ray diffraction data, as shown below.</p>

<p><a href="https://i.sstatic.net/n6K6a.png" rel="nofollow noreferrer"><img src="https://i.sstatic.net/n6K6a.png" alt="enter image description here"></a></p>

<p><a href="https://i.sstatic.net/JjL5W.png" rel="nofollow noreferrer"><img src="https://i.sstatic.net/JjL5W.png" alt="enter image description here"></a></p>

<p>Is there any free software or open source tool where I can just type the data from these tables to assemble a crystal structure like the ones I'm used to download from the database? How can I do it?</p>

<p>I posted <a href="https://chemistry.stackexchange.com/q/132937/89908">this question</a> in chemistry.stackexchange at first, before materials.stackexchange entered beta. As it went unanswered, I'm giving it a try here.</p>

<h3>References:</h3>

<ol>
<li>Reuter, H.; Kamaha, S.; Zerzouf, O. Hydrogen Bonds in the Crystal Structure of Strontium Hydroxide Octahydrate Sr(OH)2 · 8H2O. <em>Zeitschrift für Naturforschung B</em> <strong>2007,</strong> <em>62</em> (2), 215–219. <a href="https://doi.org/10.1515/znb-2007-0212" rel="nofollow noreferrer">DOI: 10.1515/znb-2007-0212</a>.</li>
</ol>

Best Answer: <p>You can use the free software <a href="https://jp-minerals.org/vesta/en/download.html" rel="noreferrer">VESTA</a> for creating a new structure by following steps</p>

<ol>
<li>From the file menu choose New structure</li>
<li>Go to Unit cell panel and choose the space group of the structure</li>
</ol>

<p><a href="https://i.sstatic.net/bdVJS.png" rel="noreferrer"><img src="https://i.sstatic.net/bdVJS.png" alt="enter image description here"></a></p>

<ol start="3">
<li>Enter the lattice parameters in the same panel</li>
<li>Change to Structure parameters panel and enter x,y,z and U<sub>eq</sub> of each atom one by one</li>
</ol>

<p><a href="https://i.sstatic.net/SjkWN.png" rel="noreferrer"><img src="https://i.sstatic.net/SjkWN.png" alt="enter image description here"></a></p>

<ol start="5">
<li>Save the structure and after verifying that the wyckoff positions displayed by the software matches with that of the publication, you can export it to cif.</li>
</ol>


================================================================================

Question: How established is density functional theory as a tool in drug design?
Body: <p>Motivated by the ongoing COVID-19 pandemic, I am wondering how established density functional theory (DFT) is as a tool/technique in drug design.</p>

<p>Drug molecules come in a wide range of sizes from small molecules (e.g. aspirin) to proteins (biologics) that have complicated intramolecular and intermolecular interactions in a physiological environment, which is often highly concentrated. Is DFT a commonly used tool/technique in drug design? Or is DFT too expensive and inaccurate for us to perform first-principles prediction of drug properties?</p>

Best Answer: <p>There are two parameters that are important in such a question: the number of molecules and the size of the systems.</p>

<p>There are some approach used to test a molecule. One is using databases with thousand of molecules. One example of such database is <a href="http://zinc.docking.org/" rel="noreferrer">Zinc</a> that have 230M 3D structures that can be downloaded and freely used. The other approach is the combinatorial chemistry. One technique is the <em>de novo</em> design. Using this technique against one target, you can generate more than 2.5M molecules.</p>

<p>As many drugs act inhibiting a protein, a single system will have thousand of atoms.</p>

<p>With all the above in mind, for example, if you want to test molecules against one of the proteins related to COVID-19, you will be dealing with thousand of molecules, which means thousand/million of atoms.</p>

<p>Such huge system are intractable using DFT (even with approaches like <a href="https://en.wikipedia.org/wiki/Fragment_molecular_orbital" rel="noreferrer">Fragment Molecular Orbitals</a>).</p>

<p>So, the solution is to use simple methods like Molecular Mechanics and Molecular Dynamics that use forcefields making the calculations faster.</p>


================================================================================

Question: What software is needed in chemoinformatics or materials modeling, that a solo programmer could work on?
Body: <p>I want to make a pet project in cheminformatics and I need to hear from the community about the actual challenges in chemistry that might be resolved with a digital solution.</p>

<p>I've been working in this field a long time ago and want to get fresh opinions about the most current issues in chemistry.</p>

<p>I understand that the most obvious issues are already covered by some solutions, which is why I need your advice. Mostly interested in organic chemistry but will gladly review all the ideas.</p>

<p>It will be a solo project so it's nothing monumental but still, I would like to do something that will have real value for chemists.</p>

<p>I will appreciate any ideas.</p>

Best Answer: <p>Disclaimer: I'm the lead developer of <a href="https://openbabel.org/" rel="noreferrer">Open Babel</a> and <a href="https://avogadro.cc/" rel="noreferrer">Avogadro</a> - and currently mentoring Google Summer of Code projects.</p>

<p>The biggest question is more "what kind of topics" interest you and/or what kind of skills you want to learn. Data Science? Informatics? Visualization? Reaction Prediction? QSAR? etc. That would help you refine the possible projects to pursue. Maybe it's a particular programming language or skill? (I usually have my students learn Python through some mini projects relevant to their research.) Maybe it's ways to automate computational or experimental tasks.</p>

<p>Generally open source projects have open lists of bugs, feature requests, etc., often some marked "help wanted" or "good first issue." </p>

<p>More involved projects are posted in various places. For example, the "Open Chemistry" mentoring organization for Google Summer of code has this year's <a href="https://wiki.openchemistry.org/GSoC_Ideas_2020" rel="noreferrer">project ideas</a> for a wide range of topics.</p>

<p>So <em>welcome</em> - there's lots to do. My suggestion as someone who's helped get many people up to speed with contributing to open source chemistry:</p>

<ul>
<li>Think about topics or skills that matter to you. There are a huge number of projects and you're most likely to make useful contributions if you're enthusiastic about it.</li>
<li>Find a project or two that seems like a good fit. Check out their code, webpages, project pages, etc. Are there good starting points? If not, submit an issue or contact the e-mail list, discussion forum, etc. Chances are good that they'll welcome new contributors.</li>
</ul>

<p>If you're not sure, you can always reach out to projects like RDKit, Open Babel, MDTraj, and other larger projects. There's also an excellent review article on <a href="https://opensourcemolecularmodeling.github.io" rel="noreferrer">"Open Source Molecular Modeling"</a> - originally <a href="https://doi.org/10.1016/j.jmgm.2016.07.008" rel="noreferrer"><em>Journal of Molecular Graphics and Modelling</em> (2016)</a> that lists tons of projects across multiple topics.</p>


================================================================================

Question: Retrieving Translational and Rotational Modes
Body: <p>I'm doing some post-processing using the vibrational modes of a molecule to solve a linear system of equations, but I believe I need all the modes in order to have a single, well-determined solution. I'm doing this with Gaussian and their <a href="http://gaussian.com/vib/" rel="nofollow noreferrer">website</a> lays out how they form these modes, but I have yet to find an option to actually print them and since they are doing it already internally and I'm reading the other modes from output files, I'd rather not repeat the process if I don't have to.</p>
<p>Is there a way to get Gaussian (or any electronic structure program) to print the coordinates of the translational/rotational modes to the log or some output file? Alternatively, can I easily generate translational/rotational modes given the vibrational modes?</p>

Best Answer: <p>I've never done this myself, and there may be other approaches, but one possible detailed answer seems to be provided by the Gaussian webpage. For stability reasons, you can find this page via <a href="https://web.archive.org/web/20191229092611/https://gaussian.com/vib/" rel="noreferrer">the Internet Archive</a> (<a href="https://web.archive.org/web/20171013014851/http://gaussian.com/wp-content/uploads/dl/vib.pdf" rel="noreferrer">pdf</a>).</p>

<p>In particular, you may want to jump to the sections
"<em>Determine the principal axes of inertia</em>"
and
"<em>Generate coordinates in the rotating and translating frame</em>".</p>

<p>For convenience, let me copy the procedure here. In short, you want to:</p>

<ol>
<li><p>translate the center of mass to the origin (trivial)</p></li>
<li><p>calculate the moments of inertia (the diagonal elements) and the products of inertia (off diagonal elements) of the moment of inertia tensor </p></li>
<li><p>obtain the translational vectors by normalizing the corresponding coordinate axis with the factor <span class="math-container">$\sqrt{m_i}$</span></p></li>
<li><p>obtain the (infinitesimal) rotational vectors by a slightly more convoluted formula:</p></li>
</ol>

<p><span class="math-container">\begin{align}
D_{4,j,i} &amp;= ((P_y)_i X_{j,3} - (P_z)_i X_{j,2})/\sqrt{m_i}\\
D_{5,j,i} &amp;= ((P_z)_i X_{j,1} - (P_x)_i X_{j,3})/\sqrt{m_i}\\
D_{6,j,i} &amp;= ((P_x)_i X_{j,2} - (P_y)_i X_{j,1})/\sqrt{m_i}
\end{align}</span> </p>

<p>where <span class="math-container">$j = x, y, z$</span>; <span class="math-container">$i$</span> is over all atoms and <span class="math-container">$P$</span> is the dot product of <span class="math-container">$R$</span> (the coordinates of the atoms with respect to the center of mass) and the corresponding row of <span class="math-container">$X$</span>, the matrix used to diagonalize the moment of inertia tensor <span class="math-container">$I$</span>.</p>

<p>The next step is to normalize these vectors: the vector is normalized using the reciprocal square root of the scalar product.</p>


================================================================================

Question: Can DFT be considered an ab initio method?
Body: <p><strong>Note added afterwards: This question also has excellent answers on the Chemistry Stack Exchange: <a href="https://chemistry.stackexchange.com/q/33764/27342">Is density functional theory an ab initio method?</a></strong></p>
<hr>
<p>I have very little experience with DFT, but coming from more of a coupled-cluster background, where the improvements are &quot;systematic&quot;, to me it seems that the choice of functional in DFT is somewhat trial-and-error based and also problem specific, perhaps requiring &quot;chemical intuition&quot;. Therefore, can DFT truly be considered an ab initio method?</p>

Best Answer: <p>As pointed out by several people already, some information can be found elsewhere, as in <a href="https://chemistry.stackexchange.com/questions/33764/is-density-functional-theory-an-ab-initio-method/33766#33766">here</a>. And also the differentiation between DFT (<strong>exact</strong>) and density functional approximations (DFAs), as pointed out regularly by Mel Levy, can be found there.<br><br>
However, I think there is one aspect missing, and here I would like to quote my late PhD supervisor Jaap Snijders. The most important aspect to know if a method is <em>ab initio</em> or not, is related to the integrals. If the integrals can be computed <em>from the beginning</em>, the method is <em>ab initio</em>; if not, then not. In DFT, DFAs and wavefunction methods, the integrals can be computed, and hence, these methods are <em>ab initio</em>. In semi-empirical methods (AM1, PM3, DFTB, xtb), some of the integrals are either estimated or approximated (from e.g. DFA results in case of DFTB/xtb), and therefore, these methods are <strong>not <em>ab initio</em></strong>. Likewise for e.g. the Empirical Valence Bond method, which like the name already indicates, is empirical.<br><br>
Whether or not a method gives the <strong>exact energy</strong> is a different aspect. In that case only Full CI with infinite basis set and DFT give the exact energy, all other methods are approximations. By choosing a basis set of a certain size, one is approximating; by using &quot;only&quot; CCSD(T), one is approximating; by using a density functional like PBE, B3LYP or r2SCAN, one is approximating; etc.</p>


================================================================================

Question: Physical/Intuitive picture for DFT that all motions and pair correlations in a many-electron system are contained in the total electron density alone
Body: <p>Quoting Becke [1]</p>

<blockquote>
  <p><em>Density-functional theory (DFT) is a subtle, seductive, provocative business. Its basic premise, that all the intricate motions and pair correlations in a many-electron system are somehow contained in the total electron density alone, is so compelling it can drive one mad</em></p>
</blockquote>

<p>How do you convince yourself that all the intricate motions and pair correlations in a many-electron system are somehow contained in the total electron density alone?</p>

<p>PS: I am not looking for proofs for Hohenberg-Kohn theorems. More like a physical/intuitive picture</p>

<p><strong>References</strong></p>

<ol>
<li>Becke, Axel D. "Perspective: Fifty years of density-functional theory in chemical physics." The Journal of chemical physics 140.18 (2014): 18A301.</li>
</ol>

Best Answer: <p>It's completely legitimate to want a physical intuition for something that is proved mathematically. </p>

<p>One <strong>very</strong> hand-wavy argument is just to say that the electron density in the ground state has to reflect the effects of all of those electron-electron correlations, interactions, exchange. After all, the electron density definitely depends on the interactions between electrons, but that doesn't mean we need to know all the details of how the electrons are actually interacting. </p>

<p>There are often macroscopic/thermodynamic quantities that include the effects of all sorts of more complicated microscopic interactions. For example, the Young's modulus of a solid allows us to accurately predict the stress/strain relationship without having to know anything about the microscopic rules that cause the Young's modulus to take it's specific value. the Another example would be an effective electron mass in solid state physics. </p>


================================================================================

Question: Special quasirandom structures vs virtual crystal approach
Body: <p>What are the advantages and disadvantages in using special quasirandom structures (SQS) vs virtual-crystal approximation (VCA) to simulate electronic properties of disordered alloys?</p>

<p>You do not need to named <strong><em>all</em></strong> advantages and <strong><em>all</em></strong> disadvantages. Instead you can answer even if you know <strong><em>one</em></strong> advantage or <strong><em>one</em></strong> disadvantage of either package.</p>

Best Answer: <p>For VCA, as the construction is as simple as <a href="https://doi.org/10.1103/PhysRevB.61.7877" rel="nofollow noreferrer">averaging</a> the potentials of atomic species, the main advantage is computational. This is in the sense that it is very simple to change the compositional ratio
of a solid solution by just changing how you average the potentials, instead of having to work with
supercells (as you would have to do with SQS or other approaches even for simple ratios).</p>
<p>However, simply by doing this, you can imagine that the new potential will not be physical and neither the
calculations associated to it. To give some examples, properties which depend on the local
environment won't be correctly reproduced (e.g., the band gap in <a href="https://doi.org/10.1103/PhysRevB.57.3753" rel="nofollow noreferrer">GaAlAsN</a> and <a href="https://doi.org/10.1063/1.119232" rel="nofollow noreferrer">GaPN and GaAsN</a>).</p>
<p>Then, how is it that VCA has been shown to be successful for same cases? Well, it happens that for <a href="https://doi.org/10.1103/PhysRevB.61.7877" rel="nofollow noreferrer">some materials</a>, the contributions of some atomic species are very similar to each other. Thus, averaging their potentials doesn't change the general properties (so much). Having said this, the real limitation of VCA is that it depends on how similar the parent atomic species are to each other.</p>


================================================================================

Question: How is Poier and Jensen&#39;s Bond Capacity Model Parameterized and Optimized?
Body: <p>I am working on adding a charge polarization model into my own research and have been exploring a few approaches. One of the most attractive options is the Bond Capacity (<em>BC herein</em>) model of Paolo and Jensen (<a href="https://pubs.acs.org/doi/abs/10.1021/acs.jctc.8b01215" rel="noreferrer">DOI: 10.1021/acs.jctc.8b01215</a>). Alas, there is no example implementation available and though it has been added to Tinker-HP (<a href="https://pubs.acs.org/doi/10.1021/acs.jctc.9b00721" rel="noreferrer">DOI: 10.1021/acs.jctc.9b00721</a>) the version that might contain it has yet to see a release. </p>

<p>I have been prototyping my own implementation, but I have ran into some roadblocks when trying to reproduce results in the paper. The primary issue is a lack of information regarding the model's parameterization and handling of integrals. I understand the questions are narrow, but I'm taking a shot in the dark and looking for any valuable insights. </p>

<p>I will briefly highlight the purpose and key equations to save you from reading the paper. I should note, the BC model was designed to describe molecular polarization and polarizabilities using fluctuating charges. </p>

<p>Now for the equations. I am going to presume we are working with a neutral system of <span class="math-container">$N$</span> atoms acting under the influence of an external electric field <span class="math-container">$F$</span>. In the BC model, the electrostatic potential for atom <span class="math-container">$i$</span>, <span class="math-container">$V_i$</span>, is the sum of the intrinsic potential of atomic electronegativity (<span class="math-container">$\chi_i$</span>), the cumulative external potential of neighboring atoms (<span class="math-container">$\phi_i$</span>), and the potential due to the external field (<span class="math-container">$\psi_i$</span>). The amount of charge that is transferred between a pair of atoms <span class="math-container">$p_{ij}$</span> is taken to be proportional to a pair capacity <span class="math-container">$\xi_{ij}$</span> and the difference in electrostatic potential for the atoms: <span class="math-container">$$p_{ij} = \xi_{ij}(V_i - V_j) \\ p_{ij} = \xi_{ij}\left[(\chi_i+\phi_i+\psi_i)-(\chi_j+\phi_j+\psi_j)\right]$$</span></p>

<p>This pair capacity parameter is geometry dependent and is defined as: <span class="math-container">$$\xi_{ij}(R_{ij}) = \xi_{ij}^{0} g(R_{ij}) = \xi_{ij}^{0} \left(\frac{1-S_{ij}}{R_{ij}}\right)$$</span> Where <span class="math-container">$\xi_{ij}^{0}$</span> is pair capacity at equilibrium geometry and <span class="math-container">$g(R_{ij})$</span> is an attenuation function that is one at equilibrium and decays to zero as inter atomic distance grows. <strong>This</strong> is where my first two questions come in. In the paper, they choose the overlap between Slater orbitals (<span class="math-container">$S_{ij}$</span>) as part of the attenuation function.</p>

<ol>
<li><p><strong>By chance, does anyone have any idea if they are using STOs with angular momenta greater than <span class="math-container">$l=0$</span> to evaluate the overlap of atoms?</strong> </p></li>
<li><p>In similar models, only one s Slater type orbital is used per atom. I presume the same in my implementation and evaluate the overlap using equations (5) and (6) from (<a href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/qua.560210612" rel="noreferrer">DOI:10.1002/qua.560210612</a>). Though, this form is not analytically derived. <strong>Does anyone have a reference that gives an exact form for the two-centered overlap of S-type slater orbitals?</strong>    </p></li>
</ol>

<p>Okay moving forward, the total charge on any atom <span class="math-container">$Q_i$</span> due to the aforementioned electrostatic potential components can be determined via, <span class="math-container">$$Q_i = \sum_j^N -\xi_{ij}(V_i - V_j)$$</span> thus the charge on every atom in the system can summarized in the matrix form, <span class="math-container">$$\textbf{Q}=-\textbf{CV}$$</span> </p>

<p>Where <span class="math-container">$\textbf{C}$</span> is symmetric (<span class="math-container">$\xi_{ij}=\xi_{ji}$</span>) and contains negative pair capacities of atoms on the off-diagonal and the diagonal is a sum of these pair capacities in the corresponding column or row, <span class="math-container">$\xi_{ii} = \sum_{k\neq j}^N \xi_{ik}$</span>. <span class="math-container">$\textbf{V}$</span> is a vector of the atomic electrostatic potentials defined above.  </p>

<p>The components of <span class="math-container">$\textbf{V}$</span> should be defined briefly. To determine the polarization potential of atom <span class="math-container">$i$</span> due to its neighbors, <span class="math-container">$\phi_i$</span>, you must evaluate a Coulomb interaction, (<span class="math-container">$J_{ik}$</span>), with a similar form to <span class="math-container">$\xi_{ij}(R_{ij})$</span>: <span class="math-container">$$\phi_{i}=\sum_{k=1}^{N} J_{ik} Q_{k}=\sum_{k \neq i}^{N} \frac{1-S_{i k}}{R_{ik}} Q_{k}+J_{ii} Q_{i} \\ \boldsymbol{\phi = JQ}$$</span>
Where again the overlap between Slater orbitals (<span class="math-container">$S_{ik}$</span>) attenuation function is applied. Meanwhile, the intrinsic potential of atomic electronegativity, <span class="math-container">$\boldsymbol{\chi}$</span>, is taken to be a vector of free parameters. The potential due to the external electric field has it's usual definition 
<span class="math-container">$$\psi_i = \sum_{\sigma=x,y,z}R_{i \sigma}F_{\sigma} \\ \boldsymbol{\psi = R^TF}$$</span> 
where <span class="math-container">$\textbf{R}$</span> is a (N,3) matrix of atomic positions and <span class="math-container">$\textbf{F}$</span> is the electric field vector. </p>

<p>Combining the above equations and performing some rearrangements allows one to define the 3x3 molecular polarizability tensor as, <span class="math-container">$$\boldsymbol{\alpha = R^T\left( I + CJ \right)^{-1}CR}$$</span> where all <span class="math-container">$\textbf{I}$</span> is the identity matrix and all other terms have been defined above. This brings me to my final question:</p>

<ol start="3">
<li>The authors are using an, "in house," code to perform the fitting of their parameters to reproduce ab initio polarizabilities. I have no way of knowing what routines they are using. <strong>I am trying to determine what STO exponential constants <span class="math-container">$\zeta$</span> and electronegativities <span class="math-container">$\chi_i$</span> were used to attain their results. I am currently using scipy's <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html" rel="noreferrer"><code>least_squares</code></a> to optimize these parameters via minimizing the residuals between the components of my polarizability matrix and theirs (iteratively of course). I am not a statistician nor a fitting expert. Since this is just a system of equations, does anyone know of a better way of fitting the parameters to ab initio data in python or C++?</strong></li>
</ol>

<p>I apologize for the long winded post, but these details have been vexing for a while now and I'm getting tired of perusing Google for answers. If you made it this far, thanks for reading.</p>

Best Answer: <p>The specific questions:</p>

<ol>
<li><p>Only <span class="math-container">$l = 0$</span> overlaps are (currently) considered, in the spirit of atoms being represented by (only) a spherical charge distribution (point charges).</p></li>
<li><p>The overlap/attenuation function for the bond capacities is very (!) preliminary. It should be designed to reflect the actual physics, and this is work in progress. However, any reasonable function that models how the bonding decays as a function of distance should at least give a fair model. The same holds for the Coulomb screening function. In the paper, it was taken to be the same as the screening function for the bond capacity, but one could (and should) considered these to have different functional forms. We opted for the simple solution of using the overlap calculated using the s-functions of the STO-3G basis set. This involves only overlap of s-type Gaussian functions which is trivial to calculate.</p></li>
<li><p>We used a Newton-Raphson optimizer with numerical gradients, a piece of code that had been developed for another purpose, and could easily be modified to do the job. We assigned the bond capacities from calculated polarizabilities, and then fitted the electronegativity parameters to give the desired atomic charges. One also need to assign the atomic Coulomb self-interaction (aka hardness) parameters to give the desired polarization response.</p></li>
</ol>

<p>As inferred from the above, the parameterization of the BC model is currently very rudimentary, and there is certainly room for refinements, but it does have some fundamental features that look attractive.</p>


================================================================================

Question: What significant matter modelling methods are implemented in commercial software, for which there is no freeware alternative?
Body: <p><a href="https://en.wikipedia.org/wiki/List_of_quantum_chemistry_and_solid-state_physics_software" rel="noreferrer">There is an ever-growing list of freeware and open-source software for solid-state physics and quantum chemistry</a>. </p>

<p>But many commercial programs still thrive, even in 2020, and their cost can be in the thousands of pounds/dollars. </p>

<p>I am curious to know what mainstream calculations still cannot be done for free? </p>

<p>For each answer, please list one method that is implemented in commercial software but not any freeware, and a reference to a paper that used this commercial method, where the authors of the paper are not involved in any way with the commercial software.</p>

Best Answer: <p>I think the way this question is asked is a little too simplistic. In order to execute a computational project, there is always more than one calculation required. Even if you are happy with the lowest level of theory (say, B3LYP/6-31G*), it does not mean that any package that lists B3LYP in the list of the available features would be useful. </p>

<p>Possible caveats:</p>

<ul>
<li><p>The method is implemented, but the convergence is not robust. So your jobs fail because of poor underlying algorithms of SCF solvers;</p></li>
<li><p>The method is implemented, but the code is ineffective. You need to spend more compute resources, wait longer, and increase your carbon footprint (not a joke! Calculations use lots of electricity);</p></li>
<li><p>The package claims the method exists, but the implementation is buggy and gives wrong numbers; </p></li>
<li><p>You can compute energy, but there are no analytic gradients or frequencies;</p></li>
<li><p>You can compute energies and optimize structures, but there are no other features -- like various properties calculations that are needed for answering a research question. To give a few examples: Solvent effects, QM/MM capabilities, spin-orbit couplings, multipole moments, wave-function analysis, NMR, IR/Raman intensities. The list can go on forever. </p></li>
</ul>

<p>It takes many years to build and maintain a software package that is mature enough to be considered an adequate research tool. You may lose a lot of time and money by going after "free stuff". </p>


================================================================================

Question: What is the status of graphics processing units in plane-wave DFT?
Body: <p>A few years back, I performed a few calculations of metallic slabs using VASP and the Atomic Simulation Environment (ASE). I'm by no means an expert, but I noticed that the calculations were very CPU intensive e.g. ~24 cpu-hours (in parallel) to get a single energy point and gradient calculation. </p>

<p>Considering the size of the system this makes sense, however, I'm wondering, have graphics processing units (GPUs) changed this scenario much? Is the use of GPUs becoming more popular in plane-wave DFT? </p>

Best Answer: <p>I recently installed VASP GPU version provided by NVIDIA (<a href="https://www.nsc.liu.se/~pla/blog/2015/11/16/vaspgpu/" rel="noreferrer">here's the installation tutorial</a>) on my machine that has an RTX 2080 Ti GPU. GPU version was announced for <code>vasp-5.4.1</code> and works fairly well for <code>vasp-5.4.4</code> too. However, I have only managed to observe around 1.5x - 2x speedup when running VASP on the GPU compared to my <code>Intel(R) Core(TM) i7-8700 CPU</code> with 12 cores. But there is definitely some degree of speedup. Maybe the GPU version of VASP is meant for more powerful (and expensive) double-precision GPUs such as <code>Titan V</code> or <code>Tesla V100</code>. While my GPU performs exceptionally well on running deep learning applications, I feel it can't keep up with its more powerful counterparts for VASP calculations. </p>

<p>Furthermore, I use <a href="https://software.intel.com/content/www/us/en/develop/documentation/mpi-developer-reference-linux/top/command-reference/mpirun.html" rel="noreferrer">intel's mpi library</a> to run VASP. Surprisingly, <code>mpirun -n 1 vasp_gpu</code> is much faster than running on 2(or more) cores (<code>mpirun -n 2 vasp_gpu</code>) even though more cores consumed more GPU memory. Using more cores is even slower than running solely on CPU with <code>vasp_std</code>. I am currently investigating what's the reason for this and greatly appreciate if any other user with experience contribute to this answer. </p>


================================================================================

Question: Should I buy a CPU or a GPU for doing calculations?
Body: <p>A few years ago there was a significant difference between CPUs and GPUs for performing calculations. It was quite clear when to go for a CPU or a GPU. </p>

<p>Today there are GPUs with a very large number of cores and threads. I was away from the subject for a while.  Is there still such a marked difference?  Or can a CPU reasonably well replace a GPU in, say, molecular dynamics? </p>

<p>Where should one put the money?</p>

Best Answer: <p>As other answers have mentioned, it depends a lot on the workload and the availability of GPU-acceleration for the codes you use (or write). In principal, there are multiple platforms, but in practical use right now, Nvidia CUDA has the best performance and largest use.</p>

<p>Nvidia offers a <a href="https://www.nvidia.com/en-us/gpu-accelerated-applications/" rel="noreferrer">GPU Application directory</a> - listing existing codes with CUDA availability.</p>

<ul>
<li><p>Many molecular dynamics codes are already GPU-enabled and show huge speedups over CPU versions (e.g., <a href="https://developer.nvidia.com/hpc-application-performance" rel="noreferrer">https://developer.nvidia.com/hpc-application-performance</a>). This includes LAMMPS, AMBER, GROMACS, NAMD.. pretty much everything. Speedup varies, but I've often seen 20x-100x show up.</p></li>
<li><p>Fewer quantum codes are GPU-enabled with mixed results, largely dependent on how much work has gone into writing GPU versions of the code. These include VASP, Quantum Espresso, CP2k, Abinit, BigDFT, Gaussian, GAMESS, etc. I've seen some good benchmarks for commercial GPU acceleration of <a href="https://www.brianqc.com" rel="noreferrer">Q-Chem called BrianQC</a> and <a href="http://www.petachem.com/performance.html" rel="noreferrer">Terachem</a> but these are not widely deployed. On codes like Quantum Espresso and VASP, even Nvidia is <a href="https://developer.nvidia.com/hpc-application-performance" rel="noreferrer">quoting a 10-20x speedup</a>.</p></li>
</ul>

<p>For machine learning, speedup can be much, much more dramatic if your workload can be stored on the GPU. Data Center GPUs can be 16GB to 32GB in the Tesla V100 cards, and now 40GB in the A100 cards, although there's some overhead (i.e. you don't get to use all of that for your data and model).</p>


================================================================================

Question: State of the art in computational materials design
Body: <p>With the advent of more computational power than ever in the recent years, interest in <em>in silico</em> design of interesting compounds has grown as well. I am wondering about the state of the art for the case of designing novel materials:</p>

<ul>
<li>What methods are commonly used (DFT, semi-empirical, more / less expensive)?</li>
<li>What size of lattices is one able to treat in high-througput screening approaches?</li>
<li>What is the effort in terms of runtime one needs to put in?</li>
</ul>

Best Answer: <p>After a little research I found a great article [1], which provides a good overview to what I asked above in Figure 2. Summarising in a table:</p>
<div class="s-table-container">
<table class="s-table">
<thead>
<tr>
<th>Method</th>
<th>effort</th>
<th>reliability</th>
<th>system size</th>
</tr>
</thead>
<tbody>
<tr>
<td>Interatomic potentials</td>
<td>high</td>
<td>high/low*</td>
<td><span class="math-container">$10^8$</span></td>
</tr>
<tr>
<td>Linear-scaling DFT</td>
<td>high</td>
<td>medium-high</td>
<td><span class="math-container">$10^6$</span></td>
</tr>
<tr>
<td>Tight binding</td>
<td>high</td>
<td>medium-high</td>
<td><span class="math-container">$10^6$</span></td>
</tr>
<tr>
<td>LDA DFT</td>
<td>low</td>
<td>medium-low</td>
<td><span class="math-container">$10^3$</span></td>
</tr>
<tr>
<td>GGA DFT</td>
<td>low</td>
<td>medium</td>
<td><span class="math-container">$10^3$</span></td>
</tr>
<tr>
<td>GGA+U DFT</td>
<td>low</td>
<td>medium-high</td>
<td><span class="math-container">$10^3$</span></td>
</tr>
<tr>
<td>Hybrid DFT</td>
<td>low</td>
<td>high</td>
<td><span class="math-container">$10^2$</span></td>
</tr>
<tr>
<td>GW</td>
<td>high</td>
<td>high</td>
<td><span class="math-container">$10^1$</span></td>
</tr>
</tbody>
</table>
</div>
<p>* high for geometries and energetics, but low for excited states
or the dielectric function</p>
<p>where <em>effort</em> refers to the manual effort from the researcher, <em>reliability</em> gives the reliability of the method and <em>system size</em> is the typical system size in the number of atoms. In the table I am providing a condensed (and maybe biased) overview, focusing on properties such as geometry, energetics, dielectric function, excited states or bandgap. See [1] for the full details.</p>
<h3>References</h3>
<p>[1] K. T. Butler, J. M. Frost, J. M. Skelton, K. L. Svane and
A. Walsh. <em>Chem. Soc. Rev.</em>, 2016, <strong>45</strong>, 6138. DOI <a href="https://doi.org/10.1039/c5cs00841g" rel="nofollow noreferrer">10.1039/c5cs00841g</a></p>


================================================================================

Question: Cancellation of Errors in density functional approximations or any wavefunction based methods
Body: <p>I am curious to find references for when is it TRUE that the error cancellation in approximate methods like DFT/MP2/etc. are valid and when they fall apart.</p>

<p>For example: My understanding is (please argue against if you don't agree) that if an approximate method only captures interactions say X, Y, and Z (x/y/z could be lets say pairwise dispersion, hydrogen bonding, aromatic interaction), then the error cancellation would be valid across all the molecular systems that have X, Y, and Z interactions. However, let's say in reality (i.e. true physical interacting picture) there is X, Y, Z, and A (where A might be say many body dispersion or exact exchange), then is it true that the error cancellation of a method correctly capturing X, Y, and Z will not be valid for systems with X, Y, Z, A. </p>

<p>Mathematically, for say the energies of molecules 1 and 2, is the following true:</p>

<p>(X1 + Y1 + Z1) - (X2 + Y2 + Z2) is the same as (X1 + Y1 + Z1 + A1) - (X2 + Y2 + Z2 + A2)</p>

<p>where Xi,Yi/etc. may just be energy contributions of pairwise dispersion/hydrogen bonding/etc. </p>

<p>Papers supporting or arguing against error cancellation are all appreciated! </p>

Best Answer: <p>I don't know if I can answer the exact question here, but I can hopefully start a discussion in the right direction.</p>
<p>Error cancellation is pervasive throughout matter modeling, and in particular electronic structure calculations. Whenever we use an approximate method to predict, for example, the energy of some system, we are neglecting numerous possible interactions/factors that would be included in the exact formulation (e.g. the Schrodinger equation for nonrelativistic calculations). If the computed result is in good agreement with the exact result, what has happened is that the various error introduced by our approximation have effectively cancelled each other out. In this sense, the accuracy of approximate computational methods is entirely dependent on error cancellation. So asking when error cancellation holds for a given level of theory is essentially asking for when that method works at all.</p>
<p>So we can easily determine extent of error cancellation provided we have some sort of exact reference. Not as simple is determining what are the main sources of a particular instance of error cancellation. Often this just takes the form of heuristics developed over time by applying different methods to different types of systems. In a few cases, efforts have been made to actually justify theoretically/mathematically why a method performs well/poorly on certain systems or for certain properties.</p>
<p>A simple example is that Hartree-Fock can be used to predict surprisingly accurate ionization potentials (using Koopman's theorem) despite the simplicity of the method. The reason for this is that the calculation neglects both correlation energy and relaxation energy. Including correlation energy should stabilize the original molecule more than its ion, as the original molecule has more electrons to correlate. This would suggest that HF underestimates the ionization potential. However, allowing the orbitals of the ion to relax should stabilize it relative to the original molecule, which suggests that HF is overestimating the ionization potential. Since these two effects have opposite sign, neglecting them only introduces a small amount of error. (This argument is based on the one given in section 3.3 of Szabo and Ostlund's Modern Quantum Chemistry).</p>


================================================================================

Question: How to calculate Special Quasirandom Structure with local ordering?
Body: <p>I have a supercell that has 12 layers, each with four atoms. I want to generate an SQS with local ordering at layer numbers 6, 7 and 8. Rest of the layers should be disordered.<br>
I followed the procedure <a href="http://cniu.me/2017/08/05/SQS.html" rel="noreferrer">"How to use SQS for disordered materials."
</a> but I get the following error:  </p>

<pre><code>Impossible to match point correlations due to incompatible supercell size.  
</code></pre>

<p>The input code (I am considering only the pairs):</p>

<pre><code>mcsqs -2=4
mcsqs -rc
</code></pre>

<p>The <code>rndstr.in</code> file(A,B,C,D can be any metallic element):</p>

<pre><code>6.92820300      0.00000000      0.00000000
2.30940100      6.53197300      0.00000000
0.00000000      0.00000000  33.94112500
1   0   0
0   1   0
0   0   1
1   0.5 1   A=.25,B=.25,C=.25,D=.25
1   1   1   A=.25,B=.25,C=.25,D=.25
0.5 1   1   A=.25,B=.25,C=.25,D=.25
0.5 0.5 1   A=.25,B=.25,C=.25,D=.25
0.25    0.25    0.916667    A=.25,B=.25,C=.25,D=.25
0.75    0.25    0.916667    A=.25,B=.25,C=.25,D=.25
0.75    0.75    0.916667    A=.25,B=.25,C=.25,D=.25
0.25    0.75    0.916667    A=.25,B=.25,C=.25,D=.25
1   1   0.833333    A=.25,B=.25,C=.25,D=.25
1   0.5 0.833333    A=.25,B=.25,C=.25,D=.25
0.5 1   0.833333    A=.25,B=.25,C=.25,D=.25
0.5 0.5 0.833333    A=.25,B=.25,C=.25,D=.25
0.75    0.25    0.75    A=.25,B=.25,C=.25,D=.25
0.25    0.75    0.75    A=.25,B=.25,C=.25,D=.25
0.25    0.25    0.75    A=.25,B=.25,C=.25,D=.25
0.75    0.75    0.75    A=.25,B=.25,C=.25,D=.25
0.5 0.5 0.666667    A=.25,B=.25,C=.25,D=.25
0.5 1   0.666667    A=.25,B=.25,C=.25,D=.25
1   1   0.666667    A=.25,B=.25,C=.25,D=.25
1   0.5 0.666667    A=.25,B=.25,C=.25,D=.25
0.25    0.25    0.583333    B
0.75    0.25    0.583333    C
0.75    0.75    0.583333    C
0.25    0.75    0.583333    D
1   1   0.5 B
1   0.5 0.5 C
0.5 1   0.5 D
0.5 0.5 0.5 A
0.25    0.75    0.416667    B
0.25    0.25    0.416667    D
0.75    0.75    0.416667    A
0.75    0.25    0.416667    A
0.5 0.5 0.333333    A=.25,B=.25,C=.25,D=.25
0.5 1   0.333333    A=.25,B=.25,C=.25,D=.25
1   1   0.333333    A=.25,B=.25,C=.25,D=.25
1   0.5 0.333333    A=.25,B=.25,C=.25,D=.25
0.25    0.25    0.25    A=.25,B=.25,C=.25,D=.25
0.75    0.75    0.25    A=.25,B=.25,C=.25,D=.25
0.75    0.25    0.25    A=.25,B=.25,C=.25,D=.25
0.25    0.75    0.25    A=.25,B=.25,C=.25,D=.25
0.5 0.5 0.166667    A=.25,B=.25,C=.25,D=.25
0.5 1   0.166667    A=.25,B=.25,C=.25,D=.25
1   0.5 0.166667    A=.25,B=.25,C=.25,D=.25
1   1   0.166667    A=.25,B=.25,C=.25,D=.25
0.25    0.75    0.083333    A=.25,B=.25,C=.25,D=.25
0.75    0.75    0.083333    A=.25,B=.25,C=.25,D=.25
0.75    0.25    0.083333    A=.25,B=.25,C=.25,D=.25
0.25    0.25    0.083333    A=.25,B=.25,C=.25,D=.25
</code></pre>

<p>Please suggest if I am doing something wrong, or is there anything else further which I need to specify to the ATAT MCSQS code?</p>

Best Answer: <p>Often we are hung up on the idea of making sure we generate an SQS of a size (i.e. # of atoms) that matches the composition of our system. For instance, in the case of <span class="math-container">$A_3B$</span> we would generally take multiples of 4 as the # of atoms. This isn't always enough, especially in your case. Since you want to make a cell that is ordered in certain layers and disordered in others, you are limiting yourself in terms of # of atoms required to generate the SQS.</p>
<p>This '<strong>integer</strong> # of atoms and corresponding composition' constraint also applies to sub-lattices in your cell. Sometimes the overall cell might comply with this constraint but certain symmetry equivalent sites may not. I don't know how exactly does one determine sublattices in a partially ordered cell but <code>corrdump</code> seems to be the way out - <a href="https://www.brown.edu/Departments/Engineering/Labs/avdw/forum/viewtopic.php?f=15&amp;t=410&amp;p=1484&amp;hilit=Impossible%20to%20match%20point%20correlations%20due%20to%20incompatible%20supercell%20size&amp;sid=9dcb407ef9eb2dd9e1e019181ce566ff" rel="noreferrer">read here</a>.</p>
<p>In your case, 192 atoms turn out to be the minimum # of atoms that comply with this constraint.</p>


================================================================================

Question: What is the cutting edge for open-source Force-Field generation?
Body: <p>The adoption of atom-typing allowed for the creation of incredibly widely used force-fields (FF) such as GROMOS/AMBER/OPLS and many more. Because of the assumptions required in Molecular Mechanical (MM) models, in order to be accurate, at least part of the force-field must be fit to experimental data. This is very tedious, and there are many years, decades even, between updates to a FF.</p>
<p>Recent FF development has utilized chemical perception rather than atom-typing and seems to have a lot of potential. For instance, the <a href="https://openforcefield.org" rel="nofollow noreferrer">openFF consortium</a> seems to be making large strides in creating an open source, easy to use and tinker with, FF.</p>
<p>Also, parallel and GPU computing have come a long way in the last decade. I would hope/assume that GPUs should make parameter optimization considerably faster than in previous decades.</p>
<p>Are there FF groups/projects in development that have the aim of being open source and highly &quot;tinkerable&quot;, and are we in sight of being able to generate custom FFs somewhat on the fly, as we need them rather than relying on databases of atom-types fit nearly exclusively to solutes in water?</p>

Best Answer: <h2>ForceBalance</h2>
<p>ForceBalance is one approach which aims at making it easier to create force fields from a combination of theoretical and experimental data. I believe it's within the OpenMM framework.</p>
<p>The paper on the work can be found here
<a href="https://pubs.acs.org/doi/10.1021/jz500737m" rel="noreferrer">https://pubs.acs.org/doi/10.1021/jz500737m</a></p>
<p>From the GitHub page it says</p>
<p>&quot;The purpose of ForceBalance is to create force fields by applying a highly general and systematic process with explicitly specified input data and optimization methods, paving the way to higher accuracy and improved reproducibility.&quot;</p>
<p>The code can be downloaded here
<a href="https://github.com/leeping/forcebalance" rel="noreferrer">https://github.com/leeping/forcebalance</a></p>


================================================================================

Question: Difference between potential energy, free energy and Coulomb energy in solid state physics
Body: <p>I often encounter terms such as (Helmholtz, Gibbs) <strong>free energy</strong>,  <strong>potential energy</strong> and <strong>total energy</strong> when describing the energy of a physical system at atomic level. Sometimes I stumble upon <a href="https://www.sciencedirect.com/science/article/abs/pii/002236977490033X" rel="noreferrer"><strong>Coulomb energy</strong></a>, which adds more to the confusion. My (vague) understanding is that potential energy of a cell should be equal to Coulomb energy of that cell.</p>
<p>Could someone explain the differences/similarities between these terms, including what contributes to the &quot;total energy&quot; found in a DFT (VASP) calculation?</p>

Best Answer: <p>I can't answer in the context of DFT/VASP, where these terms might have specific uses, but I can offer some general comments.</p>
<p>These terms are not necessarily mutually exclusive:</p>
<ul>
<li><strong>Total energy</strong> probably refers to the combined kinetic and interaction energy <span class="math-container">$E=K+P...$</span> resulting from summing/integrating over the whole Hamiltonian. In DFT this would be the combination of all functionals.</li>
<li><strong>[Helmholtz/Gibbs] Free Energy</strong> refers to the thermodynamic energy available to do work (at constant <em>T</em>). Basically, the total internal energy of the system subtracting out the energy that is 'trapped in entropy.'</li>
<li><strong>Potential energy</strong> (in this context) most likely refers to the particle-particle interactions. This could include the energy from the pseudopotentials and the electron-electron (Coulomb) interactions.</li>
<li><strong>Coulomb energy:</strong> in the most general sense, you are correct: the only source of potential energy is the Coulomb force. However, it's possible that this term is referring to a <em>specific</em> contribution such as <em>just</em> the potential energy from the psuedopotentials. (Someone better-versed in DFT can provide a more intelligent answer here). In the specific context of the reference you cited, the Coulomb energy appears to be calculated from the Coulomb interactions of the nuclei (in the lattice) with the electronic charge uniformly smeared out between them.</li>
</ul>


================================================================================

Question: Quadrature over three Euler Angles for orientation averaging
Body: <p>Does anybody know about an accurate quadrature rule over <em>three</em> Euler angles <span class="math-container">$\theta, \phi, \chi$</span>?
I am trying to calculate the average value of an arbitrary function <span class="math-container">$f(\theta, \phi, \chi)$</span> for a given probability distribution <span class="math-container">$\rho(\theta, \phi, \chi)$</span>:</p>
<p><span class="math-container">$$
\langle f\rangle_{\rho} = \int_0^{2\pi} d\chi \int_0^{2\pi} d\phi\int_0^{\pi} \sin\theta d\theta f(\theta, \phi, \chi)\rho(\theta, \phi, \chi) \approx \sum_{l} w_l g(\theta_l, \phi_l, \chi_l)
$$</span>
where <span class="math-container">$f(\theta, \phi, \chi)$</span> is <em>not</em>  sum-of-products of single angle-coordinate functions. <span class="math-container">$w_l$</span> are quadrature weights and <span class="math-container">$(\theta_l, \phi_l, \chi_l)$</span> represents a single grid point. <span class="math-container">$g(\theta, \phi, \chi) =  \sin\theta f(\theta, \phi, \chi)\rho(\theta, \phi, \chi)$</span> and an appropriate quadrature weight-function is implicitly included in <span class="math-container">$f$</span>.</p>
<p>Such an integration is sometimes needed in orientation averaged calculations of material/molecule's properties.</p>
<p>What we can assume about <span class="math-container">$\rho(\theta, \phi, \chi)$</span> is that it is a linear combination of the products of the Wigner D-matrices: <span class="math-container">$\rho(\theta, \phi, \chi) = \sum_{K,K'} c_{KK'} D^{(J)}_{KM}(\theta, \phi, \chi)D^{(J')}_{K'M'}(\theta, \phi, \chi)$</span>.</p>
<p>So to sum up. It seems there is a need for a simple quadrature scheme for integration over three Euler angles: <span class="math-container">$\theta, \phi, \chi$</span>.</p>
<ul>
<li><strong>Update:</strong></li>
</ul>
<p>Ways around the need for explicit three-Euler angle <span class="math-container">$\theta, \phi, \chi$</span> quadrature:</p>
<ol>
<li><p>Expanding <span class="math-container">$f(\theta, \phi, \chi)$</span> in the Wigner matrices basis is one way of proceeding here. The appropriate integrals can be calculated analytically. But for some functions <span class="math-container">$f(\theta, \phi, \chi)$</span> this method is very inefficient, as the Wigner expansion has many terms.</p>
</li>
<li><p>Another possibility, if we are dealing with a quantum system, is to solve the Schroedinger equation for wavefunctions in three separate system-fixed embeddings of the coordinate frame. Then one can choose the system-fixed z-axis to be the axis in the system that we want to quantify (average over). In such a case only <span class="math-container">$\theta, \phi$</span> are needed.
<em>The down-side</em> to this approach is that one needs to repeat calculations for three independent embeddings, which for some embeddings and some external potentials can be very unnatural. In the case of rotational dynamics problems the complete symmetric-top basis sets guarantee accuracy of the solutions regardless of the embedding chosen. Rotational wavefunction representations among different embeddings differ only by the coefficients. In good embeddings the rotational wavefunction can be reprsented compactly in the basis, but if the embedding is poor, often many basis functions are needed. In the case of problems which add coupling of rotational degrees of freedom to some other internal or external degrees of freedom, the choice of embedding is often critical for quick convergence of the variational procedure.</p>
</li>
</ol>

Best Answer: <p>I don't know of any exact methods, but <a href="https://github.com/nschloe/quadpy" rel="nofollow noreferrer"><code>quadpy</code></a> is a really good place to start, and possibly ask the related question there.</p>
<p>Not really an answer, but more of a <em>hint</em>.</p>


================================================================================

Question: What are the types of MCSCF?
Body: <p>Similar to:</p>
<ul>
<li><a href="https://mattermodeling.stackexchange.com/q/1568/5">What are the types of SCF?</a></li>
<li><a href="https://mattermodeling.stackexchange.com/q/1594/5">What are the types of Quantum Monte Carlo?</a></li>
<li><a href="https://mattermodeling.stackexchange.com/q/1523/5">What are the types of ab initio Molecular Dynamics?</a></li>
<li><a href="https://mattermodeling.stackexchange.com/q/1521/5">What are the types of pseudopotentials?</a></li>
<li><a href="https://mattermodeling.stackexchange.com/q/1513/5">What are the types of DFT?</a></li>
<li><a href="https://mattermodeling.stackexchange.com/q/1439/5">What are the types of charge analysis?</a></li>
<li><a href="https://mattermodeling.stackexchange.com/q/901/5">What are the types of bond orders?</a></li>
</ul>
<p>Let's be the first resource that explains the following in <strong>up to 3 paragraphs</strong>:</p>
<ul>
<li>UCASSCF</li>
<li>RASSCF [<a href="https://mattermodeling.stackexchange.com/a/1569/5">link to answer</a>]</li>
<li>GASSCF</li>
<li>LASSCF [<a href="https://mattermodeling.stackexchange.com/a/1614/5">link to answer</a>]</li>
<li>FCIQMC-SCF</li>
<li>MRCI</li>
<li>MR-ACPF</li>
<li>MR-AQCC</li>
<li>DMRG-SCF</li>
<li>SHCI-SCF</li>
<li>SA-CASSCF</li>
<li>DOLO-SCF [<a href="https://mattermodeling.stackexchange.com/a/1833/5">link to answer</a>]</li>
<li>DW-CASSCF</li>
</ul>
<p>For FCIQMC-SCF, DMRG-SCF, and SHCI-SCF, instead of explaining what FCQIMC, DMRG and SHCI are, please instead say which programs implement these methods, and anything special that had to be done in order for these to work (for example SHCI-SCF can be done with variational SHCI or with SHCI-PT2, and the one with PT2 is not variational, so the authors had to do a little extra to make it work).</p>

Best Answer: <h2>LASSCF: Localized Active Space SCF</h2>
<p>An approximation to or generalization of (depending on how you look at it) CASSCF. In CASSCF, the wave function consists of an antisymmetrized product of two factors defined in two non-overlapping sets of orbitals: a single determinant of occupied inactive orbitals, and a general correlated wave function describing the electrons occupying the active orbitals. In LASSCF, the wave function is a product of one single determinant and <strong>several</strong> general correlated wave function terms, defined in the Fock spaces of multiple, distinct, non-overlapping sets of active orbitals; usually, each active subspace is <em>localized</em> (hence the name) around a particular atom or cluster of atoms within a large molecule, and the difference active subspaces are presumed to interact with one another only weakly. The initialism &quot;vLASSCF&quot; corresponds to the <em>variational</em> extension in which all orbitals are optimized without any (direct or indirect) constraint and the Hellmann-Feynman theorem applies, and corresponds to a realization for <em>ab initio</em> chemical models of the cluster mean-field (cMF) method developed by Jiménez-Hoyos and Scuseria.</p>
<p>The purpose of LASSCF is similar to that of RASSCF and GASSCF, in that one purports to reduce the cost of an untenably expensive CASSCF calculation by splitting a large active space, described by a factorially large CI vector, into weakly-interacting subspace parts. However, the approximation (compared to CAS with the same active orbitals) inherent to a LAS wave function is more severe than in either RAS or GAS, and the corresponding savings in computational cost are likewise greater in principle. RAS and GAS limit the configurations the CI vector of the active space is allowed to explore in terms of excitations from a reference determinant, while LAS requires that the notional CI vector describing all active subspaces collectively must factorize into a product of subspace parts. This means that the total cost of solving the CI problems in LASSCF is linear with respect to the size of the molecule, provided the size of a single active subspace is held fixed.</p>
<p>The method is currently only implemented in <a href="https://github.com/MatthewRHermes/mrh" rel="nofollow noreferrer">mrh</a>, an extension to <a href="http://pyscf.org/" rel="nofollow noreferrer">PySCF</a>.</p>


================================================================================

Question: How is group theory used to deduce which of these integrals are equal to 0?
Body: <p>The number of all two-electron integrals:
<span class="math-container">$$
\tag{1}
\langle \phi_1 \phi_2|\phi_3\phi_4 \rangle = \int d^3\mathbf r' \int d^3\mathbf r'' \, \phi_1(\mathbf r'') \, \phi_2(\mathbf r') \frac{1}{|\mathbf r' - \mathbf r''|} \, \phi_3(\mathbf r') \, \phi_4(\mathbf r''),
$$</span></p>
<p>for <span class="math-container">$N$</span> number of basis functions (I am using real-valued ones) is <span class="math-container">$N^4$</span>.</p>
<p>Not all of them are unique, but the number of unique integrals is <span class="math-container">$N(N+1)(N^2 + N + 2)/8$</span>.</p>
<p>Yet this number is still huge for a reasonably accurate basis set. I have a feeling that this number can be significantly reduced if one makes use of the <strong>point-group symmetry</strong> of the molecule to determine the vanishing integrals. If yes, <strong>how is this done given the knowledge of the point group?</strong></p>

Best Answer: <p>The question is too broad to be answered directly so I will provide a somewhat general scheme.</p>
<p>Basically in an integral like
<span class="math-container">$$
\int  d\mu A B C
$$</span>
one would seek to expand each part in irreducible representations of a given group, say for instance
<span class="math-container">\begin{align}
B=\frac{1}{\vert \mathbf{r}-\mathbf{r}^\prime\vert}
=\frac{1}{r} \sum_{\ell} \left(\frac{r'}{r}\right)^\ell \sqrt{\frac{4\pi}{2\ell+1}}Y^{\ell}_0(\theta,\varphi)
\end{align}</span>
where here the group would be <span class="math-container">$SO(3)$</span> and the irreducible representations are labelled by <span class="math-container">$\ell$</span>.  Doing the same for <span class="math-container">$C$</span> and <span class="math-container">$A$</span>, <em>v.g.</em>
<span class="math-container">\begin{align}
C&amp;=\sum_{\ell m} c_{\ell m}Y^\ell_{m}(\theta,\varphi)\, ,\\
A&amp;=\sum_{\ell m} a_{\ell m}Y^\ell_{m}(\theta,\varphi)\, .
\end{align}</span>
The integral then becomes
<span class="math-container">\begin{align}
\frac{1}{r}\sum_{\ell_1m_1;\ell_2m_2;\ell}
a_{\ell_1m_1}c_{\ell_2m_2}\left(\frac{r'}{r}\right)^\ell 
\sqrt{\frac{4\pi}{2\ell+1}}
\int Y^{\ell_1}_{m_1}(\theta,\varphi)Y^\ell_0(\theta,\varphi)
Y^{\ell_2}_{m_2}(\theta,\varphi) \tag{1}
\end{align}</span>
and the last term is automatically <span class="math-container">$0$</span> unless we have
<span class="math-container">\begin{align}
\ell_1\otimes\ell\otimes\ell_2&amp;=\mathbf{0}+\ldots...\,  , \tag{2a}\\
m_1+m_2&amp;=0 \tag{2b}
\end{align}</span>
where (2a) comes from angular momentum coupling to the representation <span class="math-container">$\mathbf{0}$</span> (i.e. total <span class="math-container">$L=0$</span>) and (2b) is the condition on <span class="math-container">$SO(2)\sim U(1)$</span> that the resulting magnetic quantum number is <span class="math-container">$0$</span>.</p>
<p>There is nothing <em>a priori</em> to restrict the sum over <span class="math-container">$\ell_1,\ell_2,\ell$</span> in (1) unless you have some prior knowledge of <span class="math-container">$A$</span>, <span class="math-container">$B$</span> and <span class="math-container">$C$</span>.</p>
<p>The same general principle holds for point groups.  In the case of
point groups you would expand each <span class="math-container">$A$</span>, <span class="math-container">$B$</span>, <span class="math-container">$C$</span> in terms of representation of the specific point group, and use the great orthogonality theorem of representations (also called <a href="https://en.wikipedia.org/wiki/Schur_orthogonality_relations#Coordinates_statement" rel="noreferrer">Schur orthogonality relations</a>).  Probably the integral would be broken in group elements multiplied by cosets, i.e .the integration would be written as <span class="math-container">$g\cdot h$</span> where <span class="math-container">$g$</span> is in the group, and some sums over <span class="math-container">$g$</span> would be <span class="math-container">$0$</span> if the combination of representations contained in the decomposition of <span class="math-container">$A$</span>, <span class="math-container">$B$</span> and <span class="math-container">$C$</span> can be combined to the identity (or trivial) representation.  There would then remain integration over cosets.  This is <em>a bit</em> of what happens in the example above: writing a rotation as <span class="math-container">$R_z(\varphi) R_y(\theta)$</span> (there is no third angle here) the condition <span class="math-container">$m_1+m_2=0$</span> gets rid of the <span class="math-container">$R_z(\varphi)$</span> integration and the result is an integration over <span class="math-container">$R_y(\theta)$</span> only.</p>
<p><a href="https://en.wikipedia.org/wiki/Mildred_Dresselhaus" rel="noreferrer">Prof. Mildred Dresselhaus</a> of MIT still has <a href="http://web.mit.edu/course/6/6.734j/www/group-full02.pdf" rel="noreferrer">coursenotes available</a>, and co-wrote an excellent textbook on the general topic.</p>
<p>Edit:</p>
<p>So it seems your “real solid harmonics“ are basically the same as my spherical harmonics, up to some linear combinations.</p>
<ol start="0">
<li>I do not understand your comment re: Hilbert space.  The Hilbert space here is the space of all 2-particle states (as you have written your states as products of two states).</li>
</ol>
<p>So a more or less general procedure would be as follows.</p>
<ol>
<li><p>Find the linear combinations of your basis sets that transform by irreducible representations of your points group.   For instance, if you “only” need axial symmetry, then combinations of the type <span class="math-container">$Y^\ell_m\pm Y^\ell_{-m}$</span> will produce cosine and sine pieces that are symmetric or antisymmetric w/r to reversal of the <span class="math-container">$\hat z$</span> axis.  There are systematic ways of finding these, using projection operator techniques (someone already pointed that out).</p>
</li>
<li><p>This decomposition is usually not that bad if the group has few representations but then some irreps may occur more than once and it can be a computational headache unless one is careful.  In other words, the projection technique might provide you with multiple solutions which you have to specialize and properly normalize.   The projection gives you (usually) one state in the irrep and you may have to work a little more to construct the remaining states, although with point groups the matrix representations are well known so it’s not that bad.</p>
</li>
<li><p>Basically the step above means you are no longer working with functions <span class="math-container">$\boldsymbol{\phi_3}\boldsymbol{\phi_4}$</span> in your original basis set but some combinations of states. You also need to expand the Coulomb term in this way.</p>
</li>
<li><p>The last step is to use orthogonality of group functions to eliminate some terms.  The non-zero terms that survive are those for which the tensor product <span class="math-container">$\Gamma^*_k\otimes \Gamma_r\otimes \Gamma_i$</span> contains the identity representation.  Here, <span class="math-container">$\Gamma^*_k$</span> is one piece in the sum of the expansion of <span class="math-container">$\phi_1\phi_2$</span>, <span class="math-container">$\Gamma_r$</span> is one piece in the sum of the expansion of <span class="math-container">$1/\vert\mathbf{r}-\mathbf{r'}\vert$</span>, and <span class="math-container">$\Gamma_i$</span> is one piece in the expansion of <span class="math-container">$\phi_3\phi_4$</span>.  This type of triple product may occur more than once for <span class="math-container">$(k,r,i)$</span> if the irreps <span class="math-container">$\Gamma_k$</span> <em>etc</em> occur more than once in the decomposition of the old basis set in the basis set.</p>
</li>
</ol>
<p>You get to decide if finding these combinations ends up saving time over simply evaluating the original integrals.</p>


================================================================================

Question: Is there any reason not to sum the kinetic and potential energy from an NPT simulation to get internal energy?
Body: <p>I would be very grateful for some newbie-level advice from a thermodynamics guru.</p>
<p>I ran NPT simulations on a particular system (in CP2K software) to get fluid densities for use in fluid dynamics applications.  Before the lockdown, a colleague mentioned that it might be interesting to use the results for describing mixing in the system thermodynamically (the system is made up of two endmember compositions).  I finally thought about it, but <strong>can't figure out if my NPT internal energy data are meaningful</strong> and don't want to bother my colleague, who has been sick.  I would love advice.</p>
<p><strong>I would like to know if I can simply sum my NPT ensemble-derived kinetic and potential energies to calculate internal energy</strong>.  The reason I ask is that I came across a comment online where someone seemed to think the thermostat would interfere with the kinetic energy values.</p>
<p>I can't decide if this is true.  I know that energy is not conserved for NPT simulations--that energy is either added or subtracted to maintain a constant temperature.  However, it seems like that should be fine because I want the system to be at a particular temperature.  I could run another set of simulations in a different ensemble.  But that would mean a lot of extra time and computational resources, so I want to be really certain before diving in to doing more simulations.</p>
<p>With regard to energy not being conserved for NPT simulations, I'm wondering if it matters that my goal is to provide people with data on the thermodynamics of mixing in the system.  In other words, for the data to be useful, I need to be compare internal energies from distinct simulations run at the same pressure and temperature, but different endmember fractions (compositions)--one internal energy value by itself probably won't be useful.  I'm not sure if I'm being clear here...  The bottom line is that I can't figure out if it would be like comparing apples and oranges to compare internal energy values from different NPT simulations (and therefore cannot use my existing results to describe the thermodynamics of this system).</p>
<p>I have searched online for information on this, but probably don't know enough to be familiar with useful search terms.  If anyone has any hints on how to think this through, I'd be very appreciative.  Thank you.</p>

Best Answer: <p>Energy in an NPT simulation is not conserved, but (once equilibrated), it will fluctuate around an average value, and that average value has meaning. That is the ensemble average for your NPT and is a valuable and useful property.</p>
<p>You are also correct that the internal energy is the summation of potential and kinetic contributions. To be thorough, pp. 60 of Computer Simulations Of Liquids by Allen &amp; Tildesley (2nd edition) note</p>
<p><span class="math-container">\begin{equation}
    E = \langle H\rangle = \langle \mathcal{K} \rangle + \langle \mathcal{V} \rangle
\end{equation}</span>
where E is internal energy, H is Hamiltonian, <span class="math-container">$\mathcal{K}$</span> is kinetic energy and <span class="math-container">$\mathcal{V}$</span> is potential energy (calculated using a force-field in this case). I recommend <a href="https://global.oup.com/academic/product/computer-simulation-of-liquids-9780198803201?cc=ca&amp;lang=en&amp;" rel="noreferrer">Allen &amp; Tildesley</a> as well as <a href="https://www.amazon.ca/Understanding-Molecular-Simulation-Algorithms-Applications/dp/0122673514" rel="noreferrer">Frenkel &amp; Smit </a>if you want more information on gathering ensemble averaged properties from atomistic simulations. They are the two tomes of the field.</p>
<p>It is fair to compare the internal energies of two different composition simulations. It is however critical that both simulations are properly equilibrated and sampled so that <span class="math-container">$\langle \mathcal{K} \rangle$</span> and <span class="math-container">$\langle \mathcal{V} \rangle$</span> are reliable estimates.</p>
<p>I recommend equilibrating for several nanoseconds at the least, and sampling for several nanoseconds at the least.</p>


================================================================================

Question: Empirically determining thermostat damping factor
Body: <p><strong>Is there a way to empirically determine an appropriate thermostat damping factor given a timestep size and a numerical integration method?</strong> For example, I would like one for the surface and the molecule, and I will describe more below.</p>
<p>I am computing a set of simulations in LAMMPS, using ReaxFF, tracking the path of a single (~4 atom) gas molecule as it interacts with a (~600 atom) surface.</p>
<p>Two thermostats are imposed on the system, one for the surface, one for the molecule.</p>
<pre><code>fix surfaceFix  surface  temp/csvr <span class="math-container">${surfTemp} $</span>{surfTemp} <span class="math-container">${surfaceDampingFactor}
fix moleculeFix molecule temp/csvr $</span>{molTemp}  <span class="math-container">${molTemp}  $</span>{moleculeDampingFactor}
</code></pre>
<p>I know intuitively that the surface temperature should be more tightly regulated than the temperature of the molecule, since the molecule trajectory is a dependent variable in the experiment. Yet I don't have a way of determining the value of each damping factor.</p>
<p>For instance with the molecule, errors are introduced in the limit of both small and large damping factors:</p>
<ul>
<li>In the case of weak thermostatting, the force calculations can diverge and we introduce an error due to the precision limits of numerical integration.</li>
<li>In the case of strong thermostatting, we introduce an error in the energy transferred between the molecule and the surface.</li>
</ul>
<p>Both of these errors could cause the effect of &quot;hopping&quot;: where the molecule is provided with a non-physical velocity that causes it to jump into or out of locations that would otherwise be impossible.</p>
<p>There must be a connection between the timestep size + integration method, and the damping factor needed to correct for the deviation in forces that these produce. Naturally the damping required would also depend on the mechanics of the thermostat, and its effect on different degrees of freedom, but an answer addressing any thermostat is of interest.</p>

Best Answer: <p>To the best of my knowledge, there is no universal or even near-universal heuristic for what the time constant should be. The only <em>rule of thumb</em> I have encountered is that the time constant should be around 10x-1000x larger than the timestep of the simulation. Beyond that point, I would treat the damping constant as a hyperparameter, do a sensitivity analysis, and pick a value where changing the damping by e.g. 10x in either direction does not affect the results. In other words, if the simulation outcome is robust to the chosen value of damping, there is little for a picky reviewer to pick. In my own simulations, thermostatting between 10fs-1ps for simulation runs with a 1-fs timestep has never made a difference, unless there was energy getting added to the simulation mid-way through (e.g. from an applied electric field). But for those cases, NVE was more appropriate in the first place.</p>
<p>For the specific system in question, a gas molecule and a surface, heat exchange between the molecule and the surface physically happens via some density of vibrational states. For example, when the molecule sits on a surface, it could transfer energy back and forth with the surface’s phonon modes. As the surface has a lot of atoms, its density of vibrational states is large, and could very well be treated as continuous. In the simplest terms, this is how equipartitioning physically happens. If the phonon-phonon couplings or vibrational coherence times (dephrasing rates) for the molecule and the surface are known, those could offer physics-based starting points for the optimization of the damping hyperparameter. The molecular vibrations are typically more singular, but the molecule also has (typically low-energy and low-frequency, i.e. slow) rotations and translations.</p>
<p>The interplay between these slow degrees of freedom and the effective potential that keeps the molecule on the surface determines binding and unbinding. This is the effective potential in which the molecule moves as a whole. The correspondence of the simulation velocity and the accuracy with which the simulation describes the molecule’s positions (and the “hopping” in the question) depends on accuracy of thermalization on this relatively slow timescale. With translations and rotations being on the timescale of a terahertz (≈1 ps), and phonon thermalization typically faster, this should be the slowest timescale in the system.</p>
<p>Now, physically speaking, how would a hop (as described in the question) happen? The molecule could move far enough to unbind or switch position. Practically, this should not happen within one timestep of the simulation. Assuming that the timestep of the simulation is small enough that translation takes a large number of time steps, then the continued presence of extra energy in the translational degree of freedom is either physical, or will be taken out by the thermostat. Ultimately, judging whether a binding or unbinding event is physical or unphysical is going to be up to the human behind the simulation.</p>
<p>A second practical piece of advice I could offer is to verify whether exempting the center-of-mass motions of the molecule and/or the surface makes any difference. Sometimes that is sufficient to prevent unphysical accumulation of energy in the translational degrees of freedom, and I would consider it for simulations where one part of the system is expected to meaningfully translate past another, such as the molecule translating past the surface in question. E.g. for LAMMPS+Nosé:</p>
<pre><code>fix molVibr molecule temp/com ${molTemp}  ${molTemp}  ${moleculeDampingFactor}
</code></pre>


================================================================================

Question: Promising functionals for transition metal chemistry
Body: <p>I'm looking to get your opinions on the most promising density functionals to use for thermochemistry and kinetics of transition metal complexes. However, as eloquently laid out by Tom Manz on this <a href="http://www.ccl.net/cgi-bin/ccl/message-new?2019+03+15+001" rel="noreferrer">CCL post</a>, I am not <em>solely</em> interested in the best functional according to a given benchmark set. As a practical user, there are several other important considerations I'd like to emphasize given my needs:</p>
<ol>
<li><p>There must be support for geometry optimizations and analytical vibrational frequencies for this functional in either ORCA or Gaussian, the former because it is free and publicly available and the latter as a last resort because many computing facilities have a license. For ORCA, this requirement sadly rules out ωB97X-V and ωB97M-V (no support for gradients with nonlocal correlation) and all meta-(hybrid)-GGAs (no support for analytical frequencies). Naturally, this requirement also implies that the functional must be present in ORCA (including via <a href="https://tddft.org/programs/libxc/" rel="noreferrer">libxc</a>) or Gaussian as well. For Gaussian, meta-(hybrid)-GGAs have support for analytical frequencies, but there is a much more limited selection of functionals (e.g. ωB97X-V and ωB97M-V are not present).</p>
</li>
<li><p>Converging the self-consistent field (SCF) should be as minimal a nightmare as possible. Older meta-(hybrid)-GGAs, such as M06, can be particularly problematic here. This is largely resolved with newer Minnesota functionals that include smoothness constraints, such as revM06 and MN15, but these functionals violate Condition #1 and #4, respectively.</p>
</li>
<li><p>Ideally, I'd like it to also be reasonable at capturing spin states of transition metal complexes. This is perhaps asking for a lot, but I put it down anyway. M06-2X, for instance, is shown to be good at predicting kinetics for organometallic reactions, but the very high fraction of HF exchange (i.e. 54%) is concerning for other properties.</p>
</li>
<li><p>If given the option, I'd prefer a functional that does not have an exorbitant number of fitted parameters. I will always have some degree of hesitation with MN15, for instance.</p>
</li>
</ol>
<p>On one hand, a Minnesota functional would seem to be ideal based solely on the title of this post alone. However, the fact that all of them are meta-(hybrid) functionals rules out their practical use with ORCA, and the newer revised versions of the M06 family are not present in Gaussian. On the other hand, I like what I've seen with the range-separated hybrids ωB97X-V and ωB97M-V, but both can't readily be used in ORCA or Gaussian as mentioned above.</p>
<p>I feel like this mostly leaves me with ωB97X-D as the clearest choice to consider. However, I guess this would be sacrificing the greater accuracy of ωB97X-V for the availability of ωB97X-D. In terms of modifications to ωB97X-D, to my surprise ωB97X with D3(BJ) corrections does worse than the standard -D (i.e. D2) correction for barrier heights according to <a href="https://pubs.acs.org/doi/pdf/10.1021/acs.jpca.9b01546" rel="noreferrer">this paper</a>. Will need to investigate this more.</p>
<p>Any suggestions are welcome.</p>

Best Answer: <p>Just to provide an answer to this question, my conclusion after all this is that in general, there is unfortunately not a great answer. I likely put too many constraints on the question.</p>
<p>As noted in <a href="https://www.tandfonline.com/doi/full/10.1080/00268976.2017.1333644" rel="noreferrer">&quot;Thirty Years of DFT&quot;</a> from the Head-Gordon group, ωB97X-V and the more expensive ωB97M-V are both excellent options for many tasks but are not widely available outside of Q-Chem at the time of writing. On the Minnesota functional side of things, revM06 and MN15 have addressed some of their older grid and SCF convergence-related challenges. Unfortunately, ORCA doesn't support analytical Hessians for meta-(hybrid) GGAs, but at least MN15 is in the current release of Gaussian. This isn't even getting into the topic of spin states. That's a whole other can of worms.</p>
<p>For now, it's a bit of a free-for-all depending on the code you have access to, but I'm hopeful in the future that ωB97M-V and ωB97X-V will become more widely available in DFT codes. Perhaps in a year or two when someone sees this post, they'll be able to provide a more definitive answer.</p>


================================================================================

Question: How to explain Miller indices to someone outside nanomaterials?
Body: <p>I often find that when talking to experimentalists outside of the field of nanomaterials, it is nearly impossible to explain Miller indices concisely.  As this often comes up during talks from other students or even faculty, how would you suggest it be explained assuming they have only a generic chemistry background?</p>

Best Answer: <p>You can see it with <a href="https://jp-minerals.org/vesta/en/" rel="noreferrer">VESTA software.</a> For example, we can see the different lattice planes of NaCl crystal.</p>
<ul>
<li>[001] plane of NaCl:</li>
</ul>
<p><a href="https://i.sstatic.net/9rdrb.png" rel="noreferrer"><img src="https://i.sstatic.net/9rdrb.png" alt="enter image description here" /></a></p>
<ul>
<li>[101] plane of NaCl:</li>
</ul>
<p><a href="https://i.sstatic.net/sotmU.png" rel="noreferrer"><img src="https://i.sstatic.net/sotmU.png" alt="enter image description here" /></a></p>
<ul>
<li>[111] plane of NaCl:</li>
</ul>
<p><a href="https://i.sstatic.net/7q4FI.png" rel="noreferrer"><img src="https://i.sstatic.net/7q4FI.png" alt="enter image description here" /></a></p>


================================================================================

Question: Theoretical paper was rejected by the editor due to no experimental data: Is this common?
Body: <p>This may be a sensitive question that may cause theoretical scientists to feel injured.</p>
<p>Today my paper was rejected after assigning it to the editor. Here is what the editor said about the reason:</p>
<blockquote>
<p>&quot;With your manuscript, in absence of experimental data supporting the
theoretical predictions, I am sadly unable to conclude that the work
represents a clear conceptual and methodological advance that would
likely generate interest among our readership. The paper is better
suited for a broad-topic journal focused on hypothesis-driven
research, without any evaluation of immediate impact.&quot;</p>
</blockquote>
<p>My question is whether the theoretical data are just something that must follow the experimental data. Are predicted data meaningless without experimental data?</p>

Best Answer: <p>This all depends very much on the journal. It is not necessarily something that should make you &quot;feel injured&quot;, since there's many journals that allow for the publication of theoretical calculations that are completely devoid of experimental support.</p>
<p>This specific wording that the editor used:</p>
<blockquote>
<p>&quot;The paper is better suited for a broad-topic journal focused on
hypothesis-driven research, without any evaluation of immediate impact&quot;</p>
</blockquote>
<p>makes it appear that the journal to which you submitted, prefers to publish papers that will have &quot;immediate impact&quot; rather than papers that generate a hypothesis to be tested later by experimentalists. Furthermore the beginning of the editor's message:</p>
<blockquote>
<p>&quot;With your manuscript, in absence of experimental data supporting the
theoretical predictions...&quot;</p>
</blockquote>
<p>indicates to me that this journal prefers to publish work that is experimentally supported.</p>
<p><em><strong>Examples of journals in our field that are like this include JACS and Angewandte Chemie</strong></em>, the former being notorious for not publishing work in theoretical chemistry without experimental support. However &quot;International Journal of Quantum Chemistry&quot; and <a href="https://www.sciencedirect.com/journal/journal-of-molecular-structure-theochem" rel="noreferrer">THEOCHEM</a> do publish a lot of papers that are purely theoretical (however you have not told us which journal you're discussing, so it's hard to tell whether or not you're a molecular matter modeler or a materials matter modeler: based on your other questions it looks like you work more in the solid-state side of things, meaning that you'd be looking more at physics journals than chemistry journals, so &quot;Physica Status Solidi B&quot; might be a journal for you to consider).</p>
<blockquote>
<p>&quot;My question is whether the theoretical data are just something that
must follow the experimental data. Are predicted data meaningless
without experimental data?&quot;</p>
</blockquote>
<p>It absolutely does not have to follow experimental data, and it is absolutely <em>not</em> &quot;meaningless&quot; without experimental data, <em>however</em> a trap that a lot of middle-scale academics fall into is that they think that because they solved a challenging problem that involved a lot of work, it should be publishable in a good journal. Unfortunately there's millions of challenging problems that we can get students to solve, which will be of very little interest to other people, so if a journal wants to maintain its reputation as one of the &quot;most cited&quot; or &quot;most viewed&quot; journals in the field, it often has to choose only the most &quot;impactful&quot; or &quot;widely interesting&quot; papers to publish. Now especially in our field of matter modeling, a lot of theoretical results are in fact meaningless from an experimental standpoint (<em>not</em> meaningless altogether, because by telling the reader what a certain method predicted under the conditions of the calculations, it still says something meaningful about the <em>method</em>) because you can get numbers with DFT (for example) that are just completely wrong. Many journals will not be interested in publishing such results, which is why journals catering <em>specifically</em> to theoretical work (e.g. International Journal of Quantum Chemistry or THEOCHEM) were created to let theoreticians have a place to put their results.</p>
<p>Finally I'll say that many editors get a <em><strong>large volume</strong></em> of papers each day, and it's not uncommon for them to give generic rejection letters that are made to look polite and personal but really are copied and pasted. If you see your question's edit history, you might notice that you made a lot of grammatical errors just in writing this question alone, and if your paper looked anything like that, the editor might (based on the writing quality alone) not have wanted to waste the time of the referees, since it's not always easy to find senior academics that are willing to volunteer their time towards reviewing a (poorly written) paper for free. So if you see lots of theoretical papers in that journal, without experimental support, consider that your paper just didn't meet the journal's quality standards.</p>
<p>I will also end with what are I wrote at the end of <a href="https://mattermodeling.stackexchange.com/a/3885/5">this answer</a>:</p>
<blockquote>
<p>&quot;It usually does no harm to look at some of the papers in the most
recent issue of a journal, and even write to the editor an informal
inquiry about the suitability of the journal for your paper, before
formally submitting the paper, since this can save you the loss of
time (and energy and enthusiasm!) associated with having a paper
rejected.&quot;</p>
</blockquote>
<p>By doing this you can see in advance whether or not the journal accepts papers without experimental support.</p>
<p>I would recommend next time also to tell us <em><strong>which journal</strong></em> you're discussing, and if you don't mind even showing us the paper we can help you a lot more than you might have originally thought.</p>


================================================================================

Question: How to generate the high symmetry paths for band structure calculations?
Body: <p>Suppose I want to calculate the band structure of a material system along a specific high symmetry path in the Brillouin Zone. If the point group of the system is given, how can I easily compute the fractional coordinates of the high symmetry points U, G, W, etc.? Can Pymatgen compute these points? I guess this should be automated. Thanks.</p>

Best Answer: <p>If you are really interested in learning how to generate the path, I strongly advice you to avoid using any automatic tool like suggested in previous answers.</p>
<p>In the Wiki page entitle <a href="https://en.wikipedia.org/wiki/Brillouin_zone" rel="noreferrer">Brillouin zone</a> you can find the first Brillouin zone for each one of the Bravais lattice. This Wiki is based on the following paper:</p>
<blockquote>
<p>Setyawan, Wahyu; Curtarolo, Stefano (2010). &quot;High-throughput electronic band structure calculations: Challenges and tools&quot;. Computational Materials Science. 49 (2): 299–312 (DOI: <a href="https://doi.org/10.1016/j.commatsci.2010.05.010" rel="noreferrer">10.1016/j.commatsci.2010.05.010</a>). <a href="https://arxiv.org/abs/1004.2974" rel="noreferrer">arXiv:1004.2974</a>.</p>
</blockquote>
<p>Of course that you also need to go through solid state books like:</p>
<blockquote>
<ul>
<li>Kittel, Charles (1996). Introduction to Solid State Physics. New York: Wiley. ISBN 978-0-471-14286-7.</li>
<li>Ashcroft, Neil W.; Mermin, N. David (1976). Solid State Physics. Orlando: Harcourt. ISBN 978-0-03-049346-1.</li>
</ul>
</blockquote>
<p>After you know the Bravais lattice of your system, you look for the Brillouin zone, then look for the corresponding table with the high symmetry points and then you create your own path. Normally we start from the <span class="math-container">$\Gamma$</span> point that is the center of the Brillouin zone. The subsequent points are selected maintaining the continuity of the path.</p>
<p>For example, if your system has tetragonal symmetry (lets said, it is primitive tetragonal), then the first Brillouin zone will be:</p>
<img src="https://i.sstatic.net/DkN47.png" width="300" height="300">
<p>In the caption of the figure, there is a suggestion for the path. If you look carefully, there are three branch: the first one between <span class="math-container">$\Gamma$</span> and Z, a second one between X and R; and finally, a third one between M and A. As you can see, there is a discontinuity between Z and X and between R and M as you have to jump to reach them.</p>
<p>Now, that you know the high symmetry points and created the path, you need to know how to generate it. The table bellow has the values of each point (in the reciprocal space) according the cell parameters.</p>
<p><a href="https://i.sstatic.net/EV7Dc.png" rel="noreferrer"><img src="https://i.sstatic.net/EV7Dc.png" alt="enter image description here" /></a></p>
<p>Finally, you go to your program and add the path in accordance with its own format.</p>


================================================================================

Question: What are the physical reasons if the SCF doesn&#39;t converge?
Body: <p>I've been mostly using ab initio methods as a routine calculation tool, so even though I know some things from the theoretical side, I still have mostly hands-on experience. One of the things that I know is if your molecular geometry does not make sense, then SCF crashes. One example would be if I use angstroms instead of bohrs in the geometry definition (who hasn't been there!). I assume that if my molecular geometry makes little chemical sense, SCF would also crash.</p>
<p>What would be some physical/numerical reasons for these failures?</p>

Best Answer: <p>I have written a section in the <a href="https://bdf-manual.readthedocs.io/zh_CN/latest/User%20Guide.html#scfconvproblems" rel="noreferrer">BDF user manual</a> on this issue. It is in Chinese but I'll roughly translate the key parts to English as below.</p>
<p>Common reasons for the SCF procedure to fail to converge include:</p>
<ol>
<li>A too small HOMO-LUMO gap, which causes repetitive changes of the frontier orbital occupation numbers. Imagine two orbitals <span class="math-container">$\psi_1$</span> and <span class="math-container">$\psi_2$</span>, where the former is occupied and the latter is unoccupied at the <span class="math-container">$N$</span>-th SCF iteration. If the orbital energies of <span class="math-container">$\psi_1$</span> and <span class="math-container">$\psi_2$</span> do not differ much, then it may well be that at the <span class="math-container">$N+1$</span>-th SCF iteration, the orbital energy of <span class="math-container">$\psi_1$</span> becomes higher than that of <span class="math-container">$\psi_2$</span>, resulting in the electron(s) on <span class="math-container">$\psi_1$</span> transferring to the orbital <span class="math-container">$\psi_2$</span>. However, this will inevitably cause a large change in the density matrix, and therefore the Fock matrix. Thus chances are that if one diagonalizes the new Fock matrix, one finds that <span class="math-container">$\psi_1$</span> is again lower in energy than <span class="math-container">$\psi_2$</span>. In this unfortunate scenario, the orbital occupation number oscillates between two patterns, which prevents convergence. Typical signatures of this scenario include an oscillating SCF energy (with an amplitude of <span class="math-container">$10^{-4} \sim 1$</span> Hartree), and a clearly wrong occupation pattern printed at the end of the SCF calculation.</li>
<li>The HOMO-LUMO gap is relatively small but not excessively small, so that the occupation numbers of the orbitals do not change, but the orbital shape oscillates (this is what the physicists call &quot;charge sloshing&quot;). (The polarizability of a system is inversely proportional to the HOMO-LUMO gap, and if the polarizability is high, a small error in the Kohn-Sham potential may result in a large distortion of the electronic density. When the HOMO-LUMO gap shrinks beyond a certain point, the distorted density may give a Kohn-Sham potential that is even more erroneous than the original one, which marks the onset of divergence.) Typical signatures include an oscillating SCF energy with a slightly smaller magnitude, and a qualitatively correct occupation pattern.</li>
<li>Numerical noise caused by a too small grid or a too loose integral cutoff threshold. Typical signatures include an oscillating SCF energy with a very small magnitude (<span class="math-container">$&lt;10^{-4}$</span> Hartree), and a qualitatively correct occupation pattern.</li>
<li>The basis set (orbital basis set, auxiliary basis sets, etc.) is close to linearly dependent, or the grid is so small that the projection of the basis set on the grid is close to linearly dependent (this problem does not exist for plane wave basis sets). Typical signatures include a wildly oscillating or unrealistically low SCF energy (error &gt; 1 Hartree), and a qualitatively wrong occupation pattern.</li>
</ol>
<p>Note that a wrong geometry or a wrong charge merely increase the likelihood that problems 1, 2 and 4 arise, and are themselves not separate reasons for SCF non-convergence. In particular, too long bonds make problems 1 and 2 more likely, while too short bonds tend to invoke problem 4. Many people tend to ignore or underestimate the importance of problems 3 and 4, such that they play around hopelessly with damp and level shift (or in the physicists' language, different mixing schemes), which actually only work for problems 1 and 2, when the true cause of the non-convergence is actually numerical noise or basis set linear dependence. (For the sake of this particular question, however, one may argue that only problems 1 and 2 are &quot;physical reasons&quot;, since problems 3 and 4 are purely numerical artifacts and are not due to any physical property of the system being studied.)</p>


================================================================================

Question: What is the state of the art in terms of local atomic environment descriptors for machine learning?
Body: <p>A lot of atomistic machine learning deals with correctly describing atomic neighborhood environments by vectors or fingerprints (see, e.g. <a href="https://doi.org/10.1063/1.5055772" rel="noreferrer"><em>J. Chem. Phys.</em> 149, 244102 (<strong>2018</strong>)</a>, <a href="https://doi.org/10.1103/PhysRevB.87.184115" rel="noreferrer"><em>Phys. Rev. B</em> 87, 184115 (<strong>2013</strong>)</a>), before feeding those descriptions to neural networks, kernel ridge regressors or other algorithms</p>

<p>So the question is: <strong>what is the current status of local atomic environment descriptors for machine learning?</strong>
Specifically, I want to know what makes a good descriptor and if there are any classification to them (i.e., what makes them different?).</p>

<p>(This question is complimentary to <a href="https://materials.stackexchange.com/q/18/63">"What is the current status of machine learning applied to materials or molecular systems?"</a>.)</p>

Best Answer: <p>If you are familiar with the <a href="https://doi.org/10.1063/1.3553717" rel="noreferrer">Behler-Parrinello symmetry functions</a> implemented in AMP, you may be interested in seeing how they compare to other atom-centered representations in terms of speed and accuracy. Marcel F. Langer, Alex Goeßmann, and Matthias Rupp have recently released <a href="https://arxiv.org/abs/2003.12081" rel="noreferrer">their benchmarking</a> <a href="https://github.com/sirmarcel/cmlkit" rel="noreferrer">efforts</a> including the symmetry functions, the <a href="https://arxiv.org/abs/1704.06439" rel="noreferrer">Many-body Tensor Representation</a>, and the <a href="https://doi.org/10.1103/PhysRevB.87.184115" rel="noreferrer">Smooth Overlap of Atomic Positions representation</a>. Their work also includes concise summaries of other representations to get you up to speed as well as <strong>what exactly makes a good representation</strong>:</p>

<ol>
<li>Invariance to rotations, translations, and permutations</li>
<li>Uniqueness: "Systems with identical representations that differ in property
introduce errors"</li>
<li>Continuity/Differentiability </li>
<li>Computational efficiency</li>
<li>Structure (e.g constant size)</li>
<li>Generality, " in the sense of being able to encode any
atomistic system"</li>
</ol>

<p><strong>What distinguishes many representations is the choice of their basis set</strong> when encoding physical distances and angles into machine-learning inputs. Where the Behler-Parrinello symmetry functions use Gaussian functions, the <a href="https://doi.org/10.1103/PhysRevB.96.014112" rel="noreferrer">Artrith-Urban-Ceder descriptor</a> uses Chebyshev polynomials. The Many-Body Tensor Representation uses a real-space basis, while the Smooth Overlap of Atomic Positions uses spherical harmonics. Michele Ceriotti's group has released <a href="https://arxiv.org/pdf/1807.00408.pdf" rel="noreferrer">an excellent paper</a> connecting these atom-centered representations with a general mathematical formulism.</p>

<p>Dr. Ceriotti is also on a paper with Gabor Csanyi where they have extensively investigated the topic of uniqueness. <a href="https://arxiv.org/pdf/2001.11696.pdf" rel="noreferrer">The paper</a> <strong>highlights the limitations of using representations that stop at 3-body terms</strong> (i.e distances and angles).</p>

<p>While invariance and equivariance might be handled by the representation, there are several groups working on finding <a href="https://blondegeek.github.io/e3nn_tutorial/" rel="noreferrer"><strong>ways to handle equivariance directly with the model architecture</strong></a>. As far as I understand, this is especially necessary when learning tensorial properties rather than scalar properties like energy.</p>

<p>As Greg alluded to, there have also been efforts to create <strong>machine learning frameworks where atomic representations can be learned and tuned automatically</strong>. Schnet (or <a href="https://schnetpack.readthedocs.io/en/stable/" rel="noreferrer">Schnetpack</a>) is a framework that uses continuous-filter convolutional neural networks to do so.</p>

<p>I recommend watching these lectures from the recent Institute for Pure and Applied Mathematics program on "Machine Learning for Physics and the Physics of Learning":</p>

<p>Richard G. Hennig: <a href="http://www.ipam.ucla.edu/abstract/?tid=16352&amp;pcode=MLPWS1" rel="noreferrer">Machine-learning for materials and physics discovery through symbolic regression and kernel methods</a></p>

<p>Tess Smidt: <a href="http://www.ipam.ucla.edu/abstract/?tid=16346&amp;pcode=MLPWS1" rel="noreferrer">Euclidean Neural Networks* for Emulating Ab Initio Calculations and Generating Atomic Geometries *also called Tensor Field Networks and 3D Steerable CNNs</a></p>

<p>Anatole von Lilienfeld: <a href="http://www.ipam.ucla.edu/abstract/?tid=15761&amp;pcode=MLPWS2" rel="noreferrer">Quantum Machine Learning</a></p>

<p>Michele Ceriotti: <a href="http://www.ipam.ucla.edu/abstract/?tid=15817&amp;pcode=MLPWS4" rel="noreferrer">Machine learning for atomic and molecular simulations</a></p>

<p>Matthias Rupp: <a href="http://www.ipam.ucla.edu/abstract/?tid=15782&amp;pcode=MLPWS4" rel="noreferrer">How to assess scientific machine learning models? Prediction errors and predictive uncertainty quantification</a></p>

<p>Gabor Csanyi: <a href="http://www.ipam.ucla.edu/abstract/?tid=15837&amp;pcode=MLPWS4" rel="noreferrer">Representation and regression problems for molecular structure and dynamics</a></p>


================================================================================

Question: Exchange-Correlation Two-Electron Integrals
Body: <p>Do any freely available or proprietary electronic structure packages explicitly compute two-electron exchange correlation integrals?</p>

<p>I have found a few derivations to calculate excited state properties/spectroscopic observables using DFT,<sup>[1]</sup> and they tend to write expressions including integrals of the form </p>

<p><span class="math-container">$$\langle\mu\nu|w|\lambda\sigma\rangle$$</span></p>

<p>where <span class="math-container">$w$</span> is the exchange-correlation kernel. However, I have yet to find a package that specifically generates these integrals. It seems that in general they do some sort of prior contraction with the density to avoid the need to generate these terms explicitly when calculating response properties. However, I had wanted to make a small standalone script that used these quantities. Its presumably much less efficient than the approach used in these packages, but the implementation is much clearer for me. Are there any packages that have an existing option to do this? Or is it something that would require some modification of either these packages themselves or some existing API?</p>

<ol>
<li>S. Hirata, M. Head-Gordon, R.J. Bartlett <strong>Configuration interaction singles, time-dependent Hartree–Fock,
and time-dependent density functional theory for the electronic
excited states of extended systems</strong> <em>J. Chem. Phys.</em> 111, 10774 (1999); <a href="https://doi.org/10.1063/1.480443" rel="nofollow noreferrer">DOI: 10.1063/1.480443</a></li>
</ol>

Best Answer: <p>I assume you're referring to eq 51 of the Hirata-Head-Gordon-Bartlett paper.</p>

<p>One should note that these are not two-electron integrals, since there is only one spatial position; these are rather weighted four-center one-electron integrals.</p>

<p>As always, the problem when you have four indices is that there is a <strong>huge</strong> number of integrals that come out, and you might not have storage for them.</p>

<p>Another issue is that the set of four-products is linearly dependent to a ridiculous degree. If you start out with a atomic basis set, in usual electron repulsion integrals you get basis function products. Most of these will be linearly dependent, and you get out a linearly growing number of independent functions (this is why the Cholesky decomposition is so powerful in repulsion integrals) and the rest <span class="math-container">$O(N^2)$</span> are linearly dependent.</p>

<p>Now, instead of basis function products, you have products of basis function products. Again, you only get a linearly growing number of independent functions, with a prefactor higher than in the two-electron case, meaning that the number of linearly dependent functions grows as <span class="math-container">$O(N^4)$</span>. So, you'd like to get a large number of integrals that are mostly linearly dependent.</p>

<p>Other than this issue, there's nothing that would prevent you from evaluating the integrals in the same way as is done in the paper. That is, you just need numerical quadrature to do it.</p>

<p>If you had an optimized version of the procedure, one could extract the integrals by <span class="math-container">$O(N^2)$</span> calls to eq (54). But this is going to be so costly that I doubt that you could do it in anything except the smallest basis set...</p>


================================================================================

Question: Is there any relevant DFT formalism apart from the Kohn-Sham approach?
Body: <p>I wonder if all DFT codes are based on the Kohn-Sham formalism?.</p>

<p>If other methods are available, what are their scopes?</p>

Best Answer: <p>At least I was able to find three other forms of DFT listed as:</p>

<ul>
<li><a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.99.165118" rel="noreferrer">Lattice density-functional theory</a>: It's suitable for modeling the lattice gas, binary liquid solutions, order-disorder phase transformations.</li>
<li><a href="https://en.wikipedia.org/wiki/Orbital-free_density_functional_theory" rel="noreferrer">Orbital-free density functional theory</a>: It's less accurate than KS but it's much faster.</li>
<li><a href="https://en.wikipedia.org/wiki/Strictly-Correlated-Electrons_density_functional_theory" rel="noreferrer">Strictly-Correlated-Electrons density functional theory</a>: It's an alternative to KS for strongly-correlated systems. In contrast to KS, when you start from a set of non-interacting electrons and you could calculate their kinetics energy exactly, and then approximate the interaction through exchange-correlation energy, in SCE DFT, you start from an infinite electronic correlation and zero kinetics energy.</li>
</ul>


================================================================================

Question: Extract coordination sphere from crystal structure
Body: <p>Some tools allow for a relatively inexpensive analysis of crystal field effects in metal complexes, starting from the coordination sphere. Additionally, there are multiple sources of crystal structures that, to a certain level, can be accessed following automated computational procedures. An example of the latter would be the Cambridge Structural Database (CSD) of  the Cambridge Crystallographic Data Centre (CCDC).</p>

<p>My question is:
<strong>Given a large series of crystal structures</strong> in any given standard form (e.g. xyz, pdb, cif...), <strong>is there a tool to automatically extract the coordination sphere of a certain metal ion</strong> with minimal user intevention?</p>

Best Answer: <p>Yes, there is! <a href="https://pymatgen.org/" rel="noreferrer">Pymatgen</a> is ideally suited for this task. Specifically, I highly recommend the <a href="https://pymatgen.org/pymatgen.analysis.local_env.html" rel="noreferrer">local environment module</a>. Given a CIF, you can calculate the coordination environment around a given atom index using the following Python code. Here, the <code>CrystalNN</code> algorithm is used, but you can find many other approaches <a href="https://pymatgen.org/pymatgen.analysis.local_env.html" rel="noreferrer">here</a> that you should consider. Simply swap out the <code>nn_object</code> line.</p>



<pre><code>import pymatgen as pm
from pymatgen.analysis.local_env import CrystalNN
p = '/path/to/cif' #path to CIF file
site_idx = 0 #index of atom to get coordination environment
structure = pm.Structure.from_file(p) #read in CIF as Pymatgen Structure
nn_object = CrystalNN() #initialize class
neighbors = nn_object.get_nn_info(structure,site_idx) #get NN info
</code></pre>

<p>An example output for <code>neighbors</code> would look something like the following, which is a list containing all the coordinating atoms and their distances, the nearest image, and the site index. Other methods, such as <code>VoronoiNN</code>, will provide additional information.</p>



<pre><code>[{'site': PeriodicSite: O (3.2910, 2.0794, 0.9655) [0.4692, 0.2958, 0.2602], 'image': (0, 0, 0), 'weight': 1, 'site_index': 25}, {'site': PeriodicSite: C (4.9976, 0.0907, 2.7314) [0.7612, 0.0129, 0.5053], 'image': (0, 0, 0), 'weight': 1, 'site_index': 35}]
</code></pre>

<p>Of course, there are simpler approaches that are just based off a fixed cutoff distance (in Pymatgen, that could be done with <code>pymatgen.core.structure.get_neighbors()</code> or in the <a href="https://wiki.fysik.dtu.dk/ase/" rel="noreferrer">Atomic Simulation Environment</a> with the <a href="https://wiki.fysik.dtu.dk/ase/ase/neighborlist.html" rel="noreferrer"><code>ase.neighborlist</code> tool</a>).</p>


================================================================================

Question: In VASP, how is the chemical potential of elements calculated?
Body: <p>I would like to calculate the chemical potential of elements having different environmental condition (rich or poor) using VASP. How is this accomplished?</p>

Best Answer: <p>The chemical potential in VASP or any other computational chemistry software is calculated based on the theory of chemical potential relation to Helmholtz energy functional as:</p>

<p><span class="math-container">$$\mu_{i} = \Bigg(\frac{\partial F(V,T,N_{1},N_{2},...,N_{i}...,N_{n})}{\partial N_{i}}\Bigg)_{V,T,N_{j \neq i}}$$</span></p>

<p>You can approximate the above equation up to first order as:</p>

<p><span class="math-container">$$\mu_{i} \simeq F(V,T,N_{i}+1,N_{j \neq i}) - F(V,T,N_{i},N_{j \neq i})$$</span></p>

<p>Here we omitted the terms of <span class="math-container">$\mathcal{O}\Big( \frac{\partial^{2} F}{\partial N_{i}^{2}} \Big)$</span>. </p>

<p>I call <span class="math-container">$F(V,T,N_{i}+1,N_{j \neq i}) - F(V,T,N_{i},N_{j \neq i}) = \Delta F_{N_{i} \rightarrow N_{i}+1}$</span></p>

<p>So, you have:</p>

<p><span class="math-container">$$\mu_{i} = \Delta F_{N_{i} \rightarrow N_{i}+1} = \mu_{\text{XC}}^{i}+\mu_{\text{ideal}}^{i}$$</span></p>

<p>Where <span class="math-container">$\mu^{i}_{\text{ideal}}$</span> is the chemical potential of the ideal gas and <span class="math-container">$\mu^{i}_{\text{XC}}$</span> is the chemical potential of exchange-correlation. The ideal gas chemical potential is trivial:</p>

<p><span class="math-container">$$\mu^{i}_{\text{ideal}} = -k_{B}T\ln{\Bigg( \frac{V}{\Lambda^{3} (N_{i}+1)}\Bigg)}$$</span></p>

<p>Where <span class="math-container">$\Lambda$</span> is the Broglie wavelength: <span class="math-container">$\Lambda = \sqrt{\frac{2\pi\hbar^{2}}{mk_{B}T}}$</span>.</p>

<p>The exchange-correlation chemical potential is calculated as:</p>

<p><span class="math-container">$$\mu^{i}_{\text{XC}} = -k_{B}T \ln{\Bigg ( \frac{1}{V} \frac{\int \exp{\Big(-\frac{U(\mathbf{r}^{N_{i}+1})}{k_{B}T}\Big)} d^{3}\mathbf{r}^{N_{i}+1}}{\int \exp{\Big(-\frac{U(\mathbf{r}^{N_{i}})}{k_{B}T}\Big)} d^{3}\mathbf{r}^{N_{i}}} \Bigg )}$$</span></p>

<p>Or in ensemble form:</p>

<p><span class="math-container">$$\mu^{i}_{\text{XC}} = -k_{B}T \ln{\Bigg ( \frac{1}{V} \Bigg \langle \int \exp{\Big( -\frac{\Delta U_{N_{i} \rightarrow N_{i}+1}}{k_{B}T} \Big)} d^{3}\mathbf{r}^{N_{i}+1} \Bigg \rangle \Bigg )}$$</span></p>


================================================================================

Question: Does anyone know any software for MD simulation which include the effect of magnetic field?
Body: <p>I am interested to generate some configurations from MD simulations which include the presence of strong magnetic field. Does anyone know any software that has some functionality to do this?</p>

Best Answer: <p>There is certainly Lammps, which has a fix efield that allows you to apply electric field in your chosen cartesian direction. But I am not sure if it works for your purposes. You can play around with it to see.</p>
<p><a href="https://lammps.sandia.gov/doc/fix_efield.html" rel="noreferrer">https://lammps.sandia.gov/doc/fix_efield.html</a></p>


================================================================================

Question: What are the common acronyms in Materials Modelling?
Body: <p>Many abbreviations are used in Materials Modelling. For example, we have DFT (density functional theory/discrete Fourier transform), TST (transition state theory) and MD (molecular dynamics).</p>

<p>Which abbreviations are commonly used in Materials Modelling?</p>

<hr>

<p><sub> This question was taken from the similar <a href="https://or.stackexchange.com/questions/865/what-are-common-and-not-so-common-abbreviations-in-operations-research">What are common and not so common abbreviations in Operations Research?</a> on OR.SE. </sub> </p>

Best Answer:  
<p>I will start with acronyms for coupled-cluster, and someone else might answer with acronyms for <strong>basis sets</strong> or for <strong>functionals</strong> or for <strong>many body perturbation theory</strong> or for <strong>composite approaches</strong> or for different <strong>types of SCF</strong>:</p>
<hr>
<h1>Coupled Cluster acronyms</h1>
<hr>
<p><strong>CCSD, CCSDT, CCSDTQ</strong>, ... (coupled cluster with singles, doubles, triples, quadrtuples, etc.)<br>
<strong>CCSD(T), CCSDT(Q)</strong>, ... (coupled cluster with perturbative triples, quadruples, pentuples, etc.)<br>
<strong>EOM-CCSD, EOM-CCSDT</strong>, ... (equation of motion coupled cluster, w/ singles, doubles, etc.)<br>
<strong>STEOM-CCSD</strong>, ... (similarity transformed equation of motion coupled cluster, w/ singles, etc.)<br>
<strong>LR-CCSD, LR-CCSDT</strong>, ... (linear response coupled cluster, a synonym for EOM-CC)<br>
<strong>CC2, CC3, CC4, CC5</strong>, ... (second, third, fourth, fifth-order coupled-cluster, etc.)<br>
<strong>SCS-CC2, SCS-CC3</strong>, ... (spin-component scaled second, third, fourth, coupled-cluster, etc.)<br>
<strong>SOS-CC2, SOS-CC3</strong>, ... (scaled opposite-spin second, third, fourth, coupled-cluster, etc.)<br>
<strong>BCCD, BCCDT, BCCDTQ</strong>, ... (Brueckner coupled cluster with doubles, triples, etc.)<br>
<strong>Mk-MRCCSD, Mk-MRCCSDT</strong>, ... (state-specific Mukherjee multireference coupled-cluster)
<strong>pCCSD, pCCSDT, pCCSDTQ</strong>, ... (parameterized coupled cluster with singles, doubles, etc.)
<strong>TCCSD,TCCSDT,TCCSDDTQ</strong>, .. (tailored coupled cluster with singles, doubles, triples, etc.)<br>
<strong>CVS-EOM-CC</strong>, .. (core-valence separated equation-of-motion coupled-cluster)</p>
<p><strong>RCCSD</strong>: partially spin-restricted coupled cluster <br>
<strong>UCCSD</strong>: spin-unrestricted coupled cluster, or unitary coupled cluster<br>
<strong>RDCSD</strong>: partially spin-restricted distinguishable coupled cluster<br>
<strong>UDCSD</strong>: spin-urestricted distinguishable coupled cluster<br>
<strong>pCCD</strong>: pair coupled cluster with doubles (sometimes called <strong>AP1RoG</strong>)</p>
<p><strong>CCSD-F12, EOM-CCSD-F12</strong>: coupled cluster, or EOM coupled cluster, with F12 correction.<br>
<strong>RI-CC2</strong>: coupled cluster, with the resolution of the identity approximation for the integrals.<br></p>


================================================================================

Question: Computing phonon dispersion from molecular dynamics simulations?
Body: <p>How would one obtain the phonon dispersion curve from a molecular dynamics simulation trajectory? What are the steps involved? Are there any packages that does this? If so, please mention them.</p>

Best Answer: <p>You are basically looking for finding phonon frequencies with respect to <span class="math-container">$\mathbf{q}$</span> the scattering vector in reciprocal space. From fluctuation-dissipation theory, the force constants of the system in the reciprocal space is given by:</p>

<p><span class="math-container">$$\Phi_{k\alpha,k^{'}\beta}(\mathbf{q}) = k_{B}T \mathbf{G}^{-1}_{k\alpha,k^{'}\beta}(\mathbf{q})$$</span></p>

<p><span class="math-container">$\mathbf{G}$</span> is the green function defined as:</p>

<p><span class="math-container">$$\mathbf{G}_{k\alpha,k^{'}\beta}(\mathbf{q}) = \langle \mathbf{u}_{k\alpha}(\mathbf{q}) \cdot \mathbf{u}_{k^{'}\beta}^{*}(\mathbf{q}) \rangle$$</span></p>

<p><span class="math-container">$\langle...\rangle$</span> is the ensemble average and <span class="math-container">$\mathbf{u}_{k\alpha}$</span> is <span class="math-container">$\alpha$</span> component of atomic displacement for <span class="math-container">$k$</span>-th atom:</p>

<p><span class="math-container">$$\mathbf{u}_{k\alpha}(\mathbf{q}) = \sum_{\ell}\mathbf{u}_{k\alpha}^{\ell}\exp{(i\mathbf{q}\cdot \mathbf{r}_{\ell})}$$</span></p>

<p>We could calculate Green function based on the instantaneous position of atoms (<span class="math-container">$\mathbf{R}$</span>) and their ensemble average as:</p>

<p><span class="math-container">$$\mathbf{G}_{k\alpha,k^{'}\beta}(\mathbf{q}) = \langle \mathbf{R}_{k\alpha}(\mathbf{q}) \cdot \mathbf{R}^{*}_{k^{'}\beta}(\mathbf{q})\rangle - \langle \mathbf{R}\rangle_{k\alpha} (\mathbf{q}) \cdot \langle \mathbf{R} \rangle^{*}_{k^{'}\beta}(\mathbf{q})$$</span></p>

<p>Now the dynamical matrix <span class="math-container">$\mathbf{D}$</span> is calculated as:</p>

<p><span class="math-container">$$\mathbf{D}_{k\alpha,k^{'}\beta}(\mathbf{q}) = (m_{k}m_{k^{'}})^{-\frac{1}{2}} \Phi_{k\alpha,k^{'}\beta}(\mathbf{q})$$</span></p>

<p>The eigenvalues of this matrix are phonon frequencies at <span class="math-container">$\mathbf{q}$</span> and if you plot them versus <span class="math-container">$\mathbf{q}$</span> you would get the phonon dispersion curve. This is done in LAMMPS by using <code>fix phonon</code> command.</p>


================================================================================

